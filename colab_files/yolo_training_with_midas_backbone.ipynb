{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yolo training with midas backbone",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLWpW1aQ6UwC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4c9a4b3-4cf9-4eb8-ebd2-659b8a6fb193"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgcsmcI16087",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "127f6abb-47dc-43be-bbe1-33dca0ee43dd"
      },
      "source": [
        "% cd /content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7agXaV15aYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f1d5480-0614-469f-9a08-71cc8ca9a664"
      },
      "source": [
        "! git clone https://github.com/intel-isl/MiDaS.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MiDaS'...\n",
            "remote: Enumerating objects: 250, done.\u001b[K\n",
            "remote: Counting objects: 100% (250/250), done.\u001b[K\n",
            "remote: Compressing objects: 100% (173/173), done.\u001b[K\n",
            "remote: Total 394 (delta 58), reused 201 (delta 37), pack-reused 144\u001b[K\n",
            "Receiving objects: 100% (394/394), 231.02 KiB | 9.63 MiB/s, done.\n",
            "Resolving deltas: 100% (139/139), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8O4jd7GhXNo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fb11b47-e576-4491-92ee-bdd9b85b90c6"
      },
      "source": [
        "cd /content/MiDaS/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/MiDaS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MahTQp3RJ2ek"
      },
      "source": [
        "# ! git checkout -b old-state b00bf61f846d73fadc1f287293648db9f88d3615"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSVHPNyQ0iwx"
      },
      "source": [
        "# ! cp /content/drive/My\\ Drive/pre_trained/model-f46da743.pt /content/MiDaS/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxzqyQVQqKRp"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import utils\n",
        "import cv2\n",
        "\n",
        "from torchvision.transforms import Compose\n",
        "import torchvision.transforms as transforms\n",
        "from midas.midas_net import MidasNet\n",
        "from midas.transforms import Resize, NormalizeImage, PrepareForNet\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGSpPBO3PBz_"
      },
      "source": [
        "from torchsummary import summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r90rL5iAJOym"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class BaseModel(torch.nn.Module):\n",
        "    def load(self, path):\n",
        "        \"\"\"Load model from file.\n",
        "\n",
        "        Args:\n",
        "            path (str): file path\n",
        "        \"\"\"\n",
        "        parameters = torch.load(path, map_location=torch.device('cpu'))\n",
        "\n",
        "        if \"optimizer\" in parameters:\n",
        "            parameters = parameters[\"model\"]\n",
        "\n",
        "        self.load_state_dict(parameters)\n",
        "\n",
        "\n",
        "def _make_encoder(backbone, features, use_pretrained, groups=1, expand=False, exportable=True):\n",
        "    if backbone == \"resnext101_wsl\":\n",
        "        pretrained = _make_pretrained_resnext101_wsl(use_pretrained)\n",
        "        scratch = _make_scratch([256, 512, 1024, 2048], features, groups=groups, expand=expand)     # efficientnet_lite3  \n",
        "    elif backbone == \"efficientnet_lite3\":\n",
        "        pretrained = _make_pretrained_efficientnet_lite3(use_pretrained, exportable=exportable)\n",
        "        scratch = _make_scratch([32, 48, 136, 384], features, groups=groups, expand=expand)  # efficientnet_lite3     \n",
        "    else:\n",
        "        print(f\"Backbone '{backbone}' not implemented\")\n",
        "        assert False\n",
        "        \n",
        "    return pretrained, scratch\n",
        "\n",
        "\n",
        "def _make_scratch(in_shape, out_shape, groups=1, expand=False):\n",
        "    scratch = nn.Module()\n",
        "\n",
        "    out_shape1 = out_shape\n",
        "    out_shape2 = out_shape\n",
        "    out_shape3 = out_shape\n",
        "    out_shape4 = out_shape\n",
        "    if expand==True:\n",
        "        out_shape1 = out_shape\n",
        "        out_shape2 = out_shape*2\n",
        "        out_shape3 = out_shape*4\n",
        "        out_shape4 = out_shape*8\n",
        "\n",
        "    scratch.layer1_rn = nn.Conv2d(\n",
        "        in_shape[0], out_shape1, kernel_size=3, stride=1, padding=1, bias=False, groups=groups\n",
        "    )\n",
        "    scratch.layer2_rn = nn.Conv2d(\n",
        "        in_shape[1], out_shape2, kernel_size=3, stride=1, padding=1, bias=False, groups=groups\n",
        "    )\n",
        "    scratch.layer3_rn = nn.Conv2d(\n",
        "        in_shape[2], out_shape3, kernel_size=3, stride=1, padding=1, bias=False, groups=groups\n",
        "    )\n",
        "    scratch.layer4_rn = nn.Conv2d(\n",
        "        in_shape[3], out_shape4, kernel_size=3, stride=1, padding=1, bias=False, groups=groups\n",
        "    )\n",
        "\n",
        "    return scratch\n",
        "\n",
        "\n",
        "def _make_pretrained_efficientnet_lite3(use_pretrained, exportable=False):\n",
        "    efficientnet = torch.hub.load(\n",
        "        \"rwightman/gen-efficientnet-pytorch\",\n",
        "        \"tf_efficientnet_lite3\",\n",
        "        pretrained=use_pretrained,\n",
        "        exportable=exportable\n",
        "    )\n",
        "    return _make_efficientnet_backbone(efficientnet)\n",
        "\n",
        "\n",
        "def _make_efficientnet_backbone(effnet):\n",
        "    pretrained = nn.Module()\n",
        "\n",
        "    pretrained.layer1 = nn.Sequential(\n",
        "        effnet.conv_stem, effnet.bn1, effnet.act1, *effnet.blocks[0:2]\n",
        "    )\n",
        "    pretrained.layer2 = nn.Sequential(*effnet.blocks[2:3])\n",
        "    pretrained.layer3 = nn.Sequential(*effnet.blocks[3:5])\n",
        "    pretrained.layer4 = nn.Sequential(*effnet.blocks[5:9])\n",
        "\n",
        "    return pretrained\n",
        "    \n",
        "\n",
        "def _make_resnet_backbone(resnet):\n",
        "    pretrained = nn.Module()\n",
        "    pretrained.layer1 = nn.Sequential(\n",
        "        resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool, resnet.layer1\n",
        "    )\n",
        "\n",
        "    pretrained.layer2 = resnet.layer2\n",
        "    pretrained.layer3 = resnet.layer3\n",
        "    pretrained.layer4 = resnet.layer4\n",
        "\n",
        "    return pretrained\n",
        "\n",
        "\n",
        "def _make_pretrained_resnext101_wsl(use_pretrained):\n",
        "    resnet = torch.hub.load(\"facebookresearch/WSL-Images\", \"resnext101_32x8d_wsl\")\n",
        "    # print(resnet)\n",
        "    return _make_resnet_backbone(resnet)\n",
        "\n",
        "\n",
        "\n",
        "class Interpolate(nn.Module):\n",
        "    \"\"\"Interpolation module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, scale_factor, mode):\n",
        "        \"\"\"Init.\n",
        "\n",
        "        Args:\n",
        "            scale_factor (float): scaling\n",
        "            mode (str): interpolation mode\n",
        "        \"\"\"\n",
        "        super(Interpolate, self).__init__()\n",
        "\n",
        "        self.interp = nn.functional.interpolate\n",
        "        self.scale_factor = scale_factor\n",
        "        self.mode = mode\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x (tensor): input\n",
        "\n",
        "        Returns:\n",
        "            tensor: interpolated data\n",
        "        \"\"\"\n",
        "\n",
        "        x = self.interp(\n",
        "            x, scale_factor=self.scale_factor, mode=self.mode, align_corners=False\n",
        "        )\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResidualConvUnit(nn.Module):\n",
        "    \"\"\"Residual convolution module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, features):\n",
        "        \"\"\"Init.\n",
        "\n",
        "        Args:\n",
        "            features (int): number of features\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            features, features, kernel_size=3, stride=1, padding=1, bias=True\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            features, features, kernel_size=3, stride=1, padding=1, bias=True\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x (tensor): input\n",
        "\n",
        "        Returns:\n",
        "            tensor: output\n",
        "        \"\"\"\n",
        "        out = self.relu(x)\n",
        "        out = self.conv1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        return out + x\n",
        "\n",
        "\n",
        "class FeatureFusionBlock(nn.Module):\n",
        "    \"\"\"Feature fusion block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, features):\n",
        "        \"\"\"Init.\n",
        "\n",
        "        Args:\n",
        "            features (int): number of features\n",
        "        \"\"\"\n",
        "        super(FeatureFusionBlock, self).__init__()\n",
        "\n",
        "        self.resConfUnit1 = ResidualConvUnit(features)\n",
        "        self.resConfUnit2 = ResidualConvUnit(features)\n",
        "\n",
        "    def forward(self, *xs):\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Returns:\n",
        "            tensor: output\n",
        "        \"\"\"\n",
        "        output = xs[0]\n",
        "\n",
        "        if len(xs) == 2:\n",
        "            output += self.resConfUnit1(xs[1])\n",
        "\n",
        "        output = self.resConfUnit2(output)\n",
        "\n",
        "        output = nn.functional.interpolate(\n",
        "            output, scale_factor=2, mode=\"bilinear\", align_corners=True\n",
        "        )\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ResidualConvUnit_custom(nn.Module):\n",
        "    \"\"\"Residual convolution module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, features, activation, bn):\n",
        "        \"\"\"Init.\n",
        "\n",
        "        Args:\n",
        "            features (int): number of features\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.bn = bn\n",
        "\n",
        "        self.groups=1\n",
        "\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            features, features, kernel_size=3, stride=1, padding=1, bias=True, groups=self.groups\n",
        "        )\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(\n",
        "            features, features, kernel_size=3, stride=1, padding=1, bias=True, groups=self.groups\n",
        "        )\n",
        "\n",
        "        if self.bn==True:\n",
        "            self.bn1 = nn.BatchNorm2d(features)\n",
        "            self.bn2 = nn.BatchNorm2d(features)\n",
        "\n",
        "        self.activation = activation\n",
        "\n",
        "        self.skip_add = nn.quantized.FloatFunctional()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x (tensor): input\n",
        "\n",
        "        Returns:\n",
        "            tensor: output\n",
        "        \"\"\"\n",
        "        \n",
        "        out = self.activation(x)\n",
        "        out = self.conv1(out)\n",
        "        if self.bn==True:\n",
        "            out = self.bn1(out)\n",
        "       \n",
        "        out = self.activation(out)\n",
        "        out = self.conv2(out)\n",
        "        if self.bn==True:\n",
        "            out = self.bn2(out)\n",
        "\n",
        "        if self.groups > 1:\n",
        "            out = self.conv_merge(out)\n",
        "\n",
        "        return self.skip_add.add(out, x)\n",
        "\n",
        "        # return out + x\n",
        "\n",
        "\n",
        "class FeatureFusionBlock_custom(nn.Module):\n",
        "    \"\"\"Feature fusion block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, features, activation, deconv=False, bn=False, expand=False, align_corners=True):\n",
        "        \"\"\"Init.\n",
        "\n",
        "        Args:\n",
        "            features (int): number of features\n",
        "        \"\"\"\n",
        "        super(FeatureFusionBlock_custom, self).__init__()\n",
        "\n",
        "        self.deconv = deconv\n",
        "        self.align_corners = align_corners\n",
        "\n",
        "        self.groups=1\n",
        "\n",
        "        self.expand = expand\n",
        "        out_features = features\n",
        "        if self.expand==True:\n",
        "            out_features = features//2\n",
        "        \n",
        "        self.out_conv = nn.Conv2d(features, out_features, kernel_size=1, stride=1, padding=0, bias=True, groups=1)\n",
        "\n",
        "        self.resConfUnit1 = ResidualConvUnit_custom(features, activation, bn)\n",
        "        self.resConfUnit2 = ResidualConvUnit_custom(features, activation, bn)\n",
        "        \n",
        "        self.skip_add = nn.quantized.FloatFunctional()\n",
        "\n",
        "    def forward(self, *xs):\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Returns:\n",
        "            tensor: output\n",
        "        \"\"\"\n",
        "        output = xs[0]\n",
        "\n",
        "        if len(xs) == 2:\n",
        "            res = self.resConfUnit1(xs[1])\n",
        "            output = self.skip_add.add(output, res)\n",
        "            # output += res\n",
        "\n",
        "        output = self.resConfUnit2(output)\n",
        "\n",
        "        output = nn.functional.interpolate(\n",
        "            output, scale_factor=2, mode=\"bilinear\", align_corners=self.align_corners\n",
        "        )\n",
        "\n",
        "        output = self.out_conv(output)\n",
        "\n",
        "        return output\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoNeLfymJJ64"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MidasNet(BaseModel):\n",
        "    \"\"\"Network for monocular depth estimation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path=None, features=256, non_negative=True):\n",
        "        \"\"\"Init.\n",
        "\n",
        "        Args:\n",
        "            path (str, optional): Path to saved model. Defaults to None.\n",
        "            features (int, optional): Number of features. Defaults to 256.\n",
        "            backbone (str, optional): Backbone network for encoder. Defaults to resnet50\n",
        "        \"\"\"\n",
        "        print(\"Loading weights: \", path)\n",
        "\n",
        "        super(MidasNet, self).__init__()\n",
        "\n",
        "        use_pretrained = False if path is None else True\n",
        "\n",
        "        self.pretrained, self.scratch = _make_encoder(backbone=\"resnext101_wsl\", features=features, use_pretrained=use_pretrained, expand = True)\n",
        "\n",
        "        self.scratch.refinenet4 = FeatureFusionBlock(features)\n",
        "        self.scratch.refinenet3 = FeatureFusionBlock(features)\n",
        "        self.scratch.refinenet2 = FeatureFusionBlock(features)\n",
        "        self.scratch.refinenet1 = FeatureFusionBlock(features)\n",
        "\n",
        "        self.scratch.output_conv = nn.Sequential(\n",
        "            nn.Conv2d(features, 128, kernel_size=3, stride=1, padding=1),\n",
        "            Interpolate(scale_factor=2, mode=\"bilinear\"),\n",
        "            nn.Conv2d(128, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),\n",
        "            nn.ReLU(True) if non_negative else nn.Identity(),\n",
        "        )\n",
        "\n",
        "        if path:\n",
        "            self.load(path)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x (tensor): input data (image)\n",
        "\n",
        "        Returns:\n",
        "            tensor: depth\n",
        "        \"\"\"\n",
        "\n",
        "        layer_1 = self.pretrained.layer1(x)\n",
        "        layer_2 = self.pretrained.layer2(layer_1)\n",
        "        layer_3 = self.pretrained.layer3(layer_2)\n",
        "        layer_4 = self.pretrained.layer4(layer_3)\n",
        "        # print(layer_2.size())\n",
        "        # print(layer_3.size())\n",
        "        # print(layer_4.size())\n",
        "        layer_1_rn = self.scratch.layer1_rn(layer_1)\n",
        "        layer_2_rn = self.scratch.layer2_rn(layer_2)\n",
        "        layer_3_rn = self.scratch.layer3_rn(layer_3)\n",
        "        layer_4_rn = self.scratch.layer4_rn(layer_4)\n",
        "        # print(layer_2_rn.size())\n",
        "        # print(layer_3_rn.size())\n",
        "        # print(layer_4_rn.size())\n",
        "        path_4 = self.scratch.refinenet4(layer_4_rn)\n",
        "        path_3 = self.scratch.refinenet3(path_4, layer_3_rn)\n",
        "        path_2 = self.scratch.refinenet2(path_3, layer_2_rn)\n",
        "        path_1 = self.scratch.refinenet1(path_2, layer_1_rn)\n",
        "\n",
        "        out = self.scratch.output_conv(path_1)\n",
        "\n",
        "        return torch.squeeze(out, dim=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GWvqH4LdL7g"
      },
      "source": [
        "input_path = \"ConstructionPPE/input_images\"\n",
        "output_path = \"output\"\n",
        "# MODEL_PATH = \"model.pt\"\n",
        "model_path = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pv9w905PIig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa36f604-a766-4511-fa72-891482d53af8"
      },
      "source": [
        "model = MidasNet(model_path, non_negative=True).to('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading weights:  None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_WSL-Images_master\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDkVnw3HzIit"
      },
      "source": [
        "# print(model.pretrained.layer1, (3, 416, 416))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9cMFzzH56yk"
      },
      "source": [
        "# print(summary(model.pretrained.layer1, (3, 416, 416)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fimVVHJjBPi0"
      },
      "source": [
        "# print(summary(model.pretrained.layer2, (256, 104, 104)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xj4wD_zYapmM"
      },
      "source": [
        "# print(summary(model.pretrained.layer3, (512, 52, 52)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ueTzeygayav"
      },
      "source": [
        "# print(summary(model.pretrained.layer4, (1024, 26, 26)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUKleb4_6uHi"
      },
      "source": [
        "# print(model.scratch.layer1_rn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctVNmqEU7Xpp"
      },
      "source": [
        "# print(model.scratch.layer2_rn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5LcCBTr8S59"
      },
      "source": [
        "# print(model.scratch.layer3_rn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lVis-GT8W3L"
      },
      "source": [
        "# print(model.scratch.layer4_rn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C-iih_eJvs8"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class YOLO(nn.Module):\n",
        "    def __init__(self, config, backbone, neck, is_training=True):\n",
        "        super(YOLO, self).__init__()\n",
        "        self.config = config\n",
        "        self.training = is_training\n",
        "\n",
        "        self.backbone = backbone\n",
        "        # self.neck = neck\n",
        "        _out_filters = [256, 512, 1024, 2048]\n",
        "        # _out_filters = [128, 256, 512, 1024]\n",
        "        #  embedding0\n",
        "        final_out_filter0 = len(config[\"yolo\"][\"anchors\"][0]) * (5 + config[\"yolo\"][\"classes\"])\n",
        "        self.embedding0 = self._make_embedding([512, 2048], _out_filters[-1], final_out_filter0)\n",
        "        #  embedding1\n",
        "        final_out_filter1 = len(config[\"yolo\"][\"anchors\"][1]) * (5 + config[\"yolo\"][\"classes\"])\n",
        "        self.embedding1_cbl = self._make_cbl(512, 256, 1)\n",
        "        self.embedding1_upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.embedding1 = self._make_embedding([256, 1024], _out_filters[-2] + 256, final_out_filter1)\n",
        "        #  embedding2\n",
        "        final_out_filter2 = len(config[\"yolo\"][\"anchors\"][2]) * (5 + config[\"yolo\"][\"classes\"])\n",
        "        self.embedding2_cbl = self._make_cbl(256, 128, 1)\n",
        "        self.embedding2_upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.embedding2 = self._make_embedding([128, 512], _out_filters[-3] + 128, final_out_filter2)\n",
        "\n",
        "    def _make_cbl(self, _in, _out, ks):\n",
        "        pad = (ks - 1) // 2 if ks else 0\n",
        "        return nn.Sequential(OrderedDict([\n",
        "            (\"conv\", nn.Conv2d(_in, _out, kernel_size=ks, stride=1, padding=pad, bias=False)),\n",
        "            (\"bn\", nn.BatchNorm2d(_out)),\n",
        "            (\"relu\", nn.LeakyReLU(0.1)),\n",
        "        ]))\n",
        "\n",
        "    def _make_embedding(self, filters_list, in_filters, out_filter):\n",
        "        m = nn.ModuleList([\n",
        "            self._make_cbl(in_filters, filters_list[0], 1),\n",
        "            self._make_cbl(filters_list[0], filters_list[1], 3),\n",
        "            self._make_cbl(filters_list[1], filters_list[0], 1),\n",
        "            self._make_cbl(filters_list[0], filters_list[1], 3),\n",
        "            self._make_cbl(filters_list[1], filters_list[0], 1),\n",
        "            self._make_cbl(filters_list[0], filters_list[1], 3)])\n",
        "        m.add_module(\"conv_out\", nn.Conv2d(filters_list[1], out_filter, kernel_size=1,\n",
        "                                           stride=1, padding=0, bias=True))\n",
        "        return m\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        layer_1 = self.backbone.layer1(x)\n",
        "        x2 = self.backbone.layer2(layer_1)\n",
        "        x1 = self.backbone.layer3(x2)\n",
        "        x0 = self.backbone.layer4(x1)\n",
        "        # print(x2.size() , x1.size(), x0.size())\n",
        "\n",
        "        # x2, x1, x0 = torch.split(x2, 256, dim = 1), torch.split(x1, 512, dim = 1), torch.split(x0, 1024, dim = 1)\n",
        "        # x2, x1, x0 = torch.cat(x2, dim = 0), torch.cat(x1, dim = 0), torch.cat(x0, dim = 0)\n",
        "        # print(x2.size() , x1.size(), x0.size())\n",
        "        # x = self.neck.layer1_rn(layer_1)\n",
        "        # x2 = self.neck.layer2_rn(layer_2)\n",
        "        # x1 = self.neck.layer3_rn(layer_3)\n",
        "        # x0 = self.neck.layer4_rn(layer_4)\n",
        "        # print(x2.size())\n",
        "        # print(x1.size())\n",
        "        # print(x0.size())\n",
        "        \n",
        "        def _branch(_embedding, _in):\n",
        "            for i, e in enumerate(_embedding):\n",
        "                _in = e(_in)\n",
        "                if i == 4:\n",
        "                    out_branch = _in\n",
        "            return _in, out_branch\n",
        "        #  backbone\n",
        "        # x2, x1, x0 = self.backbone(x)\n",
        "\n",
        "        # x2, x1, x0 = self.backbone.layer2, self.backbone.layer3, self.backbone.layer4\n",
        "        \n",
        "        #  yolo branch 0\n",
        "        out0, out0_branch = _branch(self.embedding0, x0)\n",
        "        #  yolo branch 1\n",
        "        x1_in = self.embedding1_cbl(out0_branch)\n",
        "        x1_in = self.embedding1_upsample(x1_in)\n",
        "        x1_in = torch.cat([x1_in, x1], 1)\n",
        "        out1, out1_branch = _branch(self.embedding1, x1_in)\n",
        "        #  yolo branch 2\n",
        "        x2_in = self.embedding2_cbl(out1_branch)\n",
        "        x2_in = self.embedding2_upsample(x2_in)\n",
        "        x2_in = torch.cat([x2_in, x2], 1)\n",
        "        out2, out2_branch = _branch(self.embedding2, x2_in)\n",
        "        return out0, out1, out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hw_XPdhIu4y"
      },
      "source": [
        "# _out_filters = [256, 512, 1024, 2048]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_neyXkU1JIo5"
      },
      "source": [
        "config = {\"yolo\": {\n",
        "    \"anchors\": [[[116, 90], [156, 198], [373, 326]],\n",
        "                [[30, 61], [62, 45], [59, 119]],\n",
        "                [[10, 13], [16, 30], [33, 23]]],\n",
        "    \"classes\": 4,\n",
        "}\n",
        ",\n",
        "\"lr\": {\n",
        "        \"backbone_lr\": 0.001,\n",
        "        \"other_lr\": 0.01,\n",
        "        \"freeze_backbone\": True,   #  freeze backbone wegiths to finetune\n",
        "        \"decay_gamma\": 0.1,\n",
        "        \"decay_step\": 20,           #  decay lr in every ? epochs\n",
        "    },\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"sgd\",\n",
        "        \"weight_decay\": 4e-05,\n",
        "    },\n",
        "    \"batch_size\": 4,\n",
        "    \"train_path\": \"../data/coco/trainvalno5k.txt\",\n",
        "    \"epochs\": 100,\n",
        "    \"img_h\": 416,\n",
        "    \"img_w\": 416,\n",
        "    \"parallels\": [0],                         #  config GPU device\n",
        "    \"working_dir\": \"YOUR_WORKING_DIR\",              #  replace with your working dir\n",
        "    \"pretrain_snapshot\": \"\",                        #  load checkpoint\n",
        "    \"evaluate_type\": \"\", \n",
        "    \"try\": 0,\n",
        "    \"export_onnx\": False,\n",
        "\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXec-zyxy_r3"
      },
      "source": [
        "detector = YOLO(config, model.pretrained, model.scratch, False).to('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSDHRlvWtkE2"
      },
      "source": [
        "# print(detector.modules)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfnT4kf4SY11"
      },
      "source": [
        "# print(summary(detector, (3, 416, 416)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg6xzExdyza4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e04a86a8-cbe0-47a5-a196-065e80d4914f"
      },
      "source": [
        "! gdown --id 1SnFAlSvsx37J7MDNs3WWLgeKY0iknikP"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1SnFAlSvsx37J7MDNs3WWLgeKY0iknikP\n",
            "To: /content/MiDaS/official_yolov3_weights_pytorch.pth\n",
            "248MB [00:01, 163MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEv7RhLlSnqj"
      },
      "source": [
        "# print(detector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppU1SqGhxaW1"
      },
      "source": [
        "# for name, param in detector.embedding0.named_parameters():\n",
        "#     if param.requires_grad:\n",
        "#         print (name, param.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI_rVacaGaL4"
      },
      "source": [
        "# detector = nn.DataParallel(detector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4qNjbTJzCrr"
      },
      "source": [
        "state_dict = torch.load('official_yolov3_weights_pytorch.pth', map_location = torch.device('cuda' if torch.cuda.is_available() else 'cpu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtrz3Jo53HVu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82953fe2-736b-4893-f08b-fe34614e44ed"
      },
      "source": [
        "\n",
        "detector.load_state_dict(state_dict, strict = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_IncompatibleKeys(missing_keys=['backbone.layer1.0.weight', 'backbone.layer1.1.weight', 'backbone.layer1.1.bias', 'backbone.layer1.1.running_mean', 'backbone.layer1.1.running_var', 'backbone.layer1.4.0.conv1.weight', 'backbone.layer1.4.0.bn1.weight', 'backbone.layer1.4.0.bn1.bias', 'backbone.layer1.4.0.bn1.running_mean', 'backbone.layer1.4.0.bn1.running_var', 'backbone.layer1.4.0.conv2.weight', 'backbone.layer1.4.0.bn2.weight', 'backbone.layer1.4.0.bn2.bias', 'backbone.layer1.4.0.bn2.running_mean', 'backbone.layer1.4.0.bn2.running_var', 'backbone.layer1.4.0.conv3.weight', 'backbone.layer1.4.0.bn3.weight', 'backbone.layer1.4.0.bn3.bias', 'backbone.layer1.4.0.bn3.running_mean', 'backbone.layer1.4.0.bn3.running_var', 'backbone.layer1.4.0.downsample.0.weight', 'backbone.layer1.4.0.downsample.1.weight', 'backbone.layer1.4.0.downsample.1.bias', 'backbone.layer1.4.0.downsample.1.running_mean', 'backbone.layer1.4.0.downsample.1.running_var', 'backbone.layer1.4.1.conv1.weight', 'backbone.layer1.4.1.bn1.weight', 'backbone.layer1.4.1.bn1.bias', 'backbone.layer1.4.1.bn1.running_mean', 'backbone.layer1.4.1.bn1.running_var', 'backbone.layer1.4.1.conv2.weight', 'backbone.layer1.4.1.bn2.weight', 'backbone.layer1.4.1.bn2.bias', 'backbone.layer1.4.1.bn2.running_mean', 'backbone.layer1.4.1.bn2.running_var', 'backbone.layer1.4.1.conv3.weight', 'backbone.layer1.4.1.bn3.weight', 'backbone.layer1.4.1.bn3.bias', 'backbone.layer1.4.1.bn3.running_mean', 'backbone.layer1.4.1.bn3.running_var', 'backbone.layer1.4.2.conv1.weight', 'backbone.layer1.4.2.bn1.weight', 'backbone.layer1.4.2.bn1.bias', 'backbone.layer1.4.2.bn1.running_mean', 'backbone.layer1.4.2.bn1.running_var', 'backbone.layer1.4.2.conv2.weight', 'backbone.layer1.4.2.bn2.weight', 'backbone.layer1.4.2.bn2.bias', 'backbone.layer1.4.2.bn2.running_mean', 'backbone.layer1.4.2.bn2.running_var', 'backbone.layer1.4.2.conv3.weight', 'backbone.layer1.4.2.bn3.weight', 'backbone.layer1.4.2.bn3.bias', 'backbone.layer1.4.2.bn3.running_mean', 'backbone.layer1.4.2.bn3.running_var', 'backbone.layer2.0.conv1.weight', 'backbone.layer2.0.bn1.weight', 'backbone.layer2.0.bn1.bias', 'backbone.layer2.0.bn1.running_mean', 'backbone.layer2.0.bn1.running_var', 'backbone.layer2.0.conv2.weight', 'backbone.layer2.0.bn2.weight', 'backbone.layer2.0.bn2.bias', 'backbone.layer2.0.bn2.running_mean', 'backbone.layer2.0.bn2.running_var', 'backbone.layer2.0.conv3.weight', 'backbone.layer2.0.bn3.weight', 'backbone.layer2.0.bn3.bias', 'backbone.layer2.0.bn3.running_mean', 'backbone.layer2.0.bn3.running_var', 'backbone.layer2.0.downsample.0.weight', 'backbone.layer2.0.downsample.1.weight', 'backbone.layer2.0.downsample.1.bias', 'backbone.layer2.0.downsample.1.running_mean', 'backbone.layer2.0.downsample.1.running_var', 'backbone.layer2.1.conv1.weight', 'backbone.layer2.1.bn1.weight', 'backbone.layer2.1.bn1.bias', 'backbone.layer2.1.bn1.running_mean', 'backbone.layer2.1.bn1.running_var', 'backbone.layer2.1.conv2.weight', 'backbone.layer2.1.bn2.weight', 'backbone.layer2.1.bn2.bias', 'backbone.layer2.1.bn2.running_mean', 'backbone.layer2.1.bn2.running_var', 'backbone.layer2.1.conv3.weight', 'backbone.layer2.1.bn3.weight', 'backbone.layer2.1.bn3.bias', 'backbone.layer2.1.bn3.running_mean', 'backbone.layer2.1.bn3.running_var', 'backbone.layer2.2.conv1.weight', 'backbone.layer2.2.bn1.weight', 'backbone.layer2.2.bn1.bias', 'backbone.layer2.2.bn1.running_mean', 'backbone.layer2.2.bn1.running_var', 'backbone.layer2.2.conv2.weight', 'backbone.layer2.2.bn2.weight', 'backbone.layer2.2.bn2.bias', 'backbone.layer2.2.bn2.running_mean', 'backbone.layer2.2.bn2.running_var', 'backbone.layer2.2.conv3.weight', 'backbone.layer2.2.bn3.weight', 'backbone.layer2.2.bn3.bias', 'backbone.layer2.2.bn3.running_mean', 'backbone.layer2.2.bn3.running_var', 'backbone.layer2.3.conv1.weight', 'backbone.layer2.3.bn1.weight', 'backbone.layer2.3.bn1.bias', 'backbone.layer2.3.bn1.running_mean', 'backbone.layer2.3.bn1.running_var', 'backbone.layer2.3.conv2.weight', 'backbone.layer2.3.bn2.weight', 'backbone.layer2.3.bn2.bias', 'backbone.layer2.3.bn2.running_mean', 'backbone.layer2.3.bn2.running_var', 'backbone.layer2.3.conv3.weight', 'backbone.layer2.3.bn3.weight', 'backbone.layer2.3.bn3.bias', 'backbone.layer2.3.bn3.running_mean', 'backbone.layer2.3.bn3.running_var', 'backbone.layer3.0.conv1.weight', 'backbone.layer3.0.bn1.weight', 'backbone.layer3.0.bn1.bias', 'backbone.layer3.0.bn1.running_mean', 'backbone.layer3.0.bn1.running_var', 'backbone.layer3.0.conv2.weight', 'backbone.layer3.0.bn2.weight', 'backbone.layer3.0.bn2.bias', 'backbone.layer3.0.bn2.running_mean', 'backbone.layer3.0.bn2.running_var', 'backbone.layer3.0.conv3.weight', 'backbone.layer3.0.bn3.weight', 'backbone.layer3.0.bn3.bias', 'backbone.layer3.0.bn3.running_mean', 'backbone.layer3.0.bn3.running_var', 'backbone.layer3.0.downsample.0.weight', 'backbone.layer3.0.downsample.1.weight', 'backbone.layer3.0.downsample.1.bias', 'backbone.layer3.0.downsample.1.running_mean', 'backbone.layer3.0.downsample.1.running_var', 'backbone.layer3.1.conv1.weight', 'backbone.layer3.1.bn1.weight', 'backbone.layer3.1.bn1.bias', 'backbone.layer3.1.bn1.running_mean', 'backbone.layer3.1.bn1.running_var', 'backbone.layer3.1.conv2.weight', 'backbone.layer3.1.bn2.weight', 'backbone.layer3.1.bn2.bias', 'backbone.layer3.1.bn2.running_mean', 'backbone.layer3.1.bn2.running_var', 'backbone.layer3.1.conv3.weight', 'backbone.layer3.1.bn3.weight', 'backbone.layer3.1.bn3.bias', 'backbone.layer3.1.bn3.running_mean', 'backbone.layer3.1.bn3.running_var', 'backbone.layer3.2.conv1.weight', 'backbone.layer3.2.bn1.weight', 'backbone.layer3.2.bn1.bias', 'backbone.layer3.2.bn1.running_mean', 'backbone.layer3.2.bn1.running_var', 'backbone.layer3.2.conv2.weight', 'backbone.layer3.2.bn2.weight', 'backbone.layer3.2.bn2.bias', 'backbone.layer3.2.bn2.running_mean', 'backbone.layer3.2.bn2.running_var', 'backbone.layer3.2.conv3.weight', 'backbone.layer3.2.bn3.weight', 'backbone.layer3.2.bn3.bias', 'backbone.layer3.2.bn3.running_mean', 'backbone.layer3.2.bn3.running_var', 'backbone.layer3.3.conv1.weight', 'backbone.layer3.3.bn1.weight', 'backbone.layer3.3.bn1.bias', 'backbone.layer3.3.bn1.running_mean', 'backbone.layer3.3.bn1.running_var', 'backbone.layer3.3.conv2.weight', 'backbone.layer3.3.bn2.weight', 'backbone.layer3.3.bn2.bias', 'backbone.layer3.3.bn2.running_mean', 'backbone.layer3.3.bn2.running_var', 'backbone.layer3.3.conv3.weight', 'backbone.layer3.3.bn3.weight', 'backbone.layer3.3.bn3.bias', 'backbone.layer3.3.bn3.running_mean', 'backbone.layer3.3.bn3.running_var', 'backbone.layer3.4.conv1.weight', 'backbone.layer3.4.bn1.weight', 'backbone.layer3.4.bn1.bias', 'backbone.layer3.4.bn1.running_mean', 'backbone.layer3.4.bn1.running_var', 'backbone.layer3.4.conv2.weight', 'backbone.layer3.4.bn2.weight', 'backbone.layer3.4.bn2.bias', 'backbone.layer3.4.bn2.running_mean', 'backbone.layer3.4.bn2.running_var', 'backbone.layer3.4.conv3.weight', 'backbone.layer3.4.bn3.weight', 'backbone.layer3.4.bn3.bias', 'backbone.layer3.4.bn3.running_mean', 'backbone.layer3.4.bn3.running_var', 'backbone.layer3.5.conv1.weight', 'backbone.layer3.5.bn1.weight', 'backbone.layer3.5.bn1.bias', 'backbone.layer3.5.bn1.running_mean', 'backbone.layer3.5.bn1.running_var', 'backbone.layer3.5.conv2.weight', 'backbone.layer3.5.bn2.weight', 'backbone.layer3.5.bn2.bias', 'backbone.layer3.5.bn2.running_mean', 'backbone.layer3.5.bn2.running_var', 'backbone.layer3.5.conv3.weight', 'backbone.layer3.5.bn3.weight', 'backbone.layer3.5.bn3.bias', 'backbone.layer3.5.bn3.running_mean', 'backbone.layer3.5.bn3.running_var', 'backbone.layer3.6.conv1.weight', 'backbone.layer3.6.bn1.weight', 'backbone.layer3.6.bn1.bias', 'backbone.layer3.6.bn1.running_mean', 'backbone.layer3.6.bn1.running_var', 'backbone.layer3.6.conv2.weight', 'backbone.layer3.6.bn2.weight', 'backbone.layer3.6.bn2.bias', 'backbone.layer3.6.bn2.running_mean', 'backbone.layer3.6.bn2.running_var', 'backbone.layer3.6.conv3.weight', 'backbone.layer3.6.bn3.weight', 'backbone.layer3.6.bn3.bias', 'backbone.layer3.6.bn3.running_mean', 'backbone.layer3.6.bn3.running_var', 'backbone.layer3.7.conv1.weight', 'backbone.layer3.7.bn1.weight', 'backbone.layer3.7.bn1.bias', 'backbone.layer3.7.bn1.running_mean', 'backbone.layer3.7.bn1.running_var', 'backbone.layer3.7.conv2.weight', 'backbone.layer3.7.bn2.weight', 'backbone.layer3.7.bn2.bias', 'backbone.layer3.7.bn2.running_mean', 'backbone.layer3.7.bn2.running_var', 'backbone.layer3.7.conv3.weight', 'backbone.layer3.7.bn3.weight', 'backbone.layer3.7.bn3.bias', 'backbone.layer3.7.bn3.running_mean', 'backbone.layer3.7.bn3.running_var', 'backbone.layer3.8.conv1.weight', 'backbone.layer3.8.bn1.weight', 'backbone.layer3.8.bn1.bias', 'backbone.layer3.8.bn1.running_mean', 'backbone.layer3.8.bn1.running_var', 'backbone.layer3.8.conv2.weight', 'backbone.layer3.8.bn2.weight', 'backbone.layer3.8.bn2.bias', 'backbone.layer3.8.bn2.running_mean', 'backbone.layer3.8.bn2.running_var', 'backbone.layer3.8.conv3.weight', 'backbone.layer3.8.bn3.weight', 'backbone.layer3.8.bn3.bias', 'backbone.layer3.8.bn3.running_mean', 'backbone.layer3.8.bn3.running_var', 'backbone.layer3.9.conv1.weight', 'backbone.layer3.9.bn1.weight', 'backbone.layer3.9.bn1.bias', 'backbone.layer3.9.bn1.running_mean', 'backbone.layer3.9.bn1.running_var', 'backbone.layer3.9.conv2.weight', 'backbone.layer3.9.bn2.weight', 'backbone.layer3.9.bn2.bias', 'backbone.layer3.9.bn2.running_mean', 'backbone.layer3.9.bn2.running_var', 'backbone.layer3.9.conv3.weight', 'backbone.layer3.9.bn3.weight', 'backbone.layer3.9.bn3.bias', 'backbone.layer3.9.bn3.running_mean', 'backbone.layer3.9.bn3.running_var', 'backbone.layer3.10.conv1.weight', 'backbone.layer3.10.bn1.weight', 'backbone.layer3.10.bn1.bias', 'backbone.layer3.10.bn1.running_mean', 'backbone.layer3.10.bn1.running_var', 'backbone.layer3.10.conv2.weight', 'backbone.layer3.10.bn2.weight', 'backbone.layer3.10.bn2.bias', 'backbone.layer3.10.bn2.running_mean', 'backbone.layer3.10.bn2.running_var', 'backbone.layer3.10.conv3.weight', 'backbone.layer3.10.bn3.weight', 'backbone.layer3.10.bn3.bias', 'backbone.layer3.10.bn3.running_mean', 'backbone.layer3.10.bn3.running_var', 'backbone.layer3.11.conv1.weight', 'backbone.layer3.11.bn1.weight', 'backbone.layer3.11.bn1.bias', 'backbone.layer3.11.bn1.running_mean', 'backbone.layer3.11.bn1.running_var', 'backbone.layer3.11.conv2.weight', 'backbone.layer3.11.bn2.weight', 'backbone.layer3.11.bn2.bias', 'backbone.layer3.11.bn2.running_mean', 'backbone.layer3.11.bn2.running_var', 'backbone.layer3.11.conv3.weight', 'backbone.layer3.11.bn3.weight', 'backbone.layer3.11.bn3.bias', 'backbone.layer3.11.bn3.running_mean', 'backbone.layer3.11.bn3.running_var', 'backbone.layer3.12.conv1.weight', 'backbone.layer3.12.bn1.weight', 'backbone.layer3.12.bn1.bias', 'backbone.layer3.12.bn1.running_mean', 'backbone.layer3.12.bn1.running_var', 'backbone.layer3.12.conv2.weight', 'backbone.layer3.12.bn2.weight', 'backbone.layer3.12.bn2.bias', 'backbone.layer3.12.bn2.running_mean', 'backbone.layer3.12.bn2.running_var', 'backbone.layer3.12.conv3.weight', 'backbone.layer3.12.bn3.weight', 'backbone.layer3.12.bn3.bias', 'backbone.layer3.12.bn3.running_mean', 'backbone.layer3.12.bn3.running_var', 'backbone.layer3.13.conv1.weight', 'backbone.layer3.13.bn1.weight', 'backbone.layer3.13.bn1.bias', 'backbone.layer3.13.bn1.running_mean', 'backbone.layer3.13.bn1.running_var', 'backbone.layer3.13.conv2.weight', 'backbone.layer3.13.bn2.weight', 'backbone.layer3.13.bn2.bias', 'backbone.layer3.13.bn2.running_mean', 'backbone.layer3.13.bn2.running_var', 'backbone.layer3.13.conv3.weight', 'backbone.layer3.13.bn3.weight', 'backbone.layer3.13.bn3.bias', 'backbone.layer3.13.bn3.running_mean', 'backbone.layer3.13.bn3.running_var', 'backbone.layer3.14.conv1.weight', 'backbone.layer3.14.bn1.weight', 'backbone.layer3.14.bn1.bias', 'backbone.layer3.14.bn1.running_mean', 'backbone.layer3.14.bn1.running_var', 'backbone.layer3.14.conv2.weight', 'backbone.layer3.14.bn2.weight', 'backbone.layer3.14.bn2.bias', 'backbone.layer3.14.bn2.running_mean', 'backbone.layer3.14.bn2.running_var', 'backbone.layer3.14.conv3.weight', 'backbone.layer3.14.bn3.weight', 'backbone.layer3.14.bn3.bias', 'backbone.layer3.14.bn3.running_mean', 'backbone.layer3.14.bn3.running_var', 'backbone.layer3.15.conv1.weight', 'backbone.layer3.15.bn1.weight', 'backbone.layer3.15.bn1.bias', 'backbone.layer3.15.bn1.running_mean', 'backbone.layer3.15.bn1.running_var', 'backbone.layer3.15.conv2.weight', 'backbone.layer3.15.bn2.weight', 'backbone.layer3.15.bn2.bias', 'backbone.layer3.15.bn2.running_mean', 'backbone.layer3.15.bn2.running_var', 'backbone.layer3.15.conv3.weight', 'backbone.layer3.15.bn3.weight', 'backbone.layer3.15.bn3.bias', 'backbone.layer3.15.bn3.running_mean', 'backbone.layer3.15.bn3.running_var', 'backbone.layer3.16.conv1.weight', 'backbone.layer3.16.bn1.weight', 'backbone.layer3.16.bn1.bias', 'backbone.layer3.16.bn1.running_mean', 'backbone.layer3.16.bn1.running_var', 'backbone.layer3.16.conv2.weight', 'backbone.layer3.16.bn2.weight', 'backbone.layer3.16.bn2.bias', 'backbone.layer3.16.bn2.running_mean', 'backbone.layer3.16.bn2.running_var', 'backbone.layer3.16.conv3.weight', 'backbone.layer3.16.bn3.weight', 'backbone.layer3.16.bn3.bias', 'backbone.layer3.16.bn3.running_mean', 'backbone.layer3.16.bn3.running_var', 'backbone.layer3.17.conv1.weight', 'backbone.layer3.17.bn1.weight', 'backbone.layer3.17.bn1.bias', 'backbone.layer3.17.bn1.running_mean', 'backbone.layer3.17.bn1.running_var', 'backbone.layer3.17.conv2.weight', 'backbone.layer3.17.bn2.weight', 'backbone.layer3.17.bn2.bias', 'backbone.layer3.17.bn2.running_mean', 'backbone.layer3.17.bn2.running_var', 'backbone.layer3.17.conv3.weight', 'backbone.layer3.17.bn3.weight', 'backbone.layer3.17.bn3.bias', 'backbone.layer3.17.bn3.running_mean', 'backbone.layer3.17.bn3.running_var', 'backbone.layer3.18.conv1.weight', 'backbone.layer3.18.bn1.weight', 'backbone.layer3.18.bn1.bias', 'backbone.layer3.18.bn1.running_mean', 'backbone.layer3.18.bn1.running_var', 'backbone.layer3.18.conv2.weight', 'backbone.layer3.18.bn2.weight', 'backbone.layer3.18.bn2.bias', 'backbone.layer3.18.bn2.running_mean', 'backbone.layer3.18.bn2.running_var', 'backbone.layer3.18.conv3.weight', 'backbone.layer3.18.bn3.weight', 'backbone.layer3.18.bn3.bias', 'backbone.layer3.18.bn3.running_mean', 'backbone.layer3.18.bn3.running_var', 'backbone.layer3.19.conv1.weight', 'backbone.layer3.19.bn1.weight', 'backbone.layer3.19.bn1.bias', 'backbone.layer3.19.bn1.running_mean', 'backbone.layer3.19.bn1.running_var', 'backbone.layer3.19.conv2.weight', 'backbone.layer3.19.bn2.weight', 'backbone.layer3.19.bn2.bias', 'backbone.layer3.19.bn2.running_mean', 'backbone.layer3.19.bn2.running_var', 'backbone.layer3.19.conv3.weight', 'backbone.layer3.19.bn3.weight', 'backbone.layer3.19.bn3.bias', 'backbone.layer3.19.bn3.running_mean', 'backbone.layer3.19.bn3.running_var', 'backbone.layer3.20.conv1.weight', 'backbone.layer3.20.bn1.weight', 'backbone.layer3.20.bn1.bias', 'backbone.layer3.20.bn1.running_mean', 'backbone.layer3.20.bn1.running_var', 'backbone.layer3.20.conv2.weight', 'backbone.layer3.20.bn2.weight', 'backbone.layer3.20.bn2.bias', 'backbone.layer3.20.bn2.running_mean', 'backbone.layer3.20.bn2.running_var', 'backbone.layer3.20.conv3.weight', 'backbone.layer3.20.bn3.weight', 'backbone.layer3.20.bn3.bias', 'backbone.layer3.20.bn3.running_mean', 'backbone.layer3.20.bn3.running_var', 'backbone.layer3.21.conv1.weight', 'backbone.layer3.21.bn1.weight', 'backbone.layer3.21.bn1.bias', 'backbone.layer3.21.bn1.running_mean', 'backbone.layer3.21.bn1.running_var', 'backbone.layer3.21.conv2.weight', 'backbone.layer3.21.bn2.weight', 'backbone.layer3.21.bn2.bias', 'backbone.layer3.21.bn2.running_mean', 'backbone.layer3.21.bn2.running_var', 'backbone.layer3.21.conv3.weight', 'backbone.layer3.21.bn3.weight', 'backbone.layer3.21.bn3.bias', 'backbone.layer3.21.bn3.running_mean', 'backbone.layer3.21.bn3.running_var', 'backbone.layer3.22.conv1.weight', 'backbone.layer3.22.bn1.weight', 'backbone.layer3.22.bn1.bias', 'backbone.layer3.22.bn1.running_mean', 'backbone.layer3.22.bn1.running_var', 'backbone.layer3.22.conv2.weight', 'backbone.layer3.22.bn2.weight', 'backbone.layer3.22.bn2.bias', 'backbone.layer3.22.bn2.running_mean', 'backbone.layer3.22.bn2.running_var', 'backbone.layer3.22.conv3.weight', 'backbone.layer3.22.bn3.weight', 'backbone.layer3.22.bn3.bias', 'backbone.layer3.22.bn3.running_mean', 'backbone.layer3.22.bn3.running_var', 'backbone.layer4.0.conv1.weight', 'backbone.layer4.0.bn1.weight', 'backbone.layer4.0.bn1.bias', 'backbone.layer4.0.bn1.running_mean', 'backbone.layer4.0.bn1.running_var', 'backbone.layer4.0.conv2.weight', 'backbone.layer4.0.bn2.weight', 'backbone.layer4.0.bn2.bias', 'backbone.layer4.0.bn2.running_mean', 'backbone.layer4.0.bn2.running_var', 'backbone.layer4.0.conv3.weight', 'backbone.layer4.0.bn3.weight', 'backbone.layer4.0.bn3.bias', 'backbone.layer4.0.bn3.running_mean', 'backbone.layer4.0.bn3.running_var', 'backbone.layer4.0.downsample.0.weight', 'backbone.layer4.0.downsample.1.weight', 'backbone.layer4.0.downsample.1.bias', 'backbone.layer4.0.downsample.1.running_mean', 'backbone.layer4.0.downsample.1.running_var', 'backbone.layer4.1.conv1.weight', 'backbone.layer4.1.bn1.weight', 'backbone.layer4.1.bn1.bias', 'backbone.layer4.1.bn1.running_mean', 'backbone.layer4.1.bn1.running_var', 'backbone.layer4.1.conv2.weight', 'backbone.layer4.1.bn2.weight', 'backbone.layer4.1.bn2.bias', 'backbone.layer4.1.bn2.running_mean', 'backbone.layer4.1.bn2.running_var', 'backbone.layer4.1.conv3.weight', 'backbone.layer4.1.bn3.weight', 'backbone.layer4.1.bn3.bias', 'backbone.layer4.1.bn3.running_mean', 'backbone.layer4.1.bn3.running_var', 'backbone.layer4.2.conv1.weight', 'backbone.layer4.2.bn1.weight', 'backbone.layer4.2.bn1.bias', 'backbone.layer4.2.bn1.running_mean', 'backbone.layer4.2.bn1.running_var', 'backbone.layer4.2.conv2.weight', 'backbone.layer4.2.bn2.weight', 'backbone.layer4.2.bn2.bias', 'backbone.layer4.2.bn2.running_mean', 'backbone.layer4.2.bn2.running_var', 'backbone.layer4.2.conv3.weight', 'backbone.layer4.2.bn3.weight', 'backbone.layer4.2.bn3.bias', 'backbone.layer4.2.bn3.running_mean', 'backbone.layer4.2.bn3.running_var', 'embedding0.0.conv.weight', 'embedding0.0.bn.weight', 'embedding0.0.bn.bias', 'embedding0.0.bn.running_mean', 'embedding0.0.bn.running_var', 'embedding0.1.conv.weight', 'embedding0.1.bn.weight', 'embedding0.1.bn.bias', 'embedding0.1.bn.running_mean', 'embedding0.1.bn.running_var', 'embedding0.2.conv.weight', 'embedding0.2.bn.weight', 'embedding0.2.bn.bias', 'embedding0.2.bn.running_mean', 'embedding0.2.bn.running_var', 'embedding0.3.conv.weight', 'embedding0.3.bn.weight', 'embedding0.3.bn.bias', 'embedding0.3.bn.running_mean', 'embedding0.3.bn.running_var', 'embedding0.4.conv.weight', 'embedding0.4.bn.weight', 'embedding0.4.bn.bias', 'embedding0.4.bn.running_mean', 'embedding0.4.bn.running_var', 'embedding0.5.conv.weight', 'embedding0.5.bn.weight', 'embedding0.5.bn.bias', 'embedding0.5.bn.running_mean', 'embedding0.5.bn.running_var', 'embedding0.conv_out.weight', 'embedding0.conv_out.bias', 'embedding1_cbl.conv.weight', 'embedding1_cbl.bn.weight', 'embedding1_cbl.bn.bias', 'embedding1_cbl.bn.running_mean', 'embedding1_cbl.bn.running_var', 'embedding1.0.conv.weight', 'embedding1.0.bn.weight', 'embedding1.0.bn.bias', 'embedding1.0.bn.running_mean', 'embedding1.0.bn.running_var', 'embedding1.1.conv.weight', 'embedding1.1.bn.weight', 'embedding1.1.bn.bias', 'embedding1.1.bn.running_mean', 'embedding1.1.bn.running_var', 'embedding1.2.conv.weight', 'embedding1.2.bn.weight', 'embedding1.2.bn.bias', 'embedding1.2.bn.running_mean', 'embedding1.2.bn.running_var', 'embedding1.3.conv.weight', 'embedding1.3.bn.weight', 'embedding1.3.bn.bias', 'embedding1.3.bn.running_mean', 'embedding1.3.bn.running_var', 'embedding1.4.conv.weight', 'embedding1.4.bn.weight', 'embedding1.4.bn.bias', 'embedding1.4.bn.running_mean', 'embedding1.4.bn.running_var', 'embedding1.5.conv.weight', 'embedding1.5.bn.weight', 'embedding1.5.bn.bias', 'embedding1.5.bn.running_mean', 'embedding1.5.bn.running_var', 'embedding1.conv_out.weight', 'embedding1.conv_out.bias', 'embedding2_cbl.conv.weight', 'embedding2_cbl.bn.weight', 'embedding2_cbl.bn.bias', 'embedding2_cbl.bn.running_mean', 'embedding2_cbl.bn.running_var', 'embedding2.0.conv.weight', 'embedding2.0.bn.weight', 'embedding2.0.bn.bias', 'embedding2.0.bn.running_mean', 'embedding2.0.bn.running_var', 'embedding2.1.conv.weight', 'embedding2.1.bn.weight', 'embedding2.1.bn.bias', 'embedding2.1.bn.running_mean', 'embedding2.1.bn.running_var', 'embedding2.2.conv.weight', 'embedding2.2.bn.weight', 'embedding2.2.bn.bias', 'embedding2.2.bn.running_mean', 'embedding2.2.bn.running_var', 'embedding2.3.conv.weight', 'embedding2.3.bn.weight', 'embedding2.3.bn.bias', 'embedding2.3.bn.running_mean', 'embedding2.3.bn.running_var', 'embedding2.4.conv.weight', 'embedding2.4.bn.weight', 'embedding2.4.bn.bias', 'embedding2.4.bn.running_mean', 'embedding2.4.bn.running_var', 'embedding2.5.conv.weight', 'embedding2.5.bn.weight', 'embedding2.5.bn.bias', 'embedding2.5.bn.running_mean', 'embedding2.5.bn.running_var', 'embedding2.conv_out.weight', 'embedding2.conv_out.bias'], unexpected_keys=['module.backbone.conv1.weight', 'module.backbone.bn1.weight', 'module.backbone.bn1.bias', 'module.backbone.bn1.running_mean', 'module.backbone.bn1.running_var', 'module.backbone.layer1.ds_conv.weight', 'module.backbone.layer1.ds_bn.weight', 'module.backbone.layer1.ds_bn.bias', 'module.backbone.layer1.ds_bn.running_mean', 'module.backbone.layer1.ds_bn.running_var', 'module.backbone.layer1.residual_0.conv1.weight', 'module.backbone.layer1.residual_0.bn1.weight', 'module.backbone.layer1.residual_0.bn1.bias', 'module.backbone.layer1.residual_0.bn1.running_mean', 'module.backbone.layer1.residual_0.bn1.running_var', 'module.backbone.layer1.residual_0.conv2.weight', 'module.backbone.layer1.residual_0.bn2.weight', 'module.backbone.layer1.residual_0.bn2.bias', 'module.backbone.layer1.residual_0.bn2.running_mean', 'module.backbone.layer1.residual_0.bn2.running_var', 'module.backbone.layer2.ds_conv.weight', 'module.backbone.layer2.ds_bn.weight', 'module.backbone.layer2.ds_bn.bias', 'module.backbone.layer2.ds_bn.running_mean', 'module.backbone.layer2.ds_bn.running_var', 'module.backbone.layer2.residual_0.conv1.weight', 'module.backbone.layer2.residual_0.bn1.weight', 'module.backbone.layer2.residual_0.bn1.bias', 'module.backbone.layer2.residual_0.bn1.running_mean', 'module.backbone.layer2.residual_0.bn1.running_var', 'module.backbone.layer2.residual_0.conv2.weight', 'module.backbone.layer2.residual_0.bn2.weight', 'module.backbone.layer2.residual_0.bn2.bias', 'module.backbone.layer2.residual_0.bn2.running_mean', 'module.backbone.layer2.residual_0.bn2.running_var', 'module.backbone.layer2.residual_1.conv1.weight', 'module.backbone.layer2.residual_1.bn1.weight', 'module.backbone.layer2.residual_1.bn1.bias', 'module.backbone.layer2.residual_1.bn1.running_mean', 'module.backbone.layer2.residual_1.bn1.running_var', 'module.backbone.layer2.residual_1.conv2.weight', 'module.backbone.layer2.residual_1.bn2.weight', 'module.backbone.layer2.residual_1.bn2.bias', 'module.backbone.layer2.residual_1.bn2.running_mean', 'module.backbone.layer2.residual_1.bn2.running_var', 'module.backbone.layer3.ds_conv.weight', 'module.backbone.layer3.ds_bn.weight', 'module.backbone.layer3.ds_bn.bias', 'module.backbone.layer3.ds_bn.running_mean', 'module.backbone.layer3.ds_bn.running_var', 'module.backbone.layer3.residual_0.conv1.weight', 'module.backbone.layer3.residual_0.bn1.weight', 'module.backbone.layer3.residual_0.bn1.bias', 'module.backbone.layer3.residual_0.bn1.running_mean', 'module.backbone.layer3.residual_0.bn1.running_var', 'module.backbone.layer3.residual_0.conv2.weight', 'module.backbone.layer3.residual_0.bn2.weight', 'module.backbone.layer3.residual_0.bn2.bias', 'module.backbone.layer3.residual_0.bn2.running_mean', 'module.backbone.layer3.residual_0.bn2.running_var', 'module.backbone.layer3.residual_1.conv1.weight', 'module.backbone.layer3.residual_1.bn1.weight', 'module.backbone.layer3.residual_1.bn1.bias', 'module.backbone.layer3.residual_1.bn1.running_mean', 'module.backbone.layer3.residual_1.bn1.running_var', 'module.backbone.layer3.residual_1.conv2.weight', 'module.backbone.layer3.residual_1.bn2.weight', 'module.backbone.layer3.residual_1.bn2.bias', 'module.backbone.layer3.residual_1.bn2.running_mean', 'module.backbone.layer3.residual_1.bn2.running_var', 'module.backbone.layer3.residual_2.conv1.weight', 'module.backbone.layer3.residual_2.bn1.weight', 'module.backbone.layer3.residual_2.bn1.bias', 'module.backbone.layer3.residual_2.bn1.running_mean', 'module.backbone.layer3.residual_2.bn1.running_var', 'module.backbone.layer3.residual_2.conv2.weight', 'module.backbone.layer3.residual_2.bn2.weight', 'module.backbone.layer3.residual_2.bn2.bias', 'module.backbone.layer3.residual_2.bn2.running_mean', 'module.backbone.layer3.residual_2.bn2.running_var', 'module.backbone.layer3.residual_3.conv1.weight', 'module.backbone.layer3.residual_3.bn1.weight', 'module.backbone.layer3.residual_3.bn1.bias', 'module.backbone.layer3.residual_3.bn1.running_mean', 'module.backbone.layer3.residual_3.bn1.running_var', 'module.backbone.layer3.residual_3.conv2.weight', 'module.backbone.layer3.residual_3.bn2.weight', 'module.backbone.layer3.residual_3.bn2.bias', 'module.backbone.layer3.residual_3.bn2.running_mean', 'module.backbone.layer3.residual_3.bn2.running_var', 'module.backbone.layer3.residual_4.conv1.weight', 'module.backbone.layer3.residual_4.bn1.weight', 'module.backbone.layer3.residual_4.bn1.bias', 'module.backbone.layer3.residual_4.bn1.running_mean', 'module.backbone.layer3.residual_4.bn1.running_var', 'module.backbone.layer3.residual_4.conv2.weight', 'module.backbone.layer3.residual_4.bn2.weight', 'module.backbone.layer3.residual_4.bn2.bias', 'module.backbone.layer3.residual_4.bn2.running_mean', 'module.backbone.layer3.residual_4.bn2.running_var', 'module.backbone.layer3.residual_5.conv1.weight', 'module.backbone.layer3.residual_5.bn1.weight', 'module.backbone.layer3.residual_5.bn1.bias', 'module.backbone.layer3.residual_5.bn1.running_mean', 'module.backbone.layer3.residual_5.bn1.running_var', 'module.backbone.layer3.residual_5.conv2.weight', 'module.backbone.layer3.residual_5.bn2.weight', 'module.backbone.layer3.residual_5.bn2.bias', 'module.backbone.layer3.residual_5.bn2.running_mean', 'module.backbone.layer3.residual_5.bn2.running_var', 'module.backbone.layer3.residual_6.conv1.weight', 'module.backbone.layer3.residual_6.bn1.weight', 'module.backbone.layer3.residual_6.bn1.bias', 'module.backbone.layer3.residual_6.bn1.running_mean', 'module.backbone.layer3.residual_6.bn1.running_var', 'module.backbone.layer3.residual_6.conv2.weight', 'module.backbone.layer3.residual_6.bn2.weight', 'module.backbone.layer3.residual_6.bn2.bias', 'module.backbone.layer3.residual_6.bn2.running_mean', 'module.backbone.layer3.residual_6.bn2.running_var', 'module.backbone.layer3.residual_7.conv1.weight', 'module.backbone.layer3.residual_7.bn1.weight', 'module.backbone.layer3.residual_7.bn1.bias', 'module.backbone.layer3.residual_7.bn1.running_mean', 'module.backbone.layer3.residual_7.bn1.running_var', 'module.backbone.layer3.residual_7.conv2.weight', 'module.backbone.layer3.residual_7.bn2.weight', 'module.backbone.layer3.residual_7.bn2.bias', 'module.backbone.layer3.residual_7.bn2.running_mean', 'module.backbone.layer3.residual_7.bn2.running_var', 'module.backbone.layer4.ds_conv.weight', 'module.backbone.layer4.ds_bn.weight', 'module.backbone.layer4.ds_bn.bias', 'module.backbone.layer4.ds_bn.running_mean', 'module.backbone.layer4.ds_bn.running_var', 'module.backbone.layer4.residual_0.conv1.weight', 'module.backbone.layer4.residual_0.bn1.weight', 'module.backbone.layer4.residual_0.bn1.bias', 'module.backbone.layer4.residual_0.bn1.running_mean', 'module.backbone.layer4.residual_0.bn1.running_var', 'module.backbone.layer4.residual_0.conv2.weight', 'module.backbone.layer4.residual_0.bn2.weight', 'module.backbone.layer4.residual_0.bn2.bias', 'module.backbone.layer4.residual_0.bn2.running_mean', 'module.backbone.layer4.residual_0.bn2.running_var', 'module.backbone.layer4.residual_1.conv1.weight', 'module.backbone.layer4.residual_1.bn1.weight', 'module.backbone.layer4.residual_1.bn1.bias', 'module.backbone.layer4.residual_1.bn1.running_mean', 'module.backbone.layer4.residual_1.bn1.running_var', 'module.backbone.layer4.residual_1.conv2.weight', 'module.backbone.layer4.residual_1.bn2.weight', 'module.backbone.layer4.residual_1.bn2.bias', 'module.backbone.layer4.residual_1.bn2.running_mean', 'module.backbone.layer4.residual_1.bn2.running_var', 'module.backbone.layer4.residual_2.conv1.weight', 'module.backbone.layer4.residual_2.bn1.weight', 'module.backbone.layer4.residual_2.bn1.bias', 'module.backbone.layer4.residual_2.bn1.running_mean', 'module.backbone.layer4.residual_2.bn1.running_var', 'module.backbone.layer4.residual_2.conv2.weight', 'module.backbone.layer4.residual_2.bn2.weight', 'module.backbone.layer4.residual_2.bn2.bias', 'module.backbone.layer4.residual_2.bn2.running_mean', 'module.backbone.layer4.residual_2.bn2.running_var', 'module.backbone.layer4.residual_3.conv1.weight', 'module.backbone.layer4.residual_3.bn1.weight', 'module.backbone.layer4.residual_3.bn1.bias', 'module.backbone.layer4.residual_3.bn1.running_mean', 'module.backbone.layer4.residual_3.bn1.running_var', 'module.backbone.layer4.residual_3.conv2.weight', 'module.backbone.layer4.residual_3.bn2.weight', 'module.backbone.layer4.residual_3.bn2.bias', 'module.backbone.layer4.residual_3.bn2.running_mean', 'module.backbone.layer4.residual_3.bn2.running_var', 'module.backbone.layer4.residual_4.conv1.weight', 'module.backbone.layer4.residual_4.bn1.weight', 'module.backbone.layer4.residual_4.bn1.bias', 'module.backbone.layer4.residual_4.bn1.running_mean', 'module.backbone.layer4.residual_4.bn1.running_var', 'module.backbone.layer4.residual_4.conv2.weight', 'module.backbone.layer4.residual_4.bn2.weight', 'module.backbone.layer4.residual_4.bn2.bias', 'module.backbone.layer4.residual_4.bn2.running_mean', 'module.backbone.layer4.residual_4.bn2.running_var', 'module.backbone.layer4.residual_5.conv1.weight', 'module.backbone.layer4.residual_5.bn1.weight', 'module.backbone.layer4.residual_5.bn1.bias', 'module.backbone.layer4.residual_5.bn1.running_mean', 'module.backbone.layer4.residual_5.bn1.running_var', 'module.backbone.layer4.residual_5.conv2.weight', 'module.backbone.layer4.residual_5.bn2.weight', 'module.backbone.layer4.residual_5.bn2.bias', 'module.backbone.layer4.residual_5.bn2.running_mean', 'module.backbone.layer4.residual_5.bn2.running_var', 'module.backbone.layer4.residual_6.conv1.weight', 'module.backbone.layer4.residual_6.bn1.weight', 'module.backbone.layer4.residual_6.bn1.bias', 'module.backbone.layer4.residual_6.bn1.running_mean', 'module.backbone.layer4.residual_6.bn1.running_var', 'module.backbone.layer4.residual_6.conv2.weight', 'module.backbone.layer4.residual_6.bn2.weight', 'module.backbone.layer4.residual_6.bn2.bias', 'module.backbone.layer4.residual_6.bn2.running_mean', 'module.backbone.layer4.residual_6.bn2.running_var', 'module.backbone.layer4.residual_7.conv1.weight', 'module.backbone.layer4.residual_7.bn1.weight', 'module.backbone.layer4.residual_7.bn1.bias', 'module.backbone.layer4.residual_7.bn1.running_mean', 'module.backbone.layer4.residual_7.bn1.running_var', 'module.backbone.layer4.residual_7.conv2.weight', 'module.backbone.layer4.residual_7.bn2.weight', 'module.backbone.layer4.residual_7.bn2.bias', 'module.backbone.layer4.residual_7.bn2.running_mean', 'module.backbone.layer4.residual_7.bn2.running_var', 'module.backbone.layer5.ds_conv.weight', 'module.backbone.layer5.ds_bn.weight', 'module.backbone.layer5.ds_bn.bias', 'module.backbone.layer5.ds_bn.running_mean', 'module.backbone.layer5.ds_bn.running_var', 'module.backbone.layer5.residual_0.conv1.weight', 'module.backbone.layer5.residual_0.bn1.weight', 'module.backbone.layer5.residual_0.bn1.bias', 'module.backbone.layer5.residual_0.bn1.running_mean', 'module.backbone.layer5.residual_0.bn1.running_var', 'module.backbone.layer5.residual_0.conv2.weight', 'module.backbone.layer5.residual_0.bn2.weight', 'module.backbone.layer5.residual_0.bn2.bias', 'module.backbone.layer5.residual_0.bn2.running_mean', 'module.backbone.layer5.residual_0.bn2.running_var', 'module.backbone.layer5.residual_1.conv1.weight', 'module.backbone.layer5.residual_1.bn1.weight', 'module.backbone.layer5.residual_1.bn1.bias', 'module.backbone.layer5.residual_1.bn1.running_mean', 'module.backbone.layer5.residual_1.bn1.running_var', 'module.backbone.layer5.residual_1.conv2.weight', 'module.backbone.layer5.residual_1.bn2.weight', 'module.backbone.layer5.residual_1.bn2.bias', 'module.backbone.layer5.residual_1.bn2.running_mean', 'module.backbone.layer5.residual_1.bn2.running_var', 'module.backbone.layer5.residual_2.conv1.weight', 'module.backbone.layer5.residual_2.bn1.weight', 'module.backbone.layer5.residual_2.bn1.bias', 'module.backbone.layer5.residual_2.bn1.running_mean', 'module.backbone.layer5.residual_2.bn1.running_var', 'module.backbone.layer5.residual_2.conv2.weight', 'module.backbone.layer5.residual_2.bn2.weight', 'module.backbone.layer5.residual_2.bn2.bias', 'module.backbone.layer5.residual_2.bn2.running_mean', 'module.backbone.layer5.residual_2.bn2.running_var', 'module.backbone.layer5.residual_3.conv1.weight', 'module.backbone.layer5.residual_3.bn1.weight', 'module.backbone.layer5.residual_3.bn1.bias', 'module.backbone.layer5.residual_3.bn1.running_mean', 'module.backbone.layer5.residual_3.bn1.running_var', 'module.backbone.layer5.residual_3.conv2.weight', 'module.backbone.layer5.residual_3.bn2.weight', 'module.backbone.layer5.residual_3.bn2.bias', 'module.backbone.layer5.residual_3.bn2.running_mean', 'module.backbone.layer5.residual_3.bn2.running_var', 'module.embedding0.0.conv.weight', 'module.embedding0.0.bn.weight', 'module.embedding0.0.bn.bias', 'module.embedding0.0.bn.running_mean', 'module.embedding0.0.bn.running_var', 'module.embedding0.1.conv.weight', 'module.embedding0.1.bn.weight', 'module.embedding0.1.bn.bias', 'module.embedding0.1.bn.running_mean', 'module.embedding0.1.bn.running_var', 'module.embedding0.2.conv.weight', 'module.embedding0.2.bn.weight', 'module.embedding0.2.bn.bias', 'module.embedding0.2.bn.running_mean', 'module.embedding0.2.bn.running_var', 'module.embedding0.3.conv.weight', 'module.embedding0.3.bn.weight', 'module.embedding0.3.bn.bias', 'module.embedding0.3.bn.running_mean', 'module.embedding0.3.bn.running_var', 'module.embedding0.4.conv.weight', 'module.embedding0.4.bn.weight', 'module.embedding0.4.bn.bias', 'module.embedding0.4.bn.running_mean', 'module.embedding0.4.bn.running_var', 'module.embedding0.5.conv.weight', 'module.embedding0.5.bn.weight', 'module.embedding0.5.bn.bias', 'module.embedding0.5.bn.running_mean', 'module.embedding0.5.bn.running_var', 'module.embedding0.conv_out.weight', 'module.embedding0.conv_out.bias', 'module.embedding1_cbl.conv.weight', 'module.embedding1_cbl.bn.weight', 'module.embedding1_cbl.bn.bias', 'module.embedding1_cbl.bn.running_mean', 'module.embedding1_cbl.bn.running_var', 'module.embedding1.0.conv.weight', 'module.embedding1.0.bn.weight', 'module.embedding1.0.bn.bias', 'module.embedding1.0.bn.running_mean', 'module.embedding1.0.bn.running_var', 'module.embedding1.1.conv.weight', 'module.embedding1.1.bn.weight', 'module.embedding1.1.bn.bias', 'module.embedding1.1.bn.running_mean', 'module.embedding1.1.bn.running_var', 'module.embedding1.2.conv.weight', 'module.embedding1.2.bn.weight', 'module.embedding1.2.bn.bias', 'module.embedding1.2.bn.running_mean', 'module.embedding1.2.bn.running_var', 'module.embedding1.3.conv.weight', 'module.embedding1.3.bn.weight', 'module.embedding1.3.bn.bias', 'module.embedding1.3.bn.running_mean', 'module.embedding1.3.bn.running_var', 'module.embedding1.4.conv.weight', 'module.embedding1.4.bn.weight', 'module.embedding1.4.bn.bias', 'module.embedding1.4.bn.running_mean', 'module.embedding1.4.bn.running_var', 'module.embedding1.5.conv.weight', 'module.embedding1.5.bn.weight', 'module.embedding1.5.bn.bias', 'module.embedding1.5.bn.running_mean', 'module.embedding1.5.bn.running_var', 'module.embedding1.conv_out.weight', 'module.embedding1.conv_out.bias', 'module.embedding2_cbl.conv.weight', 'module.embedding2_cbl.bn.weight', 'module.embedding2_cbl.bn.bias', 'module.embedding2_cbl.bn.running_mean', 'module.embedding2_cbl.bn.running_var', 'module.embedding2.0.conv.weight', 'module.embedding2.0.bn.weight', 'module.embedding2.0.bn.bias', 'module.embedding2.0.bn.running_mean', 'module.embedding2.0.bn.running_var', 'module.embedding2.1.conv.weight', 'module.embedding2.1.bn.weight', 'module.embedding2.1.bn.bias', 'module.embedding2.1.bn.running_mean', 'module.embedding2.1.bn.running_var', 'module.embedding2.2.conv.weight', 'module.embedding2.2.bn.weight', 'module.embedding2.2.bn.bias', 'module.embedding2.2.bn.running_mean', 'module.embedding2.2.bn.running_var', 'module.embedding2.3.conv.weight', 'module.embedding2.3.bn.weight', 'module.embedding2.3.bn.bias', 'module.embedding2.3.bn.running_mean', 'module.embedding2.3.bn.running_var', 'module.embedding2.4.conv.weight', 'module.embedding2.4.bn.weight', 'module.embedding2.4.bn.bias', 'module.embedding2.4.bn.running_mean', 'module.embedding2.4.bn.running_var', 'module.embedding2.5.conv.weight', 'module.embedding2.5.bn.weight', 'module.embedding2.5.bn.bias', 'module.embedding2.5.bn.running_mean', 'module.embedding2.5.bn.running_var', 'module.embedding2.conv_out.weight', 'module.embedding2.conv_out.bias'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ua3SKLmQv9Df"
      },
      "source": [
        "# ! pip3 install torchviz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxyHQDdkF0T-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3416adae-351c-4333-f659-38d1181484b9"
      },
      "source": [
        "% cd /content/MiDaS/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/MiDaS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znIPRujp4XsE"
      },
      "source": [
        "# ! git clone https://github.com/BobLiu20/YOLOv3_PyTorch.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m78cTC0ywm4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e84d0ed-1d20-417f-97c9-05730a4210b2"
      },
      "source": [
        "% cd YOLOv3_PyTorch/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/MiDaS/YOLOv3_PyTorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0Fn46nvhmSb"
      },
      "source": [
        "from nets.yolo_loss import YOLOLoss\n",
        "from common.coco_dataset import COCODataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYYwWac9ihjH"
      },
      "source": [
        "yolo_losses = []\n",
        "for i in range(3):\n",
        "    yolo_losses.append(YOLOLoss(config[\"yolo\"][\"anchors\"][i],\n",
        "                                config[\"yolo\"][\"classes\"], (416, 416)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSR_Fuy1GbA4",
        "outputId": "d08e05dd-252d-4c6d-e8aa-b49f8bb70a10"
      },
      "source": [
        "% cd /content/\n",
        "! git clone https://github.com/theschoolofai/YoloV3.git\n",
        "% cd /content/MiDaS/YOLOv3_PyTorch/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "fatal: destination path 'YoloV3' already exists and is not an empty directory.\n",
            "/content/MiDaS/YOLOv3_PyTorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PC5696pWGonD"
      },
      "source": [
        "# ! cp -r /content/YoloV3/data/smalcoco/ /content/MiDaS/YOLOv3_PyTorch/data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-mwidkt3UVr",
        "outputId": "ba3f9fe1-95d7-4a79-cca9-ecdfd8c0497b"
      },
      "source": [
        "% cd /content/MiDaS/YOLOv3_PyTorch/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/MiDaS/YOLOv3_PyTorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52PIEbguifaX"
      },
      "source": [
        "# DataLoader\n",
        "dataloader = torch.utils.data.DataLoader(COCODataset(\"/content/MiDaS/YOLOv3_PyTorch/data/custom/input_file_dir.txt\",\n",
        "                                                        (416, 416),\n",
        "                                                        is_training=True),\n",
        "                                            batch_size=4,\n",
        "                                            shuffle=True, num_workers=16, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyQFVGbru5ss"
      },
      "source": [
        "# ! cp -r /content/drive/MyDrive/yolo_data /content/MiDaS/YOLOv3_PyTorch/data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3V9CO2xk5JX3"
      },
      "source": [
        "# dataloader = torch.utils.data.DataLoader(LoadImagesAndLabels(\"./data/yolo_data/customdata/train.txt\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnYIew49ksW_"
      },
      "source": [
        "sample = next(iter(dataloader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSORuoy87jl2"
      },
      "source": [
        "# print(sample[0].shape)\n",
        "# print(sample[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc1QO8Nj1Pqj"
      },
      "source": [
        "import logging\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8sAWAS1u5UH"
      },
      "source": [
        "import time\n",
        "def _get_optimizer(config, net):\n",
        "    optimizer = None\n",
        "\n",
        "    # Assign different lr for each layer\n",
        "    params = None\n",
        "    base_params = list(\n",
        "        map(id, net.backbone.parameters())\n",
        "    )\n",
        "    logits_params = filter(lambda p: id(p) not in base_params, net.parameters())\n",
        "\n",
        "    if not config[\"lr\"][\"freeze_backbone\"]:\n",
        "        params = [\n",
        "            {\"params\": logits_params, \"lr\": config[\"lr\"][\"other_lr\"]},\n",
        "            {\"params\": net.backbone.parameters(), \"lr\": config[\"lr\"][\"backbone_lr\"]},\n",
        "        ]\n",
        "    else:\n",
        "        logging.info(\"freeze backbone's parameters.\")\n",
        "        for p in net.backbone.parameters():\n",
        "            p.requires_grad = False\n",
        "        params = [\n",
        "            {\"params\": logits_params, \"lr\": config[\"lr\"][\"other_lr\"]},\n",
        "        ]\n",
        "\n",
        "    # Initialize optimizer class\n",
        "    if config[\"optimizer\"][\"type\"] == \"adam\":\n",
        "        optimizer = optim.Adam(params, weight_decay=config[\"optimizer\"][\"weight_decay\"])\n",
        "    elif config[\"optimizer\"][\"type\"] == \"amsgrad\":\n",
        "        optimizer = optim.Adam(params, weight_decay=config[\"optimizer\"][\"weight_decay\"],\n",
        "                               amsgrad=True)\n",
        "    elif config[\"optimizer\"][\"type\"] == \"rmsprop\":\n",
        "        optimizer = optim.RMSprop(params, weight_decay=config[\"optimizer\"][\"weight_decay\"])\n",
        "    else:\n",
        "        # Default to sgd\n",
        "        logging.info(\"Using SGD optimizer.\")\n",
        "        optimizer = optim.SGD(params, momentum=0.9,\n",
        "                              weight_decay=config[\"optimizer\"][\"weight_decay\"],\n",
        "                              nesterov=(config[\"optimizer\"][\"type\"] == \"nesterov\"))\n",
        "\n",
        "    return optimizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lReZLmYwxez"
      },
      "source": [
        "optimizer = _get_optimizer(config, detector)\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=config[\"lr\"][\"decay_step\"],\n",
        "    gamma=config[\"lr\"][\"decay_gamma\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mMAsRh_94DU"
      },
      "source": [
        "def _save_checkpoint(state_dict):\n",
        "    checkpoint_path = \"/content/MiDaS/saved_model.pth\"\n",
        "    torch.save(state_dict, checkpoint_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBvnpDguk0Uc",
        "outputId": "75b81754-9b93-4ff0-879d-a74f3027ae21"
      },
      "source": [
        "for epoch in range(80):\n",
        "    print('epoch', epoch)\n",
        "    total_loss = 0\n",
        "    for step, samples in enumerate(dataloader):\n",
        "        # sample = samples[0][0]\n",
        "        # images, labels = samples[0][0], samples[1][0]\n",
        "        # images, labels = samples[0], samples[1]\n",
        "        images, labels = samples['image'], samples['label']\n",
        "        # print(labels)\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        images, lables = images.to(device, dtype = torch.float), labels.to(device, dtype = torch.float)\n",
        "        start_time = time.time()\n",
        "        # config[\"global_step\"] += 1\n",
        "\n",
        "        # Forward and backward\n",
        "        optimizer.zero_grad()\n",
        "        outputs = detector(images)\n",
        "        # print(len(images))\n",
        "        # print(images.shape)\n",
        "        # print(len(outputs))\n",
        "        # print(outputs[0].shape)\n",
        "        # print(samples['image_path'])\n",
        "        losses_name = [\"total_loss\", \"x\", \"y\", \"w\", \"h\", \"conf\", \"cls\"]\n",
        "        losses = []\n",
        "        for _ in range(len(losses_name)):\n",
        "            losses.append([])\n",
        "        for i in range(3):\n",
        "            _loss_item = yolo_losses[i](outputs[i], labels)\n",
        "            for j, l in enumerate(_loss_item):\n",
        "                losses[j].append(l)\n",
        "        losses = [sum(l) for l in losses]\n",
        "        loss = losses[0]\n",
        "        loss.backward()\n",
        "        # print(loss)\n",
        "        total_loss += loss\n",
        "        optimizer.step()\n",
        "        # print(loss)\n",
        "        if step > 0 and step % 10 == 0:\n",
        "            _loss = loss.item()\n",
        "            duration = float(time.time() - start_time)\n",
        "            example_per_second = config[\"batch_size\"] / duration\n",
        "            lr = optimizer.param_groups[0]['lr']\n",
        "            print(epoch, step, _loss, example_per_second, lr)\n",
        "            logging.info(\n",
        "                \"epoch [%.3d] iter = %d loss = %.2f example/sec = %.3f lr = %.5f \"%\n",
        "                (epoch, step, _loss, example_per_second, lr)\n",
        "            )\n",
        "            # config[\"tensorboard_writer\"].add_scalar(\"lr\",\n",
        "            #                                         lr,\n",
        "            #                                         config[\"global_step\"])\n",
        "            # config[\"tensorboard_writer\"].add_scalar(\"example/sec\",\n",
        "            #                                         example_per_second,\n",
        "            #                                         config[\"global_step\"])\n",
        "            for i, name in enumerate(losses_name):\n",
        "                value = _loss if i == 0 else losses[i]\n",
        "                # config[\"tensorboard_writer\"].add_scalar(name,\n",
        "                #                                         value,\n",
        "                #                                         config[\"global_step\"])\n",
        "\n",
        "        if step > 0 and step % 860 == 0:\n",
        "            detector.train(False)\n",
        "            _save_checkpoint(detector.state_dict())\n",
        "            detector.train(True)\n",
        "\n",
        "    lr_scheduler.step()\n",
        "    print(total_loss/4)\n",
        "detector.train(False)\n",
        "_save_checkpoint(detector.state_dict())\n",
        "detector.train(True)\n",
        "logging.info(\"Bye~\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0\n",
            "0 10 0.24229156970977783 13.3806673897786 0.01\n",
            "0 20 1.0431218147277832 12.762366830596727 0.01\n",
            "0 30 0.28811752796173096 12.862875705448003 0.01\n",
            "0 40 0.2521117329597473 12.774474830946161 0.01\n",
            "0 50 0.15034037828445435 12.030119008949514 0.01\n",
            "0 60 0.4553418755531311 10.828565306180407 0.01\n",
            "0 70 1.272276759147644 11.861308537812251 0.01\n",
            "0 80 0.21085157990455627 11.970412905076284 0.01\n",
            "0 90 0.5299996137619019 11.023833303546084 0.01\n",
            "0 100 0.5856153964996338 10.051449434257526 0.01\n",
            "0 110 2.286013126373291 11.619592581896054 0.01\n",
            "0 120 0.915529727935791 12.055282426402432 0.01\n",
            "0 130 0.6528151631355286 11.654834606570166 0.01\n",
            "0 140 0.5314655303955078 10.880293025469818 0.01\n",
            "0 150 0.3455304503440857 12.153818402833368 0.01\n",
            "0 160 0.2979307770729065 11.716513877433867 0.01\n",
            "0 170 0.8336289525032043 10.169414925213909 0.01\n",
            "0 180 0.4534981846809387 12.003875076020464 0.01\n",
            "0 190 0.6094105243682861 12.875817535199486 0.01\n",
            "0 200 0.32847514748573303 12.607130297301115 0.01\n",
            "0 210 0.6336209177970886 12.414297902316266 0.01\n",
            "0 220 1.7846193313598633 12.275856013533486 0.01\n",
            "0 230 0.38383811712265015 12.472292123087309 0.01\n",
            "0 240 0.5687568187713623 12.432650980252665 0.01\n",
            "0 250 0.5176275372505188 12.367096884050653 0.01\n",
            "0 260 0.8313134908676147 12.541893736534663 0.01\n",
            "0 270 0.49421173334121704 12.040306554874658 0.01\n",
            "0 280 0.6459885835647583 12.42646283293337 0.01\n",
            "0 290 0.3834748864173889 11.474126953079583 0.01\n",
            "0 300 0.3815029263496399 11.786901300989618 0.01\n",
            "0 310 0.28420156240463257 11.055826029654035 0.01\n",
            "0 320 0.834671139717102 10.413315863204172 0.01\n",
            "0 330 0.3837393820285797 11.874278790349521 0.01\n",
            "0 340 0.24947679042816162 12.343258381087402 0.01\n",
            "0 350 0.2949807941913605 12.78865264661559 0.01\n",
            "0 360 0.2968396246433258 10.123112160731978 0.01\n",
            "0 370 0.18980781733989716 12.745971209685507 0.01\n",
            "0 380 0.08825277537107468 12.787863081608947 0.01\n",
            "0 390 0.17616039514541626 12.29652290735693 0.01\n",
            "0 400 0.32021939754486084 12.409816773193306 0.01\n",
            "0 410 0.41642606258392334 11.969149000681313 0.01\n",
            "0 420 1.6482784748077393 12.99265150605792 0.01\n",
            "0 430 0.8984371423721313 12.133364768639522 0.01\n",
            "0 440 0.9433339834213257 12.420041782090555 0.01\n",
            "0 450 0.4240961968898773 11.345301298840058 0.01\n",
            "0 460 0.5962233543395996 12.67561511111514 0.01\n",
            "0 470 0.20980454981327057 12.840783831091946 0.01\n",
            "0 480 0.15992669761180878 10.923548515635376 0.01\n",
            "0 490 0.4058275520801544 12.550844592067268 0.01\n",
            "0 500 0.5144736766815186 12.868775767709968 0.01\n",
            "0 510 1.2140138149261475 12.9978555402675 0.01\n",
            "0 520 0.4168909788131714 12.025316129069253 0.01\n",
            "0 530 0.56467205286026 11.685845173773949 0.01\n",
            "0 540 0.18735487759113312 12.594760520960094 0.01\n",
            "0 550 0.3610767126083374 11.540972780659677 0.01\n",
            "0 560 0.8970374464988708 11.46855024543899 0.01\n",
            "0 570 0.7785930037498474 11.668996216336524 0.01\n",
            "0 580 1.287902593612671 10.147812475049658 0.01\n",
            "0 590 0.2380838692188263 11.613857534269334 0.01\n",
            "0 600 0.581496000289917 12.206005798451079 0.01\n",
            "0 610 0.4628872573375702 10.7751536902355 0.01\n",
            "0 620 0.5708481073379517 10.337519963720252 0.01\n",
            "0 630 1.3157187700271606 11.825942302663252 0.01\n",
            "0 640 0.425240695476532 9.962699591390276 0.01\n",
            "0 650 0.331473708152771 12.854311505981542 0.01\n",
            "0 660 0.998249888420105 12.546836707756025 0.01\n",
            "0 670 0.46777987480163574 12.13151354536876 0.01\n",
            "0 680 0.4193473160266876 11.10729654650082 0.01\n",
            "0 690 1.3652935028076172 12.054806016906833 0.01\n",
            "0 700 0.2727915942668915 12.813265687208208 0.01\n",
            "0 710 0.6421283483505249 12.134891794931143 0.01\n",
            "0 720 0.5027062892913818 12.171108625219993 0.01\n",
            "0 730 0.13857463002204895 11.952938360195098 0.01\n",
            "0 740 0.9993758201599121 10.720916605161335 0.01\n",
            "0 750 0.24797043204307556 11.087997663071171 0.01\n",
            "0 760 0.27294206619262695 12.012409632429437 0.01\n",
            "0 770 0.6744173169136047 12.012796682256775 0.01\n",
            "0 780 0.386910617351532 12.338057088058926 0.01\n",
            "0 790 0.36621662974357605 12.867986148129116 0.01\n",
            "0 800 2.496598720550537 12.532749669448034 0.01\n",
            "0 810 0.2656472325325012 10.930601315407994 0.01\n",
            "0 820 0.6215302348136902 10.874052008371429 0.01\n",
            "0 830 0.4604450762271881 7.801600011904297 0.01\n",
            "0 840 0.48166683316230774 12.620824958174357 0.01\n",
            "0 850 0.23743939399719238 12.79444577053571 0.01\n",
            "0 860 0.19410327076911926 12.48291758865607 0.01\n",
            "tensor(146.8541, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 1\n",
            "1 10 0.5509160757064819 11.374221790740606 0.01\n",
            "1 20 0.16649086773395538 13.048563019929194 0.01\n",
            "1 30 0.927905797958374 11.142506284473283 0.01\n",
            "1 40 0.5998349189758301 11.346728482106323 0.01\n",
            "1 50 0.4435639977455139 11.95287023355424 0.01\n",
            "1 60 0.17056460678577423 10.961359695487587 0.01\n",
            "1 70 0.3938100337982178 11.621942932194314 0.01\n",
            "1 80 0.4681318998336792 11.389270857093203 0.01\n",
            "1 90 0.14262838661670685 12.198568647966361 0.01\n",
            "1 100 1.200590968132019 11.100248970013107 0.01\n",
            "1 110 0.7886302471160889 10.902182487851602 0.01\n",
            "1 120 2.003538131713867 12.700611744186311 0.01\n",
            "1 130 0.33472269773483276 12.001676793718605 0.01\n",
            "1 140 0.2509267032146454 12.310453955444611 0.01\n",
            "1 150 0.7345973253250122 11.964744501758283 0.01\n",
            "1 160 0.4632733464241028 11.294305190722323 0.01\n",
            "1 170 0.4691941440105438 9.58095278342026 0.01\n",
            "1 180 0.6562602519989014 11.543466160266135 0.01\n",
            "1 190 0.16279245913028717 12.098026136938056 0.01\n",
            "1 200 1.3684695959091187 11.46546225166287 0.01\n",
            "1 210 2.7404112815856934 11.603118561638484 0.01\n",
            "1 220 0.2429937720298767 11.908785428837104 0.01\n",
            "1 230 0.20705237984657288 12.937506410063627 0.01\n",
            "1 240 0.7467443943023682 11.494920635573159 0.01\n",
            "1 250 1.1838929653167725 10.610419162131087 0.01\n",
            "1 260 0.5578032732009888 11.680304768694374 0.01\n",
            "1 270 0.39199864864349365 12.748198771167573 0.01\n",
            "1 280 0.4869072437286377 12.59726658527392 0.01\n",
            "1 290 0.3799028992652893 11.738951799338368 0.01\n",
            "1 300 0.5487985014915466 10.823472640391413 0.01\n",
            "1 310 0.3125014007091522 11.842002322208145 0.01\n",
            "1 320 0.7006195187568665 12.490417337015069 0.01\n",
            "1 330 0.3425127863883972 11.80178434885617 0.01\n",
            "1 340 0.6672648787498474 12.883144008766292 0.01\n",
            "1 350 0.3519381582736969 10.588628526131366 0.01\n",
            "1 360 0.8201408982276917 10.326531340806847 0.01\n",
            "1 370 0.23474399745464325 11.97789932182988 0.01\n",
            "1 380 4.601768493652344 12.460287198606713 0.01\n",
            "1 390 0.9956965446472168 12.487869943825034 0.01\n",
            "1 400 0.28002846240997314 12.601070142074729 0.01\n",
            "1 410 0.31469476222991943 12.513577396042875 0.01\n",
            "1 420 0.7779165506362915 12.289731243878837 0.01\n",
            "1 430 0.3304441273212433 11.552654900467486 0.01\n",
            "1 440 0.48575931787490845 12.642499259635477 0.01\n",
            "1 450 0.19011837244033813 12.129610269802091 0.01\n",
            "1 460 0.4863530993461609 10.852963227707464 0.01\n",
            "1 470 0.668014407157898 11.60827271651672 0.01\n",
            "1 480 0.4327940046787262 11.346498266971004 0.01\n",
            "1 490 0.3848525583744049 12.529146300936107 0.01\n",
            "1 500 0.9511226415634155 11.514311891975362 0.01\n",
            "1 510 0.652086079120636 12.322895817526241 0.01\n",
            "1 520 0.3024582266807556 9.739320235895525 0.01\n",
            "1 530 0.46208158135414124 9.838151387245341 0.01\n",
            "1 540 0.4790439009666443 12.579971431677846 0.01\n",
            "1 550 0.5589307546615601 9.27856119723256 0.01\n",
            "1 560 1.292327880859375 8.449456762877865 0.01\n",
            "1 570 0.15686197578907013 11.517276034873344 0.01\n",
            "1 580 0.36093050241470337 11.76192933258553 0.01\n",
            "1 590 0.4486202597618103 11.904171834511649 0.01\n",
            "1 600 0.7405081987380981 11.919572870301627 0.01\n",
            "1 610 0.19075772166252136 12.56775842527872 0.01\n",
            "1 620 0.5516684055328369 11.50007368671425 0.01\n",
            "1 630 0.4429592490196228 11.431494701324787 0.01\n",
            "1 640 0.6050688028335571 10.047338121482774 0.01\n",
            "1 650 0.17388507723808289 12.722792744833066 0.01\n",
            "1 660 0.4360489845275879 11.025543022525282 0.01\n",
            "1 670 0.4305277466773987 11.53187394491276 0.01\n",
            "1 680 1.0315663814544678 12.264997141591381 0.01\n",
            "1 690 0.62366783618927 9.97844939616795 0.01\n",
            "1 700 1.2043986320495605 11.129570896450616 0.01\n",
            "1 710 0.37185007333755493 12.311411519610902 0.01\n",
            "1 720 0.4548192024230957 12.395513527611191 0.01\n",
            "1 730 1.9046823978424072 9.594041786183203 0.01\n",
            "1 740 0.4366462826728821 4.849207681980238 0.01\n",
            "1 750 0.2089637815952301 12.140037366839028 0.01\n",
            "1 760 1.3226077556610107 12.384908278891226 0.01\n",
            "1 770 0.44132328033447266 12.406623189445373 0.01\n",
            "1 780 0.356148898601532 12.145442278220216 0.01\n",
            "1 790 0.4776657223701477 12.494249680703309 0.01\n",
            "1 800 0.5585922598838806 10.335806822641267 0.01\n",
            "1 810 0.5548735857009888 12.420924512964177 0.01\n",
            "1 820 0.31911128759384155 12.44876719687824 0.01\n",
            "1 830 0.28724750876426697 11.928013371829719 0.01\n",
            "1 840 0.5219630002975464 12.570771777156212 0.01\n",
            "1 850 0.4648978114128113 12.081734470270616 0.01\n",
            "1 860 0.7468128204345703 12.66769051879705 0.01\n",
            "tensor(140.3127, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 2\n",
            "2 10 1.400111198425293 12.309613951058598 0.01\n",
            "2 20 1.1441574096679688 12.482666823407916 0.01\n",
            "2 30 0.34221023321151733 12.809274292987094 0.01\n",
            "2 40 0.45728224515914917 12.548854409133611 0.01\n",
            "2 50 0.40079009532928467 11.920292727983233 0.01\n",
            "2 60 0.405994713306427 12.285087690393707 0.01\n",
            "2 70 1.0243887901306152 10.711197229182961 0.01\n",
            "2 80 0.4498600363731384 11.736348634391248 0.01\n",
            "2 90 0.13879305124282837 9.593498668523928 0.01\n",
            "2 100 0.22858017683029175 10.520180716281082 0.01\n",
            "2 110 0.5679982304573059 10.971639034649451 0.01\n",
            "2 120 0.397874653339386 11.804491376295772 0.01\n",
            "2 130 0.5305808186531067 11.528926051450357 0.01\n",
            "2 140 0.2329505980014801 12.221637344681868 0.01\n",
            "2 150 1.9976651668548584 12.182518307670524 0.01\n",
            "2 160 0.42948800325393677 11.436567642367313 0.01\n",
            "2 170 0.18720801174640656 11.82600065413288 0.01\n",
            "2 180 0.1805853247642517 11.957564271882303 0.01\n",
            "2 190 1.4645047187805176 11.419892970086895 0.01\n",
            "2 200 0.8740666508674622 12.613490253747647 0.01\n",
            "2 210 0.3808864653110504 10.566028800040305 0.01\n",
            "2 220 0.525813102722168 11.088136897106429 0.01\n",
            "2 230 0.4785915017127991 12.508334184508415 0.01\n",
            "2 240 1.408805012702942 11.465148842736703 0.01\n",
            "2 250 1.3084272146224976 12.401900067563872 0.01\n",
            "2 260 0.5714777708053589 11.851990836141995 0.01\n",
            "2 270 0.6765576601028442 11.71812601799778 0.01\n",
            "2 280 0.13994231820106506 12.002037394195716 0.01\n",
            "2 290 0.26625335216522217 12.58237724896692 0.01\n",
            "2 300 0.38154715299606323 12.43153629047496 0.01\n",
            "2 310 0.8358086347579956 12.787892323018749 0.01\n",
            "2 320 0.5943522453308105 10.454781176526305 0.01\n",
            "2 330 1.329970121383667 12.214145570472681 0.01\n",
            "2 340 0.313449501991272 12.631838267569865 0.01\n",
            "2 350 0.603745698928833 10.199107703549428 0.01\n",
            "2 360 0.258395254611969 12.12476268595773 0.01\n",
            "2 370 0.4689398407936096 12.523263695020617 0.01\n",
            "2 380 4.099982738494873 12.284125222219945 0.01\n",
            "2 390 0.5606061816215515 12.288435015399715 0.01\n",
            "2 400 0.31139710545539856 12.372523425412572 0.01\n",
            "2 410 0.12644220888614655 12.528407164630275 0.01\n",
            "2 420 0.4560304284095764 12.787746117307023 0.01\n",
            "2 430 0.2872861623764038 11.903453922512542 0.01\n",
            "2 440 0.6796716451644897 11.103055170428412 0.01\n",
            "2 450 0.4180019795894623 12.565038233113397 0.01\n",
            "2 460 1.0231317281723022 12.500980573322444 0.01\n",
            "2 470 0.3490579128265381 12.335417271714636 0.01\n",
            "2 480 2.2952566146850586 12.86344771536241 0.01\n",
            "2 490 0.20713216066360474 12.816172752175978 0.01\n",
            "2 500 0.19005638360977173 12.753005206947664 0.01\n",
            "2 510 0.3443942666053772 12.239255134519187 0.01\n",
            "2 520 0.31334418058395386 11.896102422726319 0.01\n",
            "2 530 0.21825119853019714 11.613511842833004 0.01\n",
            "2 540 0.64799565076828 12.426343182486466 0.01\n",
            "2 550 0.22211609780788422 11.800257004136402 0.01\n",
            "2 560 0.2539798617362976 9.698322859757363 0.01\n",
            "2 570 0.3487871289253235 12.538997296717572 0.01\n",
            "2 580 1.4565805196762085 12.089717999086275 0.01\n",
            "2 590 1.0561726093292236 11.427118914202698 0.01\n",
            "2 600 1.2780030965805054 12.052511117017838 0.01\n",
            "2 610 0.8908160924911499 12.408256483798178 0.01\n",
            "2 620 0.23517540097236633 11.734772515372095 0.01\n",
            "2 630 0.5550605654716492 11.260239256029724 0.01\n",
            "2 640 0.3549514710903168 7.7927364103802725 0.01\n",
            "2 650 0.14956359565258026 12.027066106075106 0.01\n",
            "2 660 0.4853207767009735 10.096064529251551 0.01\n",
            "2 670 0.4575519859790802 7.2017211449803655 0.01\n",
            "2 680 0.5737699866294861 7.598858623282274 0.01\n",
            "2 690 0.8377017974853516 11.792816856264023 0.01\n",
            "2 700 0.4484347701072693 11.718420670309898 0.01\n",
            "2 710 0.3998958468437195 12.669737712544064 0.01\n",
            "2 720 0.34532445669174194 12.553248673384791 0.01\n",
            "2 730 1.3962349891662598 11.351880069611157 0.01\n",
            "2 740 0.1842842847108841 8.29650845959009 0.01\n",
            "2 750 0.18765607476234436 10.620010203991203 0.01\n",
            "2 760 1.0484281778335571 11.877305060939694 0.01\n",
            "2 770 1.4424407482147217 12.337721378612489 0.01\n",
            "2 780 0.5282512903213501 11.410471763237629 0.01\n",
            "2 790 0.369206964969635 12.518404660782986 0.01\n",
            "2 800 0.3691140115261078 12.597730080006608 0.01\n",
            "2 810 0.5768060684204102 10.037593975965606 0.01\n",
            "2 820 0.8860546350479126 12.189687905285375 0.01\n",
            "2 830 0.24666449427604675 12.236702048127832 0.01\n",
            "2 840 0.13776229321956635 12.345728941998559 0.01\n",
            "2 850 0.24798767268657684 12.727406243077661 0.01\n",
            "2 860 0.7449998259544373 12.522263547075626 0.01\n",
            "tensor(135.0751, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 3\n",
            "3 10 0.515641987323761 11.608039797690461 0.01\n",
            "3 20 0.6841984987258911 5.601543317665552 0.01\n",
            "3 30 0.48718079924583435 12.374950949297062 0.01\n",
            "3 40 0.45128417015075684 10.785412462625688 0.01\n",
            "3 50 2.5896856784820557 12.738799878817053 0.01\n",
            "3 60 0.2111455202102661 12.078594446092309 0.01\n",
            "3 70 0.372437059879303 11.405460709553692 0.01\n",
            "3 80 1.2860937118530273 11.845605948921293 0.01\n",
            "3 90 0.30566132068634033 10.834425350821112 0.01\n",
            "3 100 1.0564813613891602 11.013188479527365 0.01\n",
            "3 110 1.7946703433990479 12.256699931327713 0.01\n",
            "3 120 0.5053876042366028 11.929141371694945 0.01\n",
            "3 130 0.496597021818161 12.885459364513947 0.01\n",
            "3 140 1.2770678997039795 12.33836559392834 0.01\n",
            "3 150 0.3001300096511841 12.221940055933032 0.01\n",
            "3 160 0.403349906206131 11.040227842836876 0.01\n",
            "3 170 0.29145562648773193 10.836335780633068 0.01\n",
            "3 180 0.2142227441072464 10.011801325262093 0.01\n",
            "3 190 0.6515012383460999 11.547001617399085 0.01\n",
            "3 200 0.21333247423171997 12.662785686413475 0.01\n",
            "3 210 0.33342331647872925 11.44500132683084 0.01\n",
            "3 220 0.17527326941490173 11.611558670941093 0.01\n",
            "3 230 1.2420762777328491 12.900866762221716 0.01\n",
            "3 240 0.3755247890949249 12.831650333808291 0.01\n",
            "3 250 0.9605392217636108 11.330337521796622 0.01\n",
            "3 260 0.8820040225982666 11.993106036507125 0.01\n",
            "3 270 0.820317268371582 11.587131315447422 0.01\n",
            "3 280 0.30995500087738037 12.773502233449745 0.01\n",
            "3 290 0.27932655811309814 10.642578392615185 0.01\n",
            "3 300 0.28119954466819763 9.30844529271263 0.01\n",
            "3 310 0.6083021759986877 12.111117367394465 0.01\n",
            "3 320 0.6454213857650757 11.726414672332812 0.01\n",
            "3 330 1.1460715532302856 12.23132263097269 0.01\n",
            "3 340 0.8281534910202026 12.334610130762682 0.01\n",
            "3 350 0.8322498798370361 11.873177946759988 0.01\n",
            "3 360 2.282489776611328 11.646711479468049 0.01\n",
            "3 370 0.2652525007724762 12.801113378086168 0.01\n",
            "3 380 0.3589499294757843 12.627341300365336 0.01\n",
            "3 390 0.4055781662464142 11.953466367994164 0.01\n",
            "3 400 0.2832722067832947 12.366012146942627 0.01\n",
            "3 410 1.6294546127319336 12.233409604309674 0.01\n",
            "3 420 0.5210380554199219 12.0057648683655 0.01\n",
            "3 430 0.5647509098052979 12.477737153532871 0.01\n",
            "3 440 0.2700200080871582 12.239835530011542 0.01\n",
            "3 450 0.1978769600391388 12.296207478331484 0.01\n",
            "3 460 0.18144240975379944 12.583320957931132 0.01\n",
            "3 470 0.29344406723976135 12.790329577170581 0.01\n",
            "3 480 0.7167434692382812 11.307192176658702 0.01\n",
            "3 490 1.0085831880569458 12.52883753718949 0.01\n",
            "3 500 4.7217912673950195 11.137454211359964 0.01\n",
            "3 510 0.7789899706840515 12.216600294324746 0.01\n",
            "3 520 0.6068803071975708 11.643955104341043 0.01\n",
            "3 530 0.2816392481327057 13.109883429264428 0.01\n",
            "3 540 0.1609451323747635 11.786992391983235 0.01\n",
            "3 550 0.7453771829605103 12.097720809225214 0.01\n",
            "3 560 0.394561767578125 9.95830588112392 0.01\n",
            "3 570 0.2984510064125061 12.55306082136679 0.01\n",
            "3 580 0.33815503120422363 12.218068263581202 0.01\n",
            "3 590 0.46532756090164185 11.499939680415876 0.01\n",
            "3 600 0.3463917374610901 12.384094648489487 0.01\n",
            "3 610 0.3592478632926941 9.137275213233929 0.01\n",
            "3 620 0.2643597722053528 12.564624189492195 0.01\n",
            "3 630 0.3109874725341797 10.096799722201661 0.01\n",
            "3 640 0.4293404221534729 12.404238251170577 0.01\n",
            "3 650 0.3605974018573761 11.664193465855368 0.01\n",
            "3 660 1.2678358554840088 11.767795080133185 0.01\n",
            "3 670 1.623816967010498 9.878061647448941 0.01\n",
            "3 680 0.7701352834701538 10.358269128779343 0.01\n",
            "3 690 0.2290552258491516 11.989686301189733 0.01\n",
            "3 700 0.27307477593421936 10.623937588257272 0.01\n",
            "3 710 0.3738836348056793 12.568699942539974 0.01\n",
            "3 720 0.3175422251224518 12.195890654929142 0.01\n",
            "3 730 0.7516908645629883 9.06836835792595 0.01\n",
            "3 740 3.8874778747558594 11.075575952901847 0.01\n",
            "3 750 0.5679223537445068 11.705665980117997 0.01\n",
            "3 760 0.5183261036872864 12.703977731789521 0.01\n",
            "3 770 0.5602080821990967 12.286779121312897 0.01\n",
            "3 780 0.2724110186100006 12.343830518600486 0.01\n",
            "3 790 0.7320890426635742 11.40070970661745 0.01\n",
            "3 800 0.21416983008384705 12.67491604685511 0.01\n",
            "3 810 0.283425509929657 12.938374429899854 0.01\n",
            "3 820 0.9404577016830444 12.933566864839529 0.01\n",
            "3 830 0.8959952592849731 11.53781392978646 0.01\n",
            "3 840 0.2948116362094879 12.095514184307683 0.01\n",
            "3 850 0.26718613505363464 11.667559844415994 0.01\n",
            "3 860 0.21412301063537598 11.763892180551718 0.01\n",
            "tensor(134.7825, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 4\n",
            "4 10 0.34475332498550415 12.30365592154861 0.01\n",
            "4 20 0.441539466381073 11.303147081175183 0.01\n",
            "4 30 0.7927782535552979 12.134777693474414 0.01\n",
            "4 40 0.35514122247695923 12.182659847698744 0.01\n",
            "4 50 0.9310843348503113 10.027377250333355 0.01\n",
            "4 60 0.30711352825164795 10.717026738883392 0.01\n",
            "4 70 0.15622416138648987 12.16944888926285 0.01\n",
            "4 80 0.9510425329208374 11.335987394560526 0.01\n",
            "4 90 0.20759481191635132 12.296946508934724 0.01\n",
            "4 100 0.23522312939167023 12.663254015098794 0.01\n",
            "4 110 0.5600165128707886 11.95288726514165 0.01\n",
            "4 120 0.48867136240005493 11.626735117319226 0.01\n",
            "4 130 1.08522629737854 12.100181316470085 0.01\n",
            "4 140 0.6605504155158997 11.922546020989436 0.01\n",
            "4 150 0.17145945131778717 11.46272833608448 0.01\n",
            "4 160 0.39600083231925964 9.534497250278893 0.01\n",
            "4 170 0.3188040256500244 12.85088508115478 0.01\n",
            "4 180 0.35051536560058594 11.125120685813714 0.01\n",
            "4 190 0.7401435375213623 11.889973275012244 0.01\n",
            "4 200 0.19764143228530884 13.016562017762254 0.01\n",
            "4 210 0.14775575697422028 11.500586092538185 0.01\n",
            "4 220 0.7738490104675293 12.245767647999113 0.01\n",
            "4 230 0.3265697956085205 11.770296607375125 0.01\n",
            "4 240 1.3816399574279785 12.157464566849253 0.01\n",
            "4 250 0.39100098609924316 11.186108255224273 0.01\n",
            "4 260 0.4276043772697449 12.948669839797294 0.01\n",
            "4 270 0.2979925870895386 12.103943813374984 0.01\n",
            "4 280 0.22688820958137512 12.85426226280219 0.01\n",
            "4 290 0.7508152723312378 10.932887777896523 0.01\n",
            "4 300 0.2093605101108551 11.45441710009053 0.01\n",
            "4 310 0.18573518097400665 12.365137203570713 0.01\n",
            "4 320 0.797573447227478 12.34449353793177 0.01\n",
            "4 330 0.5395264029502869 12.397904273479009 0.01\n",
            "4 340 0.31770458817481995 12.268396324720387 0.01\n",
            "4 350 0.3034977912902832 10.924338033488196 0.01\n",
            "4 360 0.6058461666107178 12.114213095341247 0.01\n",
            "4 370 0.03883926942944527 12.914590982918812 0.01\n",
            "4 380 0.35085541009902954 11.87661560809443 0.01\n",
            "4 390 0.362021803855896 12.413076290197518 0.01\n",
            "4 400 0.4176216125488281 11.536076505542773 0.01\n",
            "4 410 0.9250273704528809 12.669775984147314 0.01\n",
            "4 420 1.33987295627594 12.221388064322268 0.01\n",
            "4 430 0.35471636056900024 12.095444422736 0.01\n",
            "4 440 0.33803820610046387 11.832973630236502 0.01\n",
            "4 450 1.5922013521194458 11.918235012889884 0.01\n",
            "4 460 0.2992306649684906 13.019349902261071 0.01\n",
            "4 470 0.8012853860855103 12.332968967659177 0.01\n",
            "4 480 0.468010276556015 11.60024642514145 0.01\n",
            "4 490 0.1610146462917328 13.00552322584296 0.01\n",
            "4 500 0.3133367896080017 11.994272106402128 0.01\n",
            "4 510 0.7985429167747498 12.432549636558058 0.01\n",
            "4 520 0.30254122614860535 11.10668623568486 0.01\n",
            "4 530 0.5713676810264587 11.279437628746766 0.01\n",
            "4 540 0.2075686901807785 12.308150991344716 0.01\n",
            "4 550 0.46987801790237427 12.544622675438891 0.01\n",
            "4 560 0.43410277366638184 11.638325162724264 0.01\n",
            "4 570 0.5346730947494507 11.765269864200086 0.01\n",
            "4 580 0.4962581992149353 10.067994846347586 0.01\n",
            "4 590 0.16984687745571136 11.547033406609744 0.01\n",
            "4 600 0.15569642186164856 12.21873563896457 0.01\n",
            "4 610 0.5837570428848267 11.055221361862909 0.01\n",
            "4 620 0.13476906716823578 12.388520625731491 0.01\n",
            "4 630 0.8128511905670166 10.696378375449077 0.01\n",
            "4 640 0.24892497062683105 9.50315191083001 0.01\n",
            "4 650 0.27436479926109314 12.310923685289968 0.01\n",
            "4 660 0.21675315499305725 11.64914561070955 0.01\n",
            "4 670 0.4609483480453491 6.757864369954174 0.01\n",
            "4 680 0.25528791546821594 10.868684120342389 0.01\n",
            "4 690 1.8451653718948364 11.95960148956676 0.01\n",
            "4 700 0.41009294986724854 11.594730778613778 0.01\n",
            "4 710 0.6361145973205566 11.420592607921407 0.01\n",
            "4 720 1.0191642045974731 12.889627122185976 0.01\n",
            "4 730 0.5151025652885437 11.681345736088677 0.01\n",
            "4 740 2.386059284210205 9.550687957191245 0.01\n",
            "4 750 0.1940251886844635 10.822467248218157 0.01\n",
            "4 760 0.23495158553123474 11.414182468090432 0.01\n",
            "4 770 0.26491671800613403 12.048685340679606 0.01\n",
            "4 780 1.2023990154266357 12.450975240860224 0.01\n",
            "4 790 0.2051762044429779 12.887171679773616 0.01\n",
            "4 800 0.38853153586387634 10.83401955871489 0.01\n",
            "4 810 0.5340001583099365 11.834793294103207 0.01\n",
            "4 820 0.19114220142364502 12.542953286567318 0.01\n",
            "4 830 0.6529870629310608 11.267188840005561 0.01\n",
            "4 840 0.2562565505504608 12.330965708641045 0.01\n",
            "4 850 0.28515931963920593 12.968086874319313 0.01\n",
            "4 860 0.27337655425071716 12.351727798723246 0.01\n",
            "tensor(128.7436, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 5\n",
            "5 10 0.8299893736839294 12.380430066266216 0.01\n",
            "5 20 0.6589642763137817 12.762260040346996 0.01\n",
            "5 30 0.9256883859634399 12.742979754440466 0.01\n",
            "5 40 0.1201803982257843 12.658830169232429 0.01\n",
            "5 50 0.3183048367500305 12.099273778873961 0.01\n",
            "5 60 0.6130437254905701 12.468779031138475 0.01\n",
            "5 70 1.2515016794204712 11.24750425538619 0.01\n",
            "5 80 0.39901405572891235 11.759439602664045 0.01\n",
            "5 90 0.19329428672790527 11.848625988108447 0.01\n",
            "5 100 0.7154374122619629 10.471995870427806 0.01\n",
            "5 110 0.22655177116394043 12.025721250853875 0.01\n",
            "5 120 0.7218383550643921 12.62335090462296 0.01\n",
            "5 130 0.5366933941841125 12.347555249391352 0.01\n",
            "5 140 0.9708094596862793 12.307934286146068 0.01\n",
            "5 150 1.095486044883728 11.806667863011482 0.01\n",
            "5 160 0.2159905731678009 11.238884687191641 0.01\n",
            "5 170 0.2787109315395355 11.571227967567598 0.01\n",
            "5 180 0.18975681066513062 12.15373916268719 0.01\n",
            "5 190 0.6830843687057495 12.022954898554714 0.01\n",
            "5 200 0.5886145234107971 12.55096665212867 0.01\n",
            "5 210 1.9296447038650513 12.015713259513003 0.01\n",
            "5 220 0.5449236631393433 9.67440401433295 0.01\n",
            "5 230 0.44091030955314636 10.604617882807744 0.01\n",
            "5 240 1.0909337997436523 9.421559521228982 0.01\n",
            "5 250 0.48005813360214233 12.499332091139573 0.01\n",
            "5 260 0.21098676323890686 7.044207004732746 0.01\n",
            "5 270 0.1723557412624359 13.124446342772298 0.01\n",
            "5 280 0.3291650116443634 12.094624784542264 0.01\n",
            "5 290 1.526411533355713 10.468441272469859 0.01\n",
            "5 300 0.2596103549003601 12.21566631571743 0.01\n",
            "5 310 0.6867988109588623 12.883975066369626 0.01\n",
            "5 320 0.8855646252632141 12.21303415412524 0.01\n",
            "5 330 0.3019365668296814 11.810582573754473 0.01\n",
            "5 340 0.39474281668663025 12.346991853165637 0.01\n",
            "5 350 0.25876301527023315 12.013510639592877 0.01\n",
            "5 360 2.7705435752868652 11.61200068936066 0.01\n",
            "5 370 0.4352934658527374 10.754811781998791 0.01\n",
            "5 380 0.8991200923919678 11.053589817414203 0.01\n",
            "5 390 0.39096754789352417 6.4840960624802655 0.01\n",
            "5 400 0.5610877871513367 7.929614177742634 0.01\n",
            "5 410 0.45501387119293213 12.62066355987347 0.01\n",
            "5 420 0.8677181005477905 12.809460111532564 0.01\n",
            "5 430 0.7911775708198547 12.379626158845596 0.01\n",
            "5 440 0.28712546825408936 12.072857899463468 0.01\n",
            "5 450 0.3513491749763489 11.978908481803389 0.01\n",
            "5 460 0.14437906444072723 12.615434590373985 0.01\n",
            "5 470 0.2714066803455353 12.268055424542961 0.01\n",
            "5 480 0.7113478183746338 12.25026523523609 0.01\n",
            "5 490 0.22778110206127167 11.561475841946729 0.01\n",
            "5 500 1.2842148542404175 11.569648011938437 0.01\n",
            "5 510 0.832206666469574 12.496259810409667 0.01\n",
            "5 520 0.5114119648933411 11.637251488192264 0.01\n",
            "5 530 0.44047874212265015 12.869131128138138 0.01\n",
            "5 540 0.5491496324539185 8.73505786504335 0.01\n",
            "5 550 0.4504603147506714 11.798265409382655 0.01\n",
            "5 560 1.2068982124328613 12.933586805860704 0.01\n",
            "5 570 0.3741793632507324 12.615918396504856 0.01\n",
            "5 580 0.1868675798177719 11.480070424607282 0.01\n",
            "5 590 0.11557328701019287 11.863698976922054 0.01\n",
            "5 600 0.23941507935523987 12.660970045573471 0.01\n",
            "5 610 0.2574189305305481 12.078281402983356 0.01\n",
            "5 620 0.10395856201648712 11.809776147191968 0.01\n",
            "5 630 0.5290038585662842 10.609184601356025 0.01\n",
            "5 640 0.31273484230041504 12.188288728756858 0.01\n",
            "5 650 0.6976495385169983 9.374238915134532 0.01\n",
            "5 660 0.8174575567245483 9.828964049783352 0.01\n",
            "5 670 1.966813564300537 10.527891305016261 0.01\n",
            "5 680 0.8310370445251465 12.495673455768552 0.01\n",
            "5 690 0.4888458251953125 12.15106321339591 0.01\n",
            "5 700 0.2897130846977234 11.736397894932715 0.01\n",
            "5 710 0.3717709183692932 12.776975086095156 0.01\n",
            "5 720 0.8675560355186462 12.042519012464425 0.01\n",
            "5 730 0.5021656155586243 10.456377579003002 0.01\n",
            "5 740 0.1369720697402954 10.252772160632233 0.01\n",
            "5 750 0.5684225559234619 10.214327027161955 0.01\n",
            "5 760 0.34225156903266907 12.945962087809486 0.01\n",
            "5 770 0.16042526066303253 11.925639332423005 0.01\n",
            "5 780 0.20817139744758606 11.988135635608813 0.01\n",
            "5 790 0.20234715938568115 12.426656119291993 0.01\n",
            "5 800 0.2555563449859619 10.891064145250404 0.01\n",
            "5 810 0.372265100479126 11.100865918590287 0.01\n",
            "5 820 0.3159829378128052 11.881216277559542 0.01\n",
            "5 830 1.2585530281066895 8.804811436608484 0.01\n",
            "5 840 0.4279130697250366 12.20180686527348 0.01\n",
            "5 850 0.34645092487335205 11.888195571302036 0.01\n",
            "5 860 4.213162422180176 12.566045225795676 0.01\n",
            "tensor(122.6495, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 6\n",
            "6 10 0.4616401791572571 12.99223898535304 0.01\n",
            "6 20 0.4600669741630554 11.911634798821135 0.01\n",
            "6 30 0.45015913248062134 11.766829428407512 0.01\n",
            "6 40 0.5981776714324951 12.695287511227509 0.01\n",
            "6 50 0.14362269639968872 12.750582155342757 0.01\n",
            "6 60 0.7357534766197205 12.206440948273354 0.01\n",
            "6 70 0.4008273482322693 11.260662489193876 0.01\n",
            "6 80 0.3565433919429779 12.477412359150295 0.01\n",
            "6 90 0.19579537212848663 10.995511933224277 0.01\n",
            "6 100 0.2148100584745407 11.784607928184847 0.01\n",
            "6 110 0.8483567237854004 8.843336799920301 0.01\n",
            "6 120 0.8209521770477295 12.949749259588302 0.01\n",
            "6 130 0.10632749646902084 12.500282010837859 0.01\n",
            "6 140 0.4975552260875702 12.666131651483157 0.01\n",
            "6 150 0.2395648956298828 12.694807204401394 0.01\n",
            "6 160 0.42159873247146606 12.027997335908527 0.01\n",
            "6 170 0.09897526353597641 12.763871792335841 0.01\n",
            "6 180 0.08729331195354462 12.502853105849967 0.01\n",
            "6 190 1.7914174795150757 11.52224340554588 0.01\n",
            "6 200 0.4900486469268799 12.092096813369533 0.01\n",
            "6 210 0.6654184460639954 11.175549861015172 0.01\n",
            "6 220 0.2559615671634674 10.709631458602768 0.01\n",
            "6 230 0.5402594804763794 10.50633271274314 0.01\n",
            "6 240 3.2969605922698975 11.996330430044711 0.01\n",
            "6 250 0.2884652018547058 11.702767347627773 0.01\n",
            "6 260 0.19967089593410492 12.372988779126151 0.01\n",
            "6 270 0.6681085824966431 11.74467954035541 0.01\n",
            "6 280 0.7523534297943115 12.62169847942728 0.01\n",
            "6 290 0.7112117409706116 10.52606165857527 0.01\n",
            "6 300 0.5532189607620239 12.304449992225935 0.01\n",
            "6 310 0.49675989151000977 10.686519629387583 0.01\n",
            "6 320 0.33145517110824585 12.28678811953118 0.01\n",
            "6 330 0.7492556571960449 12.441132850040933 0.01\n",
            "6 340 0.3263101577758789 12.74436397501751 0.01\n",
            "6 350 0.2416561245918274 12.063933638792774 0.01\n",
            "6 360 0.2186022251844406 11.252075574450734 0.01\n",
            "6 370 0.21602597832679749 12.739583396611529 0.01\n",
            "6 380 0.23132121562957764 11.70376333547031 0.01\n",
            "6 390 0.15138110518455505 10.287330258476805 0.01\n",
            "6 400 0.22672517597675323 12.825862697685228 0.01\n",
            "6 410 0.28586697578430176 12.36167510072237 0.01\n",
            "6 420 0.24536176025867462 12.419756760378103 0.01\n",
            "6 430 0.5308253765106201 11.981988325978447 0.01\n",
            "6 440 0.2106710970401764 12.106869876457685 0.01\n",
            "6 450 0.4996517300605774 12.18441167776978 0.01\n",
            "6 460 0.336961567401886 11.25731528195934 0.01\n",
            "6 470 0.4551071524620056 12.785514458487812 0.01\n",
            "6 480 0.6731477379798889 11.685763778847486 0.01\n",
            "6 490 0.3865205645561218 10.981168556181794 0.01\n",
            "6 500 0.6769606471061707 11.49351892258125 0.01\n",
            "6 510 0.7272997498512268 11.562902149760985 0.01\n",
            "6 520 0.7540751099586487 12.41878226883945 0.01\n",
            "6 530 0.7140710353851318 11.390453921770307 0.01\n",
            "6 540 0.3355123996734619 11.889796323331396 0.01\n",
            "6 550 7.235605239868164 9.95308341688271 0.01\n",
            "6 560 1.3167325258255005 7.717484013923216 0.01\n",
            "6 570 0.8259819746017456 10.664388507500636 0.01\n",
            "6 580 0.36499226093292236 12.700496370522389 0.01\n",
            "6 590 0.8037576079368591 10.930508737388413 0.01\n",
            "6 600 0.6945224404335022 12.585879121002776 0.01\n",
            "6 610 0.8985098600387573 11.07485946192864 0.01\n",
            "6 620 0.7136701345443726 11.944011874801998 0.01\n",
            "6 630 0.9734541177749634 10.611902356827818 0.01\n",
            "6 640 1.9871206283569336 12.56074855916735 0.01\n",
            "6 650 0.7244662046432495 11.460144457301487 0.01\n",
            "6 660 0.1905876249074936 12.020654883323697 0.01\n",
            "6 670 0.20996317267417908 12.512634786720385 0.01\n",
            "6 680 1.3551807403564453 11.260942142847075 0.01\n",
            "6 690 0.2063504457473755 11.00481589784157 0.01\n",
            "6 700 0.4893530309200287 12.007320098278832 0.01\n",
            "6 710 0.324834406375885 12.206520877123673 0.01\n",
            "6 720 0.5618916749954224 11.820176683897303 0.01\n",
            "6 730 0.4532891511917114 11.565923259291328 0.01\n",
            "6 740 0.6110028028488159 12.482081743674597 0.01\n",
            "6 750 0.5320597290992737 10.895711131315755 0.01\n",
            "6 760 1.2592109441757202 11.645789848156182 0.01\n",
            "6 770 0.2451895773410797 12.355975967394842 0.01\n",
            "6 780 0.15027093887329102 11.771089392334192 0.01\n",
            "6 790 0.44883060455322266 10.886922688075543 0.01\n",
            "6 800 0.4141829013824463 11.921656461465982 0.01\n",
            "6 810 0.7988460659980774 10.844012278131082 0.01\n",
            "6 820 0.32088199257850647 10.891650988656002 0.01\n",
            "6 830 0.30994513630867004 12.133513943923449 0.01\n",
            "6 840 0.7228515148162842 12.694595880911287 0.01\n",
            "6 850 0.31500717997550964 12.62609640623342 0.01\n",
            "6 860 0.4312501847743988 12.790895152910107 0.01\n",
            "tensor(122.2519, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 7\n",
            "7 10 0.2553461492061615 11.873740948131875 0.01\n",
            "7 20 0.20171260833740234 12.474332293137458 0.01\n",
            "7 30 0.3263579308986664 9.339520632743403 0.01\n",
            "7 40 0.17863856256008148 12.435802672288167 0.01\n",
            "7 50 0.2692807912826538 12.61215327737359 0.01\n",
            "7 60 0.5103016495704651 11.398548651955705 0.01\n",
            "7 70 0.3303447365760803 11.543450275458117 0.01\n",
            "7 80 0.18499508500099182 11.816829873507949 0.01\n",
            "7 90 0.3306116461753845 12.11204417081538 0.01\n",
            "7 100 1.228325366973877 12.271492206528816 0.01\n",
            "7 110 1.6819679737091064 11.941724961634936 0.01\n",
            "7 120 0.4039594829082489 11.782911241273721 0.01\n",
            "7 130 0.472883403301239 12.025790210471914 0.01\n",
            "7 140 1.2466226816177368 12.696815128517935 0.01\n",
            "7 150 0.2165362387895584 12.375014844368323 0.01\n",
            "7 160 0.9351711273193359 10.443398962956508 0.01\n",
            "7 170 0.23643216490745544 12.693635408257377 0.01\n",
            "7 180 1.1332793235778809 12.969460278881936 0.01\n",
            "7 190 0.2742638885974884 11.373967325941965 0.01\n",
            "7 200 0.16304247081279755 11.501650465935274 0.01\n",
            "7 210 0.17066296935081482 11.875438678450642 0.01\n",
            "7 220 0.6406327486038208 12.670952948826685 0.01\n",
            "7 230 0.3223206400871277 11.438828927721605 0.01\n",
            "7 240 1.038280725479126 12.573805421423534 0.01\n",
            "7 250 1.399380087852478 10.515934158576265 0.01\n",
            "7 260 0.2314291000366211 12.580641194264436 0.01\n",
            "7 270 0.7737867832183838 12.519058541907004 0.01\n",
            "7 280 0.5921469926834106 11.437674780718798 0.01\n",
            "7 290 0.42467015981674194 11.341704242014535 0.01\n",
            "7 300 0.644426703453064 12.032068848050201 0.01\n",
            "7 310 0.4011201560497284 12.368573890901102 0.01\n",
            "7 320 0.3997788429260254 12.287562015611748 0.01\n",
            "7 330 0.26267218589782715 12.51546304080733 0.01\n",
            "7 340 0.2974590063095093 11.155345680276497 0.01\n",
            "7 350 0.3302534222602844 11.703453091570651 0.01\n",
            "7 360 0.7457888126373291 11.899013239316949 0.01\n",
            "7 370 0.3019912838935852 11.88834720301127 0.01\n",
            "7 380 0.5968769788742065 11.047250305858121 0.01\n",
            "7 390 0.38137170672416687 12.026367778322566 0.01\n",
            "7 400 0.3826342523097992 11.857988684272836 0.01\n",
            "7 410 0.40933066606521606 12.508110372942213 0.01\n",
            "7 420 0.6248809099197388 11.188413335760782 0.01\n",
            "7 430 1.3605560064315796 8.862541256217487 0.01\n",
            "7 440 0.5059564709663391 9.96391845335275 0.01\n",
            "7 450 0.9232357740402222 10.958109301438506 0.01\n",
            "7 460 0.4887847304344177 7.893823164075733 0.01\n",
            "7 470 0.1491086632013321 9.879329695786279 0.01\n",
            "7 480 0.15700125694274902 11.29013254984667 0.01\n",
            "7 490 0.5583634376525879 11.899409895596913 0.01\n",
            "7 500 0.3952888548374176 11.274093974885107 0.01\n",
            "7 510 0.09349103271961212 11.10283473685952 0.01\n",
            "7 520 1.0256657600402832 11.658236089965374 0.01\n",
            "7 530 0.7138473987579346 12.449256779218313 0.01\n",
            "7 540 0.6615267395973206 11.480431785133863 0.01\n",
            "7 550 0.9129768013954163 9.092799887269594 0.01\n",
            "7 560 1.8946819305419922 10.318896986418988 0.01\n",
            "7 570 0.5722771883010864 12.516751580334248 0.01\n",
            "7 580 0.18310192227363586 11.63307169601997 0.01\n",
            "7 590 0.14320173859596252 12.36230360165261 0.01\n",
            "7 600 0.34465906023979187 12.688892804929976 0.01\n",
            "7 610 0.27040931582450867 11.921461623293201 0.01\n",
            "7 620 0.6870080828666687 11.984333491198482 0.01\n",
            "7 630 1.788746953010559 11.912793537905728 0.01\n",
            "7 640 0.25746142864227295 11.76192933258553 0.01\n",
            "7 650 0.3621506989002228 10.837350751764108 0.01\n",
            "7 660 0.9504505395889282 11.743265575532996 0.01\n",
            "7 670 0.7371790409088135 5.3153632836273905 0.01\n",
            "7 680 1.0119861364364624 11.425204588001272 0.01\n",
            "7 690 0.6166325211524963 12.902364877595609 0.01\n",
            "7 700 0.3059612512588501 11.131151097608793 0.01\n",
            "7 710 1.537883996963501 12.158601137503407 0.01\n",
            "7 720 0.30225324630737305 11.229511609212405 0.01\n",
            "7 730 0.49356648325920105 10.022740700692687 0.01\n",
            "7 740 0.23892742395401 6.714982539349003 0.01\n",
            "7 750 0.15964339673519135 11.032525045949082 0.01\n",
            "7 760 1.6199337244033813 9.953231035740876 0.01\n",
            "7 770 0.7701733112335205 12.333567352231768 0.01\n",
            "7 780 0.2639007270336151 11.124847737362284 0.01\n",
            "7 790 0.16465380787849426 12.15538580916645 0.01\n",
            "7 800 0.37019869685173035 12.446052099599775 0.01\n",
            "7 810 0.5591809749603271 11.417887816630143 0.01\n",
            "7 820 0.4016917645931244 11.319367764404733 0.01\n",
            "7 830 0.6910463571548462 8.995445233150981 0.01\n",
            "7 840 0.32818806171417236 11.785286739773795 0.01\n",
            "7 850 0.7084313631057739 12.865381090886226 0.01\n",
            "7 860 0.43903249502182007 10.54361147822961 0.01\n",
            "tensor(116.7791, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 8\n",
            "8 10 0.24925494194030762 11.938648787506601 0.01\n",
            "8 20 0.30457359552383423 12.978871408944066 0.01\n",
            "8 30 0.1581038534641266 12.778814419758502 0.01\n",
            "8 40 0.38329848647117615 12.20895476770392 0.01\n",
            "8 50 0.15498486161231995 12.02768690994155 0.01\n",
            "8 60 0.9681696891784668 12.039416615716702 0.01\n",
            "8 70 0.37769466638565063 9.776026580287724 0.01\n",
            "8 80 0.20572297275066376 11.9106876759301 0.01\n",
            "8 90 0.46729880571365356 11.142654291264716 0.01\n",
            "8 100 2.217825174331665 11.739231071721951 0.01\n",
            "8 110 0.33002153038978577 10.45271635043718 0.01\n",
            "8 120 0.13175085186958313 12.414343832273474 0.01\n",
            "8 130 0.2174576222896576 11.15556820388156 0.01\n",
            "8 140 0.4122704863548279 11.751902646157898 0.01\n",
            "8 150 0.41883185505867004 9.918009731666816 0.01\n",
            "8 160 0.2818387448787689 11.459533892516598 0.01\n",
            "8 170 0.2062063217163086 11.983383331904802 0.01\n",
            "8 180 0.14871087670326233 12.1359978125538 0.01\n",
            "8 190 0.20299825072288513 12.670646725589664 0.01\n",
            "8 200 0.4033105969429016 12.189989035908308 0.01\n",
            "8 210 0.13924214243888855 10.896475398975898 0.01\n",
            "8 220 1.272618293762207 11.609027759037412 0.01\n",
            "8 230 0.3409242331981659 9.793345957443828 0.01\n",
            "8 240 0.7433626055717468 12.270666485282264 0.01\n",
            "8 250 0.27111461758613586 11.253773792466903 0.01\n",
            "8 260 0.47505730390548706 12.69651726233405 0.01\n",
            "8 270 0.26388925313949585 12.655869926156813 0.01\n",
            "8 280 0.2834628224372864 11.598049158172348 0.01\n",
            "8 290 0.42545270919799805 11.87069969504645 0.01\n",
            "8 300 0.4216306805610657 11.701575650546083 0.01\n",
            "8 310 0.4139460325241089 12.079933758145229 0.01\n",
            "8 320 0.4752970337867737 12.135734456738113 0.01\n",
            "8 330 0.28011757135391235 12.723477801885783 0.01\n",
            "8 340 0.266851007938385 12.575171981645298 0.01\n",
            "8 350 0.2197699248790741 11.465634633875979 0.01\n",
            "8 360 0.36850234866142273 11.552893557332313 0.01\n",
            "8 370 0.5578845739364624 12.61155599655718 0.01\n",
            "8 380 0.8756479024887085 10.93133487361723 0.01\n",
            "8 390 0.733380913734436 12.327767282496001 0.01\n",
            "8 400 0.23284968733787537 10.80747140504982 0.01\n",
            "8 410 0.6757773756980896 12.765357683349857 0.01\n",
            "8 420 0.8175712823867798 11.959396884053012 0.01\n",
            "8 430 0.7816208600997925 11.271193090512353 0.01\n",
            "8 440 1.0080809593200684 12.398114996449175 0.01\n",
            "8 450 0.4529234766960144 12.02665227243239 0.01\n",
            "8 460 0.21930988132953644 12.818562035173613 0.01\n",
            "8 470 0.42683666944503784 12.423398666965825 0.01\n",
            "8 480 0.26385778188705444 10.846073334984004 0.01\n",
            "8 490 0.17927119135856628 9.745667889433955 0.01\n",
            "8 500 2.8839192390441895 12.642480206111145 0.01\n",
            "8 510 0.33132797479629517 12.708365810186711 0.01\n",
            "8 520 0.36946240067481995 10.733935292676232 0.01\n",
            "8 530 1.1203129291534424 9.959789729266042 0.01\n",
            "8 540 0.17006239295005798 12.450531721471455 0.01\n",
            "8 550 4.196435928344727 9.121805003471536 0.01\n",
            "8 560 1.7148549556732178 8.040344498231358 0.01\n",
            "8 570 0.5137318968772888 10.535513964698287 0.01\n",
            "8 580 0.3910076916217804 12.59928161664042 0.01\n",
            "8 590 0.25828543305397034 12.481580290412287 0.01\n",
            "8 600 0.7163474559783936 11.562288555225292 0.01\n",
            "8 610 0.13234826922416687 11.443315156041647 0.01\n",
            "8 620 0.23554928600788116 11.91214224804798 0.01\n",
            "8 630 0.41428709030151367 11.10487775657484 0.01\n",
            "8 640 1.116720199584961 9.216225005493298 0.01\n",
            "8 650 0.5356630682945251 12.568728190237318 0.01\n",
            "8 660 0.6095309853553772 10.53995513179329 0.01\n",
            "8 670 0.16492024064064026 9.069843981446443 0.01\n",
            "8 680 0.9522799253463745 12.031680555356349 0.01\n",
            "8 690 0.6824554204940796 11.88682263802303 0.01\n",
            "8 700 0.20208072662353516 11.527056663716863 0.01\n",
            "8 710 0.4010343849658966 11.147562943019723 0.01\n",
            "8 720 0.7041864395141602 12.813118900729965 0.01\n",
            "8 730 0.16029725968837738 12.175675234355326 0.01\n",
            "8 740 0.6164647936820984 12.432816818953187 0.01\n",
            "8 750 0.2308375984430313 11.246335620065505 0.01\n",
            "8 760 0.21723221242427826 10.122189917277488 0.01\n",
            "8 770 0.30526837706565857 12.346555710751234 0.01\n",
            "8 780 1.2537857294082642 11.659548621366644 0.01\n",
            "8 790 0.18930819630622864 10.879559246371473 0.01\n",
            "8 800 0.24126318097114563 12.876044817299434 0.01\n",
            "8 810 0.12831789255142212 11.445532261222473 0.01\n",
            "8 820 0.6460869908332825 9.54824742373651 0.01\n",
            "8 830 0.34553593397140503 11.751696853973195 0.01\n",
            "8 840 0.3691682517528534 12.659842699121889 0.01\n",
            "8 850 0.2439539134502411 12.019853961048637 0.01\n",
            "8 860 0.7060329914093018 10.816543418414385 0.01\n",
            "tensor(112.0160, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 9\n",
            "9 10 0.6435737609863281 11.913444898985201 0.01\n",
            "9 20 0.21633678674697876 11.876455868247154 0.01\n",
            "9 30 1.0569188594818115 12.991886854256865 0.01\n",
            "9 40 0.2432960718870163 12.682725046112909 0.01\n",
            "9 50 0.3107147514820099 11.627508680495394 0.01\n",
            "9 60 0.2265394926071167 11.980567978868427 0.01\n",
            "9 70 0.22882992029190063 11.642646081162292 0.01\n",
            "9 80 0.3323180079460144 10.192558319248906 0.01\n",
            "9 90 0.17587585747241974 11.664947690228255 0.01\n",
            "9 100 0.12572380900382996 11.471483027524505 0.01\n",
            "9 110 0.4022402763366699 10.251769762526436 0.01\n",
            "9 120 0.6585707664489746 11.80599489400989 0.01\n",
            "9 130 0.3552234172821045 12.707682379442558 0.01\n",
            "9 140 0.5352513790130615 11.89589998411726 0.01\n",
            "9 150 0.2799830734729767 10.91866457802251 0.01\n",
            "9 160 1.6578264236450195 12.553690147758555 0.01\n",
            "9 170 0.18935316801071167 11.662190784228267 0.01\n",
            "9 180 0.36612358689308167 11.784492041338261 0.01\n",
            "9 190 0.5340027809143066 12.44774196994833 0.01\n",
            "9 200 0.38194504380226135 11.96038587556283 0.01\n",
            "9 210 0.33247053623199463 12.99093116606373 0.01\n",
            "9 220 0.09373011440038681 12.454395096994118 0.01\n",
            "9 230 0.21544593572616577 11.887732276155756 0.01\n",
            "9 240 0.4161308705806732 12.851574157768143 0.01\n",
            "9 250 0.24445129930973053 12.571732586797191 0.01\n",
            "9 260 0.21023856103420258 12.17239788144816 0.01\n",
            "9 270 0.2417297065258026 12.04987089846766 0.01\n",
            "9 280 0.4029060900211334 12.917325408160515 0.01\n",
            "9 290 0.24185316264629364 12.131574951299335 0.01\n",
            "9 300 0.18444475531578064 11.90530377754534 0.01\n",
            "9 310 0.3233228921890259 12.103026980233732 0.01\n",
            "9 320 0.5066664814949036 12.572844294930833 0.01\n",
            "9 330 0.36488014459609985 11.887066879035046 0.01\n",
            "9 340 0.9438365697860718 11.591718629628733 0.01\n",
            "9 350 0.959921658039093 11.207352388103272 0.01\n",
            "9 360 0.3097972869873047 12.182801391015907 0.01\n",
            "9 370 0.41572481393814087 12.589250687721268 0.01\n",
            "9 380 0.10431420803070068 12.5676642813107 0.01\n",
            "9 390 0.1076076328754425 11.113565304024334 0.01\n",
            "9 400 0.08470945060253143 12.538809870981291 0.01\n",
            "9 410 0.2669852674007416 12.435139025391758 0.01\n",
            "9 420 0.7956094741821289 12.009451675410396 0.01\n",
            "9 430 0.3420107066631317 9.64385174284926 0.01\n",
            "9 440 0.3483220934867859 12.20089289285504 0.01\n",
            "9 450 0.24415947496891022 12.152506671225217 0.01\n",
            "9 460 0.37712010741233826 12.974806292974577 0.01\n",
            "9 470 0.29037392139434814 12.085973046257049 0.01\n",
            "9 480 0.3324975371360779 12.317802104212095 0.01\n",
            "9 490 0.5856491327285767 10.303333048788051 0.01\n",
            "9 500 0.25865092873573303 12.598893696654207 0.01\n",
            "9 510 0.44997188448905945 10.807882173247277 0.01\n",
            "9 520 1.8507535457611084 11.491598028706365 0.01\n",
            "9 530 0.3123369514942169 12.129654117356164 0.01\n",
            "9 540 0.2928387224674225 12.803682699873239 0.01\n",
            "9 550 0.5814294815063477 12.412883426050813 0.01\n",
            "9 560 0.20785067975521088 9.864459357915866 0.01\n",
            "9 570 0.31725409626960754 9.27876646037619 0.01\n",
            "9 580 0.21094565093517303 11.87895334577597 0.01\n",
            "9 590 0.4311901926994324 11.910543929371107 0.01\n",
            "9 600 0.19902348518371582 12.79418233286256 0.01\n",
            "9 610 0.16751159727573395 11.665442449509733 0.01\n",
            "9 620 0.7175371646881104 9.08906103304189 0.01\n",
            "9 630 0.09383559226989746 12.29718986772097 0.01\n",
            "9 640 0.1870054453611374 12.937456527405278 0.01\n",
            "9 650 0.4429067373275757 11.237732896788609 0.01\n",
            "9 660 1.531859278678894 9.60433423133685 0.01\n",
            "9 670 0.10290057957172394 12.71840434104625 0.01\n",
            "9 680 1.077411413192749 12.157825779447402 0.01\n",
            "9 690 0.5039207935333252 10.679139187295954 0.01\n",
            "9 700 0.3181329369544983 9.686491280133856 0.01\n",
            "9 710 0.6523746252059937 11.889366605461937 0.01\n",
            "9 720 0.3695980906486511 11.89195380789961 0.01\n",
            "9 730 0.5531470775604248 11.853581852390645 0.01\n",
            "9 740 0.7145849466323853 10.727915881229043 0.01\n",
            "9 750 0.27556443214416504 10.602212046566475 0.01\n",
            "9 760 0.43227705359458923 11.567302813017099 0.01\n",
            "9 770 0.5391143560409546 11.47324027843979 0.01\n",
            "9 780 0.2874714732170105 11.761187249822992 0.01\n",
            "9 790 0.39502155780792236 12.357950950386895 0.01\n",
            "9 800 1.0663474798202515 9.760585127240274 0.01\n",
            "9 810 0.13878312706947327 12.435682841913872 0.01\n",
            "9 820 0.4989718794822693 11.975513933302878 0.01\n",
            "9 830 0.21418990194797516 12.34799146242732 0.01\n",
            "9 840 0.2732808589935303 12.698141283073248 0.01\n",
            "9 850 0.39926013350486755 12.459907790294215 0.01\n",
            "9 860 0.39907634258270264 12.413829436164718 0.01\n",
            "tensor(110.2535, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 10\n",
            "10 10 0.5066238641738892 11.486798601910904 0.01\n",
            "10 20 0.14010289311408997 12.673613886179918 0.01\n",
            "10 30 0.5943674445152283 12.633464809434955 0.01\n",
            "10 40 0.6045535802841187 11.036712697878073 0.01\n",
            "10 50 0.3493843376636505 12.12229217259864 0.01\n",
            "10 60 0.30482709407806396 11.608810875067636 0.01\n",
            "10 70 0.17137648165225983 11.87128766115223 0.01\n",
            "10 80 0.9467923045158386 11.614163046156998 0.01\n",
            "10 90 0.14157584309577942 11.392217376218094 0.01\n",
            "10 100 0.35419324040412903 10.64263915276117 0.01\n",
            "10 110 0.29996323585510254 11.955774820722697 0.01\n",
            "10 120 0.5339568853378296 12.241577046809548 0.01\n",
            "10 130 0.8753517866134644 12.899775638906274 0.01\n",
            "10 140 0.26054996252059937 11.15958999328185 0.01\n",
            "10 150 0.6049821376800537 12.049161266506607 0.01\n",
            "10 160 0.7708644270896912 12.131487228731666 0.01\n",
            "10 170 0.18683075904846191 9.24310536133978 0.01\n",
            "10 180 0.20777952671051025 12.217134060511793 0.01\n",
            "10 190 1.0565845966339111 11.862692358819022 0.01\n",
            "10 200 0.5610983967781067 12.635072739034921 0.01\n",
            "10 210 0.23297640681266785 11.679093251245895 0.01\n",
            "10 220 0.32717350125312805 11.618425809096562 0.01\n",
            "10 230 0.22742275893688202 12.187421046476066 0.01\n",
            "10 240 0.29391559958457947 12.268163075183852 0.01\n",
            "10 250 0.28439223766326904 12.73876118907747 0.01\n",
            "10 260 0.3085119128227234 12.659011648525109 0.01\n",
            "10 270 0.4160856008529663 12.288606029887216 0.01\n",
            "10 280 0.133982315659523 12.252376574067245 0.01\n",
            "10 290 1.113793134689331 10.426919589392112 0.01\n",
            "10 300 0.34798726439476013 9.218042890242582 0.01\n",
            "10 310 0.4283885657787323 8.707733366273889 0.01\n",
            "10 320 0.33042657375335693 11.875304187057877 0.01\n",
            "10 330 0.2853703796863556 12.47350687123242 0.01\n",
            "10 340 1.0402390956878662 9.673996790560118 0.01\n",
            "10 350 0.314850389957428 12.24419471938042 0.01\n",
            "10 360 0.5145449638366699 11.526518139072174 0.01\n",
            "10 370 0.526210606098175 11.388157608931753 0.01\n",
            "10 380 0.12512798607349396 11.861954280769133 0.01\n",
            "10 390 0.19932964444160461 11.686154484835779 0.01\n",
            "10 400 0.32665953040122986 12.747084893109582 0.01\n",
            "10 410 0.47051775455474854 12.625754340917318 0.01\n",
            "10 420 1.0365607738494873 11.628467719409386 0.01\n",
            "10 430 0.18219900131225586 12.091626204408339 0.01\n",
            "10 440 0.4285304546356201 12.443430476926512 0.01\n",
            "10 450 0.534915566444397 11.294654950949635 0.01\n",
            "10 460 0.09432660043239594 11.70633572755297 0.01\n",
            "10 470 0.5867681503295898 11.7434053127078 0.01\n",
            "10 480 1.062321424484253 11.536806318652982 0.01\n",
            "10 490 3.132880210876465 12.271187035731318 0.01\n",
            "10 500 0.45266085863113403 12.670522326627466 0.01\n",
            "10 510 1.07062566280365 12.322651440256072 0.01\n",
            "10 520 0.4029252231121063 9.719729864857115 0.01\n",
            "10 530 0.4689779281616211 5.914547053129061 0.01\n",
            "10 540 0.13272538781166077 9.36240065447232 0.01\n",
            "10 550 0.1278448849916458 12.201327678192216 0.01\n",
            "10 560 0.3043890595436096 10.430977808422393 0.01\n",
            "10 570 0.13671541213989258 11.032467007252519 0.01\n",
            "10 580 0.1973925232887268 10.492547366577755 0.01\n",
            "10 590 0.21239227056503296 10.486120460390826 0.01\n",
            "10 600 0.3774053156375885 12.429178597225567 0.01\n",
            "10 610 0.5150695443153381 11.55253557573262 0.01\n",
            "10 620 0.387798011302948 11.431533646855534 0.01\n",
            "10 630 0.3439902067184448 11.557843932192835 0.01\n",
            "10 640 0.9551432132720947 10.869500937147922 0.01\n",
            "10 650 0.7285608649253845 11.682549585959674 0.01\n",
            "10 660 0.5434951782226562 11.788988466235597 0.01\n",
            "10 670 0.2554786205291748 12.318272395001975 0.01\n",
            "10 680 0.2860763669013977 12.703804580337637 0.01\n",
            "10 690 0.4130721986293793 8.561258622000574 0.01\n",
            "10 700 0.35202959179878235 7.862069482610603 0.01\n",
            "10 710 0.18671923875808716 12.458362634155687 0.01\n",
            "10 720 0.28786176443099976 12.651823318660917 0.01\n",
            "10 730 0.42094510793685913 10.313682651019398 0.01\n",
            "10 740 0.21309404075145721 12.263706125122255 0.01\n",
            "10 750 0.33129414916038513 10.911620072127501 0.01\n",
            "10 760 0.8952144980430603 12.810291471038058 0.01\n",
            "10 770 0.720771849155426 12.415216566038378 0.01\n",
            "10 780 0.5611514449119568 10.637773787422264 0.01\n",
            "10 790 0.5223461389541626 12.163758021401037 0.01\n",
            "10 800 0.84831303358078 11.554644009011087 0.01\n",
            "10 810 0.24637667834758759 12.126410254371617 0.01\n",
            "10 820 0.48543012142181396 12.613006633828315 0.01\n",
            "10 830 0.4802244007587433 12.395953135817319 0.01\n",
            "10 840 0.07633069157600403 12.968297377310217 0.01\n",
            "10 850 0.33795514702796936 12.278910728191436 0.01\n",
            "10 860 0.26317641139030457 12.150104031708489 0.01\n",
            "tensor(105.2781, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 11\n",
            "11 10 0.11839687824249268 11.63960091328446 0.01\n",
            "11 20 0.32065215706825256 12.386462701553363 0.01\n",
            "11 30 0.3213726282119751 12.229539457379033 0.01\n",
            "11 40 0.14703331887722015 11.24982716781184 0.01\n",
            "11 50 0.217073455452919 12.342332175649203 0.01\n",
            "11 60 0.1045263409614563 10.149377899601278 0.01\n",
            "11 70 0.18468497693538666 10.435156015298313 0.01\n",
            "11 80 0.12408920377492905 9.68903657785116 0.01\n",
            "11 90 0.2048746943473816 11.281174458894544 0.01\n",
            "11 100 1.4213883876800537 9.64043817833288 0.01\n",
            "11 110 0.25193387269973755 11.529235034074611 0.01\n",
            "11 120 0.3997916579246521 11.25527619919107 0.01\n",
            "11 130 0.3162232041358948 12.492342516753537 0.01\n",
            "11 140 0.5085089206695557 12.268620611485275 0.01\n",
            "11 150 0.31392815709114075 9.810187564722606 0.01\n",
            "11 160 0.14428681135177612 11.664355656413123 0.01\n",
            "11 170 0.4869967997074127 9.53581411739296 0.01\n",
            "11 180 0.1504979431629181 13.108346980360782 0.01\n",
            "11 190 0.2741298973560333 12.278802888851807 0.01\n",
            "11 200 0.2606683373451233 12.802764268713414 0.01\n",
            "11 210 0.22790323197841644 10.868810859730852 0.01\n",
            "11 220 0.774488091468811 10.685015504759697 0.01\n",
            "11 230 0.20176522433757782 9.792454239820694 0.01\n",
            "11 240 0.6225525736808777 9.20371676813647 0.01\n",
            "11 250 0.3628392517566681 11.992651673602781 0.01\n",
            "11 260 0.3191860318183899 12.256816337146143 0.01\n",
            "11 270 0.09124808013439178 12.39730879076624 0.01\n",
            "11 280 0.4057556092739105 11.93906508297177 0.01\n",
            "11 290 0.10720213502645493 12.196848212734372 0.01\n",
            "11 300 0.8726803064346313 11.293605735249571 0.01\n",
            "11 310 0.16731607913970947 12.931044821444123 0.01\n",
            "11 320 0.2007293403148651 11.946810071165812 0.01\n",
            "11 330 1.1153697967529297 12.529876166655589 0.01\n",
            "11 340 0.7225874662399292 11.269868117975735 0.01\n",
            "11 350 0.5307483077049255 7.814358168453912 0.01\n",
            "11 360 0.4334741234779358 12.261716340461609 0.01\n",
            "11 370 0.3948415219783783 12.68092285272874 0.01\n",
            "11 380 0.3117070198059082 12.089291131487453 0.01\n",
            "11 390 0.23540101945400238 11.014468243787086 0.01\n",
            "11 400 0.6957680583000183 11.57593847589004 0.01\n",
            "11 410 0.08849604427814484 12.529763874087092 0.01\n",
            "11 420 0.17252643406391144 12.285573479261542 0.01\n",
            "11 430 0.3295874297618866 11.850416987757011 0.01\n",
            "11 440 0.4257954955101013 12.0899270880924 0.01\n",
            "11 450 1.0560070276260376 11.943391176085335 0.01\n",
            "11 460 0.5284359455108643 9.534020449832672 0.01\n",
            "11 470 1.4556283950805664 12.673384120893026 0.01\n",
            "11 480 0.19355475902557373 11.916660451643926 0.01\n",
            "11 490 0.3103405237197876 11.537274399573642 0.01\n",
            "11 500 0.1088842824101448 12.185340883010031 0.01\n",
            "11 510 0.23107127845287323 11.995129655408105 0.01\n",
            "11 520 0.3112759590148926 11.08175770782239 0.01\n",
            "11 530 0.252422034740448 12.536917185077249 0.01\n",
            "11 540 0.4726874530315399 12.759765540330363 0.01\n",
            "11 550 0.8400347232818604 11.091428235211675 0.01\n",
            "11 560 0.24221956729888916 12.14332368268674 0.01\n",
            "11 570 0.5753151178359985 12.448868805140648 0.01\n",
            "11 580 0.2816181480884552 11.163102154011167 0.01\n",
            "11 590 0.724391520023346 11.93899711438219 0.01\n",
            "11 600 0.08756615221500397 12.516546143212153 0.01\n",
            "11 610 0.23887649178504944 11.246388391908496 0.01\n",
            "11 620 0.6358110904693604 10.025250135046633 0.01\n",
            "11 630 0.5046154856681824 10.841468584249217 0.01\n",
            "11 640 0.7099846005439758 11.33068951730547 0.01\n",
            "11 650 0.349354088306427 11.551350417205143 0.01\n",
            "11 660 0.13038261234760284 12.43966613454419 0.01\n",
            "11 670 0.2668791115283966 11.231654152250988 0.01\n",
            "11 680 0.5405448079109192 9.410440703824818 0.01\n",
            "11 690 0.6571431159973145 11.984470463521662 0.01\n",
            "11 700 0.2724978029727936 11.399911802497389 0.01\n",
            "11 710 1.9608073234558105 12.742099043348553 0.01\n",
            "11 720 0.47724151611328125 11.174723616149157 0.01\n",
            "11 730 0.6796506643295288 7.613662756793871 0.01\n",
            "11 740 0.11969379335641861 12.37955308152971 0.01\n",
            "11 750 0.543987512588501 12.258437294045269 0.01\n",
            "11 760 0.20489469170570374 12.053818669850898 0.01\n",
            "11 770 0.20886407792568207 12.909463329655296 0.01\n",
            "11 780 1.5123558044433594 12.258007385250249 0.01\n",
            "11 790 0.7062749862670898 10.068429874471814 0.01\n",
            "11 800 0.21609050035476685 10.634516952445425 0.01\n",
            "11 810 1.6271729469299316 10.122983891647998 0.01\n",
            "11 820 1.4066599607467651 12.36328747055126 0.01\n",
            "11 830 0.6246594786643982 12.709665493971377 0.01\n",
            "11 840 0.4891695976257324 12.55954496658966 0.01\n",
            "11 850 0.5513074398040771 12.18283677735911 0.01\n",
            "11 860 0.19035916030406952 12.752607763355025 0.01\n",
            "tensor(104.9049, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 12\n",
            "12 10 0.297943651676178 11.393292732616572 0.01\n",
            "12 20 0.7406951189041138 12.655564431510193 0.01\n",
            "12 30 0.9586082696914673 12.614979277322956 0.01\n",
            "12 40 0.22564008831977844 11.568882128144907 0.01\n",
            "12 50 1.159069299697876 10.395939086294415 0.01\n",
            "12 60 0.3017747700214386 12.052909414323933 0.01\n",
            "12 70 1.003208041191101 12.696373138552676 0.01\n",
            "12 80 0.13299784064292908 11.76173143469263 0.01\n",
            "12 90 0.41440334916114807 11.712612206315807 0.01\n",
            "12 100 0.1807027906179428 10.697121754466057 0.01\n",
            "12 110 0.48349428176879883 11.135539615713672 0.01\n",
            "12 120 0.2186773121356964 11.092623569719914 0.01\n",
            "12 130 0.24467523396015167 12.659842699121889 0.01\n",
            "12 140 0.23840484023094177 12.32929833328434 0.01\n",
            "12 150 1.0126062631607056 11.91892930646322 0.01\n",
            "12 160 0.5959216356277466 12.105524585219573 0.01\n",
            "12 170 0.4323360323905945 11.352940141536054 0.01\n",
            "12 180 1.40913987159729 7.766528484696983 0.01\n",
            "12 190 0.07144580036401749 12.595933045786081 0.01\n",
            "12 200 0.28283581137657166 11.252701968543546 0.01\n",
            "12 210 1.0045971870422363 10.88430939663802 0.01\n",
            "12 220 1.4982819557189941 8.113111086824693 0.01\n",
            "12 230 0.19756977260112762 12.073970015890197 0.01\n",
            "12 240 0.559002161026001 10.991211467871576 0.01\n",
            "12 250 0.39261507987976074 12.370799184483058 0.01\n",
            "12 260 0.23882579803466797 11.912066127907687 0.01\n",
            "12 270 0.4100612998008728 12.582169651990533 0.01\n",
            "12 280 0.20757977664470673 11.762498326124861 0.01\n",
            "12 290 0.1499459445476532 11.568467316759685 0.01\n",
            "12 300 0.18623505532741547 11.841216671642497 0.01\n",
            "12 310 0.2548985183238983 12.467528143910148 0.01\n",
            "12 320 0.47672683000564575 12.244096424758197 0.01\n",
            "12 330 0.753416121006012 12.532581154085193 0.01\n",
            "12 340 0.32867780327796936 11.162233189668363 0.01\n",
            "12 350 0.37228429317474365 11.688214262823639 0.01\n",
            "12 360 0.5313313603401184 12.023997517399346 0.01\n",
            "12 370 0.19662396609783173 12.224495892674723 0.01\n",
            "12 380 0.15084628760814667 10.443210444445413 0.01\n",
            "12 390 0.5996118783950806 7.06581946994191 0.01\n",
            "12 400 0.1306191384792328 11.688092121350609 0.01\n",
            "12 410 0.10176707804203033 12.82102079291074 0.01\n",
            "12 420 0.31492382287979126 11.18486286269336 0.01\n",
            "12 430 0.14710873365402222 12.332071499661142 0.01\n",
            "12 440 4.244457721710205 10.796517513145558 0.01\n",
            "12 450 0.7229946255683899 10.940680325875089 0.01\n",
            "12 460 1.2506115436553955 12.144079611209152 0.01\n",
            "12 470 0.2483811378479004 12.38324456463975 0.01\n",
            "12 480 0.4039447009563446 9.243859086546355 0.01\n",
            "12 490 0.27288272976875305 9.68452308797061 0.01\n",
            "12 500 0.10855784267187119 12.193931676485937 0.01\n",
            "12 510 0.3403394818305969 11.724300443472925 0.01\n",
            "12 520 0.3367336392402649 10.729459556079682 0.01\n",
            "12 530 2.071242094039917 11.482631861720149 0.01\n",
            "12 540 0.8739742636680603 12.321809770510441 0.01\n",
            "12 550 0.1491699069738388 11.931482864315532 0.01\n",
            "12 560 0.8272255063056946 10.528803063022018 0.01\n",
            "12 570 0.21138626337051392 12.606959775772285 0.01\n",
            "12 580 0.8293496370315552 10.161087840767864 0.01\n",
            "12 590 0.2223750799894333 12.709944719990121 0.01\n",
            "12 600 0.329603374004364 12.392995539849576 0.01\n",
            "12 610 0.2376740574836731 11.26139566478431 0.01\n",
            "12 620 0.2682621479034424 9.80657500264494 0.01\n",
            "12 630 0.12000184506177902 11.018490229482701 0.01\n",
            "12 640 0.3265862464904785 12.030386427357536 0.01\n",
            "12 650 0.4159443974494934 11.385437251623271 0.01\n",
            "12 660 0.6410610675811768 12.524030271677708 0.01\n",
            "12 670 0.21505849063396454 11.432748880726693 0.01\n",
            "12 680 0.21068543195724487 11.945619189072573 0.01\n",
            "12 690 1.101240634918213 12.358142111280038 0.01\n",
            "12 700 0.5780825614929199 10.55506721006811 0.01\n",
            "12 710 0.4049876630306244 12.094886359130667 0.01\n",
            "12 720 0.19611366093158722 12.50187484724019 0.01\n",
            "12 730 0.3394259810447693 11.650812081077552 0.01\n",
            "12 740 0.5521913170814514 11.189040123833475 0.01\n",
            "12 750 0.1705644130706787 12.362239837775784 0.01\n",
            "12 760 0.3721379041671753 10.559491650449262 0.01\n",
            "12 770 0.8189420700073242 12.194614145046952 0.01\n",
            "12 780 1.010819435119629 10.004959180387504 0.01\n",
            "12 790 0.1219729408621788 10.414873765434015 0.01\n",
            "12 800 6.985854148864746 11.80044790060658 0.01\n",
            "12 810 0.1753472089767456 11.942473000407166 0.01\n",
            "12 820 0.5799559950828552 11.171256209132919 0.01\n",
            "12 830 0.2176615446805954 12.274005955124407 0.01\n",
            "12 840 0.9684221148490906 12.328446695971794 0.01\n",
            "12 850 0.5108197927474976 11.40961817681042 0.01\n",
            "12 860 0.4153495728969574 11.834776597395635 0.01\n",
            "tensor(102.0677, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 13\n",
            "13 10 0.3403642773628235 12.192274570819583 0.01\n",
            "13 20 0.28400978446006775 11.955936701409723 0.01\n",
            "13 30 1.4501726627349854 11.962048773044847 0.01\n",
            "13 40 0.3597533702850342 10.17441032601115 0.01\n",
            "13 50 0.10579395294189453 11.463394069339762 0.01\n",
            "13 60 0.22377195954322815 10.459428389030615 0.01\n",
            "13 70 0.3851780295372009 11.500917209817572 0.01\n",
            "13 80 0.4438871443271637 11.595123434605922 0.01\n",
            "13 90 0.24425126612186432 11.070869280036531 0.01\n",
            "13 100 0.2721223831176758 8.77833077299802 0.01\n",
            "13 110 0.2085217386484146 12.002904641201692 0.01\n",
            "13 120 0.16352108120918274 11.712448670995919 0.01\n",
            "13 130 0.6789201498031616 11.677215489424729 0.01\n",
            "13 140 0.42158448696136475 11.402832830383602 0.01\n",
            "13 150 0.6948537826538086 11.659248819635264 0.01\n",
            "13 160 0.30979055166244507 12.776877781602508 0.01\n",
            "13 170 1.0955066680908203 12.815634307294742 0.01\n",
            "13 180 0.39780014753341675 10.674471309665753 0.01\n",
            "13 190 0.15464748442173004 11.459612166417354 0.01\n",
            "13 200 0.1251811683177948 12.861771278532688 0.01\n",
            "13 210 0.28156232833862305 10.328928137393847 0.01\n",
            "13 220 1.1700689792633057 11.94463263803751 0.01\n",
            "13 230 0.46463876962661743 12.867216362329767 0.01\n",
            "13 240 0.49358370900154114 12.02925644994877 0.01\n",
            "13 250 0.5388749241828918 12.452952982672826 0.01\n",
            "13 260 0.1693100482225418 12.284718876354802 0.01\n",
            "13 270 0.5572727918624878 12.309054011818066 0.01\n",
            "13 280 0.4051641523838043 12.391494390404231 0.01\n",
            "13 290 0.15477848052978516 12.218299612124778 0.01\n",
            "13 300 0.697193443775177 10.008683586017971 0.01\n",
            "13 310 0.3725050985813141 10.729061588509941 0.01\n",
            "13 320 0.6977498531341553 11.99698237981541 0.01\n",
            "13 330 0.31581759452819824 11.923495030815765 0.01\n",
            "13 340 0.3222810626029968 12.023954430394262 0.01\n",
            "13 350 0.2966933250427246 12.703429435050323 0.01\n",
            "13 360 0.25249460339546204 12.34950027014414 0.01\n",
            "13 370 0.3817634880542755 12.448314598404748 0.01\n",
            "13 380 0.23853512108325958 12.324027320100313 0.01\n",
            "13 390 0.4682842493057251 11.946793056892691 0.01\n",
            "13 400 0.47123220562934875 10.869282637877507 0.01\n",
            "13 410 0.1853221356868744 12.308936611663443 0.01\n",
            "13 420 0.17628830671310425 12.636633488116354 0.01\n",
            "13 430 0.20820248126983643 11.663423122297868 0.01\n",
            "13 440 0.5603419542312622 12.299849415476922 0.01\n",
            "13 450 0.6607651114463806 12.618224100144253 0.01\n",
            "13 460 0.16822102665901184 12.39531205093708 0.01\n",
            "13 470 0.41316676139831543 10.671063009153958 0.01\n",
            "13 480 0.3101803958415985 11.538480477956677 0.01\n",
            "13 490 0.25176236033439636 12.642213462800632 0.01\n",
            "13 500 0.27731719613075256 11.742394348157475 0.01\n",
            "13 510 0.4800809919834137 12.225520308443203 0.01\n",
            "13 520 0.30608558654785156 12.175136249174523 0.01\n",
            "13 530 0.15187987685203552 12.248217216346125 0.01\n",
            "13 540 0.2879267930984497 10.878444655968835 0.01\n",
            "13 550 1.2324947118759155 11.89323518486487 0.01\n",
            "13 560 0.2519720196723938 11.726857281853347 0.01\n",
            "13 570 0.16975252330303192 11.353009283538722 0.01\n",
            "13 580 0.10025614500045776 12.239933756233151 0.01\n",
            "13 590 1.591676950454712 10.17088836658618 0.01\n",
            "13 600 0.3019399344921112 8.3366208324886 0.01\n",
            "13 610 0.19320270419120789 12.381599572549911 0.01\n",
            "13 620 0.21772733330726624 10.47913194904223 0.01\n",
            "13 630 0.15725401043891907 8.739476117424292 0.01\n",
            "13 640 0.08635527640581131 11.364144442058647 0.01\n",
            "13 650 0.3073374032974243 11.524641615531035 0.01\n",
            "13 660 0.17216277122497559 10.479989455805898 0.01\n",
            "13 670 1.0108230113983154 9.550976120305341 0.01\n",
            "13 680 0.6371732950210571 11.018497465916129 0.01\n",
            "13 690 0.3010663092136383 8.40437881684982 0.01\n",
            "13 700 0.19432322680950165 11.23664907439655 0.01\n",
            "13 710 0.13128486275672913 12.821549891976948 0.01\n",
            "13 720 0.48252543807029724 9.648099906033305 0.01\n",
            "13 730 0.8659474849700928 10.541624018784546 0.01\n",
            "13 740 0.4508218467235565 11.989900513049935 0.01\n",
            "13 750 0.41555532813072205 11.450508260339586 0.01\n",
            "13 760 0.16181568801403046 9.951176574910452 0.01\n",
            "13 770 0.31660884618759155 12.445387358343465 0.01\n",
            "13 780 0.2996186912059784 11.365568670807184 0.01\n",
            "13 790 0.7361559867858887 11.932611520508223 0.01\n",
            "13 800 0.4627147912979126 11.011742776036625 0.01\n",
            "13 810 0.6036658883094788 12.601354081698565 0.01\n",
            "13 820 0.5170796513557434 12.206956063672967 0.01\n",
            "13 830 0.656872034072876 12.494463691527676 0.01\n",
            "13 840 0.28317293524742126 11.493078004914484 0.01\n",
            "13 850 0.5259737372398376 12.386060342969191 0.01\n",
            "13 860 0.23077620565891266 12.652662967775605 0.01\n",
            "tensor(98.0791, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 14\n",
            "14 10 0.2994469404220581 11.841693064331249 0.01\n",
            "14 20 0.21593086421489716 12.082300022036932 0.01\n",
            "14 30 0.26117590069770813 12.748605627338518 0.01\n",
            "14 40 4.0858378410339355 12.123658715126954 0.01\n",
            "14 50 0.09037994593381882 11.948086279782592 0.01\n",
            "14 60 0.3275809586048126 12.27714060536786 0.01\n",
            "14 70 0.35003554821014404 12.263562695624588 0.01\n",
            "14 80 0.1848549246788025 11.982972498871499 0.01\n",
            "14 90 2.0794358253479004 11.699396941752719 0.01\n",
            "14 100 0.16156767308712006 11.257201980200676 0.01\n",
            "14 110 0.20043163001537323 11.06996348549251 0.01\n",
            "14 120 0.16555434465408325 11.87445528044431 0.01\n",
            "14 130 0.28446507453918457 11.863665420232957 0.01\n",
            "14 140 0.2970769703388214 11.617130563455085 0.01\n",
            "14 150 0.12251933664083481 12.568812934091182 0.01\n",
            "14 160 0.23312276601791382 12.0168579082327 0.01\n",
            "14 170 0.5604137182235718 12.408293191982528 0.01\n",
            "14 180 0.47455281019210815 11.59465866101627 0.01\n",
            "14 190 0.8102463483810425 12.758300354979909 0.01\n",
            "14 200 0.13685768842697144 12.556536994044011 0.01\n",
            "14 210 0.5621045231819153 11.53690151847517 0.01\n",
            "14 220 0.13491864502429962 11.048035981219174 0.01\n",
            "14 230 0.31436705589294434 12.405146254177634 0.01\n",
            "14 240 0.17791317403316498 11.80391000804178 0.01\n",
            "14 250 0.0973072350025177 12.791412015858494 0.01\n",
            "14 260 0.37665969133377075 12.244587913651843 0.01\n",
            "14 270 0.17779943346977234 12.340806963782768 0.01\n",
            "14 280 0.10694387555122375 11.23569337673879 0.01\n",
            "14 290 0.12531480193138123 12.350782137422675 0.01\n",
            "14 300 0.17191746830940247 10.73942522378582 0.01\n",
            "14 310 0.41811618208885193 12.633388704569105 0.01\n",
            "14 320 0.2763204276561737 11.69901350769941 0.01\n",
            "14 330 0.6275042295455933 12.032914549978232 0.01\n",
            "14 340 0.1379328817129135 12.437591184599144 0.01\n",
            "14 350 0.46806979179382324 12.889013173866147 0.01\n",
            "14 360 0.7554141283035278 11.66751927407364 0.01\n",
            "14 370 0.18589214980602264 11.78146323650753 0.01\n",
            "14 380 0.12108088284730911 12.358214936033047 0.01\n",
            "14 390 0.22702357172966003 10.393974707102096 0.01\n",
            "14 400 0.861018180847168 10.399870073034327 0.01\n",
            "14 410 0.13051234185695648 12.306191892556013 0.01\n",
            "14 420 1.6115710735321045 12.003205201584285 0.01\n",
            "14 430 0.36367806792259216 12.206316616586758 0.01\n",
            "14 440 0.10848686099052429 12.176320312137117 0.01\n",
            "14 450 1.6231880187988281 11.388776053263243 0.01\n",
            "14 460 0.834077775478363 12.148414832214716 0.01\n",
            "14 470 0.5606822967529297 11.961426198301737 0.01\n",
            "14 480 0.12677699327468872 11.981611815629671 0.01\n",
            "14 490 0.3280070722103119 10.068774298399651 0.01\n",
            "14 500 0.11533492803573608 12.14101253163302 0.01\n",
            "14 510 0.36065009236335754 12.732312960275877 0.01\n",
            "14 520 0.14309574663639069 10.898124600914997 0.01\n",
            "14 530 0.23854126036167145 10.15825161889942 0.01\n",
            "14 540 0.6220835447311401 10.535619820801946 0.01\n",
            "14 550 0.14233705401420593 12.244203655333425 0.01\n",
            "14 560 0.9477611184120178 12.475983464719754 0.01\n",
            "14 570 0.7027158141136169 12.357304688305604 0.01\n",
            "14 580 0.31631696224212646 9.704920308688502 0.01\n",
            "14 590 0.46901315450668335 12.671900420704397 0.01\n",
            "14 600 0.4657660722732544 12.093831410830317 0.01\n",
            "14 610 0.23425987362861633 9.35446596539277 0.01\n",
            "14 620 0.0870933085680008 12.508436767486385 0.01\n",
            "14 630 0.44704490900039673 9.883188192873146 0.01\n",
            "14 640 0.1929069459438324 10.901573257992998 0.01\n",
            "14 650 0.24893617630004883 8.850591157224821 0.01\n",
            "14 660 0.346498966217041 11.092088204006641 0.01\n",
            "14 670 0.514362096786499 7.962408159642876 0.01\n",
            "14 680 0.9724451303482056 9.144252445049075 0.01\n",
            "14 690 0.1693839281797409 12.617559819263699 0.01\n",
            "14 700 0.13746824860572815 11.630958735828049 0.01\n",
            "14 710 0.3039003610610962 12.559563770949804 0.01\n",
            "14 720 0.49867725372314453 12.128505416081106 0.01\n",
            "14 730 0.6581900715827942 10.913238367634213 0.01\n",
            "14 740 0.3775518536567688 11.714067871875148 0.01\n",
            "14 750 0.2364392876625061 9.5987623666077 0.01\n",
            "14 760 0.5579089522361755 11.590301218219826 0.01\n",
            "14 770 0.4705706834793091 12.154549221776916 0.01\n",
            "14 780 0.35287153720855713 12.097781873534847 0.01\n",
            "14 790 0.8210453987121582 9.263898153097239 0.01\n",
            "14 800 0.2381386160850525 10.053961216910956 0.01\n",
            "14 810 0.14483791589736938 11.46746063643348 0.01\n",
            "14 820 0.516838550567627 10.631268847811388 0.01\n",
            "14 830 0.18806596100330353 12.252331834766174 0.01\n",
            "14 840 0.31375014781951904 12.153924057968915 0.01\n",
            "14 850 0.18687322735786438 12.065382499415689 0.01\n",
            "14 860 0.506037712097168 12.512933419849492 0.01\n",
            "tensor(93.8723, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 15\n",
            "15 10 0.09034774452447891 10.084146269629182 0.01\n",
            "15 20 0.17078188061714172 10.692751622502353 0.01\n",
            "15 30 0.15378127992153168 12.298226503777311 0.01\n",
            "15 40 0.37074971199035645 12.770070148090413 0.01\n",
            "15 50 0.39224088191986084 12.117126623318 0.01\n",
            "15 60 0.35376426577568054 11.517813696252725 0.01\n",
            "15 70 0.4890686869621277 12.27714060536786 0.01\n",
            "15 80 1.5276720523834229 12.159279656932556 0.01\n",
            "15 90 1.367088794708252 10.096514138668965 0.01\n",
            "15 100 0.33261746168136597 10.360021711490356 0.01\n",
            "15 110 0.579043984413147 11.420950233698619 0.01\n",
            "15 120 0.5383634567260742 11.507511989552354 0.01\n",
            "15 130 0.209835022687912 11.798539213797158 0.01\n",
            "15 140 0.6338328123092651 11.602131605771877 0.01\n",
            "15 150 0.2820362150669098 11.618393625581882 0.01\n",
            "15 160 0.31959831714630127 10.538683953478877 0.01\n",
            "15 170 0.6574305891990662 11.293043192624783 0.01\n",
            "15 180 0.5946239829063416 9.183720768560844 0.01\n",
            "15 190 0.8742738962173462 11.935684583835775 0.01\n",
            "15 200 0.3949028253555298 12.71396115460105 0.01\n",
            "15 210 6.648836135864258 9.056458412729944 0.01\n",
            "15 220 0.992347002029419 10.96752928809708 0.01\n",
            "15 230 0.2041054666042328 12.781481907545144 0.01\n",
            "15 240 0.4852840304374695 12.601467661131016 0.01\n",
            "15 250 0.19853758811950684 12.518768957553734 0.01\n",
            "15 260 0.30193138122558594 12.584396958841143 0.01\n",
            "15 270 0.336232990026474 11.430139562135297 0.01\n",
            "15 280 0.3463686406612396 10.791718715225024 0.01\n",
            "15 290 0.09539765119552612 12.477458757312572 0.01\n",
            "15 300 0.3026095926761627 10.419589096990922 0.01\n",
            "15 310 0.213425412774086 11.070204529243584 0.01\n",
            "15 320 0.3577459454536438 11.962526408557407 0.01\n",
            "15 330 0.18823862075805664 10.761261752439646 0.01\n",
            "15 340 0.22230932116508484 10.6436924388695 0.01\n",
            "15 350 0.17345939576625824 12.418534073733033 0.01\n",
            "15 360 0.8588988184928894 12.337249601620139 0.01\n",
            "15 370 0.9029439687728882 10.706726765323594 0.01\n",
            "15 380 0.21420647203922272 10.52757420881062 0.01\n",
            "15 390 0.10853411257266998 9.68399203679835 0.01\n",
            "15 400 0.12549063563346863 12.775544859274353 0.01\n",
            "15 410 0.1485990732908249 12.25669097712557 0.01\n",
            "15 420 0.49982601404190063 12.808149716845588 0.01\n",
            "15 430 0.3283613324165344 11.882739404687873 0.01\n",
            "15 440 0.5597079396247864 12.82629413783903 0.01\n",
            "15 450 0.6081610321998596 12.942097260803392 0.01\n",
            "15 460 0.2898065745830536 11.113057359647874 0.01\n",
            "15 470 0.29631122946739197 9.381813708220795 0.01\n",
            "15 480 0.21840202808380127 11.98665388259132 0.01\n",
            "15 490 0.5293376445770264 11.572760457910377 0.01\n",
            "15 500 0.33444440364837646 11.580564891260607 0.01\n",
            "15 510 0.16856159269809723 12.254515493797221 0.01\n",
            "15 520 0.1081433966755867 10.121823509573588 0.01\n",
            "15 530 0.18655872344970703 11.417880046087435 0.01\n",
            "15 540 0.2821262776851654 12.218103855094586 0.01\n",
            "15 550 0.33893322944641113 12.233445285258659 0.01\n",
            "15 560 0.22345787286758423 10.528869138579271 0.01\n",
            "15 570 0.5766363143920898 11.724472503366629 0.01\n",
            "15 580 0.3536123037338257 12.226161769196182 0.01\n",
            "15 590 0.4618241786956787 11.966220842653033 0.01\n",
            "15 600 0.2395555078983307 11.567924915984175 0.01\n",
            "15 610 0.44177114963531494 10.910108676991587 0.01\n",
            "15 620 0.14556244015693665 12.78282580852599 0.01\n",
            "15 630 0.2798866331577301 10.174940990104774 0.01\n",
            "15 640 0.21881376206874847 12.192584690025857 0.01\n",
            "15 650 0.14073991775512695 12.263275846693848 0.01\n",
            "15 660 0.36850622296333313 10.65218710833383 0.01\n",
            "15 670 0.22839704155921936 11.188935654277877 0.01\n",
            "15 680 0.17754879593849182 11.740455098603297 0.01\n",
            "15 690 1.6368085145950317 9.218068214035327 0.01\n",
            "15 700 0.22189204394817352 12.443836571517577 0.01\n",
            "15 710 3.5907187461853027 12.562432095077 0.01\n",
            "15 720 0.8014811873435974 10.94927711684713 0.01\n",
            "15 730 0.4031932055950165 11.50007368671425 0.01\n",
            "15 740 0.23348288238048553 8.230758513233214 0.01\n",
            "15 750 0.20493023097515106 10.201172782305628 0.01\n",
            "15 760 0.3964068293571472 7.977689196045491 0.01\n",
            "15 770 0.053500108420848846 13.199804881119102 0.01\n",
            "15 780 0.22944442927837372 12.261268279848865 0.01\n",
            "15 790 0.3215511739253998 11.371916601144159 0.01\n",
            "15 800 0.4350433647632599 12.74433493233671 0.01\n",
            "15 810 0.6099813580513 12.196289619075312 0.01\n",
            "15 820 0.33456918597221375 11.895064997678022 0.01\n",
            "15 830 0.33874526619911194 11.233368775426577 0.01\n",
            "15 840 0.21895620226860046 11.655231344019763 0.01\n",
            "15 850 0.7691638469696045 12.61985663028516 0.01\n",
            "15 860 0.2649419903755188 12.480911748733476 0.01\n",
            "tensor(92.8006, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 16\n",
            "16 10 0.15484808385372162 12.242211260617028 0.001\n",
            "16 20 0.1636229157447815 12.656251815206167 0.001\n",
            "16 30 0.14619997143745422 12.774397017696009 0.001\n",
            "16 40 0.11510150134563446 12.446532234671643 0.001\n",
            "16 50 0.655430018901825 12.346801037067891 0.001\n",
            "16 60 0.11576412618160248 12.637204590220735 0.001\n",
            "16 70 0.18221236765384674 11.187577727557949 0.001\n",
            "16 80 0.11204690486192703 12.557476830679459 0.001\n",
            "16 90 0.29179656505584717 8.507301875770754 0.001\n",
            "16 100 0.699564516544342 12.398289077349194 0.001\n",
            "16 110 0.39158573746681213 10.453497892130413 0.001\n",
            "16 120 0.14359577000141144 11.706597113196041 0.001\n",
            "16 130 0.2095385491847992 12.302564241701175 0.001\n",
            "16 140 0.23455631732940674 12.22184211829909 0.001\n",
            "16 150 0.17655861377716064 11.820767984217571 0.001\n",
            "16 160 0.0895293653011322 12.248244041861195 0.001\n",
            "16 170 0.06386774778366089 11.118035743988273 0.001\n",
            "16 180 0.19663171470165253 10.436168631185685 0.001\n",
            "16 190 0.10303764045238495 12.870305931449263 0.001\n",
            "16 200 0.40262895822525024 12.036229466680728 0.001\n",
            "16 210 0.2597225308418274 6.991456339486113 0.001\n",
            "16 220 0.09621240198612213 12.714433276419603 0.001\n",
            "16 230 0.07880526036024094 9.494079645797353 0.001\n",
            "16 240 0.21485459804534912 12.26809130788001 0.001\n",
            "16 250 0.2613273561000824 12.79869154001895 0.001\n",
            "16 260 0.4413214325904846 12.423545859599523 0.001\n",
            "16 270 0.624266505241394 11.984924206706385 0.001\n",
            "16 280 0.37281519174575806 12.290847659772735 0.001\n",
            "16 290 0.09620653092861176 12.84718501944244 0.001\n",
            "16 300 0.15308301150798798 12.117441683260493 0.001\n",
            "16 310 0.10423149168491364 11.993697617161697 0.001\n",
            "16 320 1.5783005952835083 11.941138496775428 0.001\n",
            "16 330 0.5920024514198303 12.027479968198596 0.001\n",
            "16 340 0.6750625371932983 11.910932898656496 0.001\n",
            "16 350 0.09761571884155273 12.918170828424692 0.001\n",
            "16 360 0.09959578514099121 11.752371879290537 0.001\n",
            "16 370 0.4040074944496155 8.635719102106785 0.001\n",
            "16 380 0.1613917052745819 11.56177860412806 0.001\n",
            "16 390 0.14582796394824982 9.064835676641803 0.001\n",
            "16 400 0.36397942900657654 11.02363773390515 0.001\n",
            "16 410 0.20956911146640778 12.790573353683731 0.001\n",
            "16 420 0.2678724527359009 12.53954086303996 0.001\n",
            "16 430 0.13484573364257812 12.131583723625877 0.001\n",
            "16 440 0.4954485297203064 9.789414474866554 0.001\n",
            "16 450 0.10904552042484283 10.89521582990772 0.001\n",
            "16 460 0.1802072674036026 11.847655392107361 0.001\n",
            "16 470 0.19978955388069153 11.527468510634789 0.001\n",
            "16 480 0.35941803455352783 12.401533373643773 0.001\n",
            "16 490 0.24108578264713287 10.095809363340956 0.001\n",
            "16 500 0.22536155581474304 10.538564796232121 0.001\n",
            "16 510 0.3047698140144348 12.10369931095581 0.001\n",
            "16 520 0.18828631937503815 10.186573956294897 0.001\n",
            "16 530 0.3330846130847931 11.103062518364787 0.001\n",
            "16 540 0.5210552215576172 12.336278943227525 0.001\n",
            "16 550 0.17453953623771667 11.2550194781715 0.001\n",
            "16 560 0.3980746865272522 12.092149105515382 0.001\n",
            "16 570 0.5050473809242249 12.259995966245071 0.001\n",
            "16 580 0.27243179082870483 12.294981964856019 0.001\n",
            "16 590 0.4556952118873596 11.642597604474608 0.001\n",
            "16 600 0.2864169180393219 11.119914843639188 0.001\n",
            "16 610 0.16307741403579712 9.518388345471577 0.001\n",
            "16 620 0.07808227837085724 12.280897108093487 0.001\n",
            "16 630 0.374633252620697 11.647673688829151 0.001\n",
            "16 640 0.19092367589473724 12.211096327911426 0.001\n",
            "16 650 0.4300815463066101 12.267499259660065 0.001\n",
            "16 660 0.1085776686668396 9.042157025488239 0.001\n",
            "16 670 0.2914869785308838 9.984138177693591 0.001\n",
            "16 680 0.2270006388425827 12.50180031952691 0.001\n",
            "16 690 0.0890069529414177 11.66081281984808 0.001\n",
            "16 700 0.6744953989982605 10.115135126987715 0.001\n",
            "16 710 0.284929484128952 12.941428390707777 0.001\n",
            "16 720 0.36038151383399963 11.361751002275437 0.001\n",
            "16 730 0.3651023805141449 11.795511477924743 0.001\n",
            "16 740 0.1336340308189392 11.81239535705686 0.001\n",
            "16 750 0.29940879344940186 11.106348020021223 0.001\n",
            "16 760 0.09079653769731522 12.013295583271395 0.001\n",
            "16 770 0.24010208249092102 12.566007578305285 0.001\n",
            "16 780 0.10096028447151184 11.75165569640114 0.001\n",
            "16 790 1.4210306406021118 11.162396574345381 0.001\n",
            "16 800 0.09992876648902893 12.878515792616884 0.001\n",
            "16 810 0.2811425030231476 10.960636423443452 0.001\n",
            "16 820 0.24472439289093018 12.54558887310252 0.001\n",
            "16 830 0.17842748761177063 11.603230908722471 0.001\n",
            "16 840 0.36953839659690857 12.990116427171657 0.001\n",
            "16 850 0.5396412014961243 12.826794252520294 0.001\n",
            "16 860 0.10519471764564514 12.649591157388384 0.001\n",
            "tensor(75.8011, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 17\n",
            "17 10 0.8013418316841125 12.264961276352476 0.001\n",
            "17 20 0.06789189577102661 11.17592208330308 0.001\n",
            "17 30 0.10692054033279419 12.487860648672628 0.001\n",
            "17 40 0.5773767828941345 10.971897341456588 0.001\n",
            "17 50 0.35681578516960144 10.933315260175275 0.001\n",
            "17 60 0.07416699826717377 12.446643040333667 0.001\n",
            "17 70 0.5329755544662476 12.656872434068479 0.001\n",
            "17 80 0.1761208325624466 10.574986448156318 0.001\n",
            "17 90 0.07527244091033936 10.316764942267568 0.001\n",
            "17 100 0.1068209856748581 12.233409604309674 0.001\n",
            "17 110 0.3455207645893097 10.746744694916035 0.001\n",
            "17 120 0.1377919614315033 11.669312752613337 0.001\n",
            "17 130 0.07917658984661102 12.429629805241758 0.001\n",
            "17 140 0.1881449818611145 12.63735689283355 0.001\n",
            "17 150 0.3996899127960205 12.188129349279814 0.001\n",
            "17 160 1.0372183322906494 12.222794851003593 0.001\n",
            "17 170 1.2472277879714966 10.301998108735432 0.001\n",
            "17 180 0.16578100621700287 12.236710973147705 0.001\n",
            "17 190 0.10994628816843033 12.692252582938872 0.001\n",
            "17 200 0.358526349067688 12.148810696203265 0.001\n",
            "17 210 0.4171563386917114 12.318977898524121 0.001\n",
            "17 220 0.1582486927509308 11.117328482776214 0.001\n",
            "17 230 0.24118250608444214 11.918912371511203 0.001\n",
            "17 240 0.21431389451026917 12.265275104305037 0.001\n",
            "17 250 0.16971611976623535 12.353428534822863 0.001\n",
            "17 260 0.34868326783180237 11.502707149933666 0.001\n",
            "17 270 0.12140759080648422 12.36101934179543 0.001\n",
            "17 280 0.19244655966758728 12.404284106721162 0.001\n",
            "17 290 0.07632546871900558 12.030489947352494 0.001\n",
            "17 300 0.12386160343885422 12.75320878521112 0.001\n",
            "17 310 0.37973538041114807 11.972693732203897 0.001\n",
            "17 320 0.4460550546646118 12.316997266758632 0.001\n",
            "17 330 0.1896963119506836 12.234712093810163 0.001\n",
            "17 340 0.2068907618522644 12.601581242610928 0.001\n",
            "17 350 0.14211244881153107 12.244945384903959 0.001\n",
            "17 360 0.1986028105020523 12.107280511909753 0.001\n",
            "17 370 0.3499393165111542 11.725177182289668 0.001\n",
            "17 380 0.6558870077133179 12.78788257586729 0.001\n",
            "17 390 0.22826404869556427 12.600748359302592 0.001\n",
            "17 400 0.4154437780380249 8.949748266565383 0.001\n",
            "17 410 0.06418008357286453 12.272120547143588 0.001\n",
            "17 420 0.09090416878461838 12.040928726399487 0.001\n",
            "17 430 0.386695921421051 12.425984244970271 0.001\n",
            "17 440 0.622296154499054 10.436162139432348 0.001\n",
            "17 450 0.24397364258766174 12.510777981851142 0.001\n",
            "17 460 0.5858027338981628 12.899299570208283 0.001\n",
            "17 470 0.14867046475410461 11.918759959108602 0.001\n",
            "17 480 0.412857323884964 11.982835560786913 0.001\n",
            "17 490 0.33118632435798645 11.693436161574624 0.001\n",
            "17 500 0.17706184089183807 12.283063983565198 0.001\n",
            "17 510 0.1329520046710968 12.432632554003483 0.001\n",
            "17 520 0.3046954870223999 11.380733792482179 0.001\n",
            "17 530 0.05738900229334831 12.39110085651634 0.001\n",
            "17 540 0.3808451294898987 11.626831807087306 0.001\n",
            "17 550 0.22033709287643433 10.871296957475678 0.001\n",
            "17 560 0.15623493492603302 11.594225974270092 0.001\n",
            "17 570 0.1278294175863266 8.23172773286702 0.001\n",
            "17 580 0.6156349182128906 12.440071983467826 0.001\n",
            "17 590 0.5163241028785706 11.177463347426292 0.001\n",
            "17 600 0.09056565165519714 12.80106454165271 0.001\n",
            "17 610 0.1847713738679886 8.954286068063867 0.001\n",
            "17 620 0.1862858384847641 11.207015500044088 0.001\n",
            "17 630 0.11610838770866394 12.751493114723198 0.001\n",
            "17 640 0.13773609697818756 11.613970089410566 0.001\n",
            "17 650 0.13892512023448944 12.579037656401828 0.001\n",
            "17 660 0.25500214099884033 11.705494471759472 0.001\n",
            "17 670 0.24736589193344116 10.16774471938004 0.001\n",
            "17 680 0.1462939828634262 11.174939470134714 0.001\n",
            "17 690 0.49710142612457275 11.614653506746686 0.001\n",
            "17 700 0.1600271463394165 10.295852991570483 0.001\n",
            "17 710 0.3101584315299988 12.609783448528814 0.001\n",
            "17 720 0.17338402569293976 11.9687476814729 0.001\n",
            "17 730 0.11929747462272644 9.960375113690604 0.001\n",
            "17 740 0.175404891371727 10.205423397656013 0.001\n",
            "17 750 0.3005749583244324 11.772344853621194 0.001\n",
            "17 760 0.37738198041915894 11.0893901607964 0.001\n",
            "17 770 0.2826647162437439 11.543601182899424 0.001\n",
            "17 780 0.44375374913215637 12.462554876282304 0.001\n",
            "17 790 0.3923633098602295 8.606874021754688 0.001\n",
            "17 800 0.11251519620418549 12.803057370725279 0.001\n",
            "17 810 0.18868139386177063 11.913724075539012 0.001\n",
            "17 820 0.05858538672327995 12.917385081255212 0.001\n",
            "17 830 0.4321625232696533 12.112149100895495 0.001\n",
            "17 840 0.08244902640581131 12.393187786473819 0.001\n",
            "17 850 0.5917964577674866 11.918700688671377 0.001\n",
            "17 860 0.0675528272986412 12.695739032784354 0.001\n",
            "tensor(71.0156, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 18\n",
            "18 10 0.21502047777175903 9.752896852686474 0.001\n",
            "18 20 0.39952534437179565 12.276368021413305 0.001\n",
            "18 30 0.2061285376548767 9.655084780877228 0.001\n",
            "18 40 0.17782273888587952 12.590497774909382 0.001\n",
            "18 50 0.078566774725914 12.843418263492994 0.001\n",
            "18 60 0.3226759731769562 12.387413834858267 0.001\n",
            "18 70 0.5043836236000061 11.977676987407074 0.001\n",
            "18 80 0.5041818022727966 11.927021247554483 0.001\n",
            "18 90 0.28723859786987305 11.85366560192064 0.001\n",
            "18 100 0.44690340757369995 10.574333258960367 0.001\n",
            "18 110 0.1426037847995758 11.232285794625282 0.001\n",
            "18 120 0.18371030688285828 11.575219675895262 0.001\n",
            "18 130 0.38083648681640625 11.514414623399691 0.001\n",
            "18 140 0.13182514905929565 11.91762545595323 0.001\n",
            "18 150 0.5487152934074402 11.400260387645993 0.001\n",
            "18 160 0.8097188472747803 12.929530074623012 0.001\n",
            "18 170 0.10506532341241837 12.746891195056 0.001\n",
            "18 180 0.2167799025774002 11.838943886936965 0.001\n",
            "18 190 0.3129956126213074 6.010898083323152 0.001\n",
            "18 200 0.08331047743558884 8.565148559534853 0.001\n",
            "18 210 0.4901931881904602 9.980105480689039 0.001\n",
            "18 220 0.3995450437068939 11.010239646983855 0.001\n",
            "18 230 0.2541956603527069 12.350000257641083 0.001\n",
            "18 240 0.2287643551826477 11.96565757135297 0.001\n",
            "18 250 0.6046688556671143 12.681967679049134 0.001\n",
            "18 260 0.17156122624874115 12.770847796168415 0.001\n",
            "18 270 0.1903788447380066 12.439177306226552 0.001\n",
            "18 280 0.10943455994129181 12.83736461334221 0.001\n",
            "18 290 0.18207910656929016 12.382495125151854 0.001\n",
            "18 300 0.26857978105545044 13.079212528815292 0.001\n",
            "18 310 0.23342494666576385 12.267050776323149 0.001\n",
            "18 320 0.733741283416748 12.029929235356079 0.001\n",
            "18 330 0.3576842248439789 11.588027677741355 0.001\n",
            "18 340 0.16271430253982544 11.34789504904822 0.001\n",
            "18 350 0.27980175614356995 12.298785458341397 0.001\n",
            "18 360 0.8721946477890015 11.301456769133146 0.001\n",
            "18 370 0.13653552532196045 12.46883463195227 0.001\n",
            "18 380 0.1571008414030075 12.779943585239534 0.001\n",
            "18 390 0.08883066475391388 10.568418241168741 0.001\n",
            "18 400 0.29172641038894653 12.35643097662936 0.001\n",
            "18 410 0.24421986937522888 12.038526808105756 0.001\n",
            "18 420 0.14445768296718597 12.5826980941324 0.001\n",
            "18 430 0.25066429376602173 10.045581456060653 0.001\n",
            "18 440 0.08200594037771225 10.372556511373991 0.001\n",
            "18 450 0.24516171216964722 11.73705474166849 0.001\n",
            "18 460 0.24535304307937622 11.42468331760306 0.001\n",
            "18 470 0.19217953085899353 11.569241123547313 0.001\n",
            "18 480 0.1614730954170227 10.990563448281735 0.001\n",
            "18 490 0.1351180076599121 10.056480283210623 0.001\n",
            "18 500 0.09071053564548492 12.83152275334608 0.001\n",
            "18 510 0.32505568861961365 11.750569240757848 0.001\n",
            "18 520 0.18342095613479614 9.614995922404766 0.001\n",
            "18 530 0.16415376961231232 12.294792752929657 0.001\n",
            "18 540 0.235606387257576 12.379516543195301 0.001\n",
            "18 550 0.15021735429763794 11.86508335578738 0.001\n",
            "18 560 0.1536637246608734 10.558474896804933 0.001\n",
            "18 570 0.26013320684432983 10.985525903100488 0.001\n",
            "18 580 0.18543429672718048 12.177142222573838 0.001\n",
            "18 590 0.2838941514492035 12.047958544698306 0.001\n",
            "18 600 0.2839832901954651 12.42583699455187 0.001\n",
            "18 610 0.22208866477012634 11.952665858291098 0.001\n",
            "18 620 0.07195024937391281 10.802808938843894 0.001\n",
            "18 630 0.14422520995140076 11.334830932333073 0.001\n",
            "18 640 1.076190710067749 11.873320792799525 0.001\n",
            "18 650 0.2615494132041931 12.566892353971774 0.001\n",
            "18 660 0.19921696186065674 12.292279489880999 0.001\n",
            "18 670 0.2828562557697296 12.110173222981098 0.001\n",
            "18 680 0.22358550131320953 11.334180047452021 0.001\n",
            "18 690 0.5624889731407166 11.287610699847074 0.001\n",
            "18 700 1.0935299396514893 10.865819190744258 0.001\n",
            "18 710 0.40816083550453186 11.930074465119718 0.001\n",
            "18 720 0.36023908853530884 11.562527610287809 0.001\n",
            "18 730 0.7321178913116455 10.32787900464093 0.001\n",
            "18 740 0.20589716732501984 9.859856320758265 0.001\n",
            "18 750 0.1076977550983429 12.258885147774036 0.001\n",
            "18 760 0.37567195296287537 10.667656884450995 0.001\n",
            "18 770 0.14785200357437134 10.271722972559296 0.001\n",
            "18 780 0.5565810203552246 10.462102570562315 0.001\n",
            "18 790 0.5111174583435059 11.629297939650703 0.001\n",
            "18 800 0.5230489373207092 12.099483198543782 0.001\n",
            "18 810 0.1800120621919632 11.038237589133098 0.001\n",
            "18 820 0.1182984858751297 9.350258735276332 0.001\n",
            "18 830 0.20778293907642365 11.351918474557808 0.001\n",
            "18 840 0.2992953658103943 11.04112151131837 0.001\n",
            "18 850 0.15279442071914673 11.96259464531581 0.001\n",
            "18 860 0.10503233224153519 12.575916646365458 0.001\n",
            "tensor(69.0708, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 19\n",
            "19 10 0.48842960596084595 12.258159641166486 0.001\n",
            "19 20 0.08560400456190109 12.26523027075712 0.001\n",
            "19 30 0.294014573097229 10.175459366615073 0.001\n",
            "19 40 0.27011579275131226 12.095470583231078 0.001\n",
            "19 50 0.2349667102098465 12.072953463922289 0.001\n",
            "19 60 0.2983729839324951 10.963766515361623 0.001\n",
            "19 70 0.1958191990852356 11.980362655340322 0.001\n",
            "19 80 0.2353159636259079 10.7388409216453 0.001\n",
            "19 90 0.1900661140680313 9.109981918192036 0.001\n",
            "19 100 1.2117873430252075 10.28252587605968 0.001\n",
            "19 110 0.09118393063545227 12.243077827959862 0.001\n",
            "19 120 0.15924188494682312 11.805097721336576 0.001\n",
            "19 130 1.0037747621536255 10.951085303398012 0.001\n",
            "19 140 0.2854432165622711 12.057119120819733 0.001\n",
            "19 150 0.4167114496231079 12.275146459188608 0.001\n",
            "19 160 0.10206842422485352 12.162294256023255 0.001\n",
            "19 170 0.22421354055404663 9.402893410773423 0.001\n",
            "19 180 0.22187240421772003 9.730197965250817 0.001\n",
            "19 190 0.4363289475440979 9.720709764329273 0.001\n",
            "19 200 0.3552805781364441 11.378680627192777 0.001\n",
            "19 210 0.21930137276649475 11.986354150517291 0.001\n",
            "19 220 0.34556087851524353 11.132938594774082 0.001\n",
            "19 230 0.12439367920160294 11.96978091951448 0.001\n",
            "19 240 0.1122172400355339 12.034718535968116 0.001\n",
            "19 250 0.08274464309215546 11.999711043640279 0.001\n",
            "19 260 0.27842316031455994 12.59937623489495 0.001\n",
            "19 270 0.5054886937141418 11.66468816115793 0.001\n",
            "19 280 0.15257783234119415 12.19455209929379 0.001\n",
            "19 290 0.25325828790664673 11.073485228502784 0.001\n",
            "19 300 0.16507117450237274 9.893334748586222 0.001\n",
            "19 310 0.0763101801276207 11.981372230454733 0.001\n",
            "19 320 0.15415054559707642 12.091730781011824 0.001\n",
            "19 330 0.26767879724502563 11.404514843624138 0.001\n",
            "19 340 0.4320012927055359 11.545046917277505 0.001\n",
            "19 350 0.5420746803283691 12.827451325200796 0.001\n",
            "19 360 0.1283240169286728 12.127339399936822 0.001\n",
            "19 370 0.6552829742431641 11.993080316932922 0.001\n",
            "19 380 0.08108831197023392 12.782913464058662 0.001\n",
            "19 390 0.1798834204673767 11.544331948425842 0.001\n",
            "19 400 0.08769825845956802 12.74885750369687 0.001\n",
            "19 410 0.6921987533569336 12.03672168031246 0.001\n",
            "19 420 0.15063826739788055 12.73193612941072 0.001\n",
            "19 430 0.14790062606334686 12.163414093051784 0.001\n",
            "19 440 0.08772622048854828 12.627654938360346 0.001\n",
            "19 450 0.09289497882127762 12.122432316628624 0.001\n",
            "19 460 0.06484036147594452 11.563531749505298 0.001\n",
            "19 470 0.26179659366607666 12.658753758455346 0.001\n",
            "19 480 0.07400918006896973 12.417532201753989 0.001\n",
            "19 490 0.19925102591514587 9.694164796946351 0.001\n",
            "19 500 0.12112028896808624 11.620204225386257 0.001\n",
            "19 510 0.352444052696228 12.788711136791026 0.001\n",
            "19 520 0.3889313340187073 11.256756348743604 0.001\n",
            "19 530 0.37765195965766907 11.950443728501837 0.001\n",
            "19 540 0.276577889919281 11.532531879544093 0.001\n",
            "19 550 0.2002093344926834 12.05142892032741 0.001\n",
            "19 560 0.18793614208698273 11.838676557859314 0.001\n",
            "19 570 0.3238331973552704 11.539488381562368 0.001\n",
            "19 580 0.1993899643421173 11.809260756973934 0.001\n",
            "19 590 0.18842065334320068 11.223006298101204 0.001\n",
            "19 600 0.15305401384830475 12.565649938397128 0.001\n",
            "19 610 0.8013189435005188 11.621129861008173 0.001\n",
            "19 620 0.14705556631088257 10.892938023311379 0.001\n",
            "19 630 0.11221760511398315 11.11081265438844 0.001\n",
            "19 640 0.49901849031448364 12.033829422646146 0.001\n",
            "19 650 0.17140093445777893 10.567433048171052 0.001\n",
            "19 660 0.2914852201938629 8.410479451814444 0.001\n",
            "19 670 0.6456625461578369 11.438017881174861 0.001\n",
            "19 680 0.40828466415405273 11.109304427573923 0.001\n",
            "19 690 0.155635803937912 11.957351213893926 0.001\n",
            "19 700 1.318690299987793 9.79049988854018 0.001\n",
            "19 710 0.0948273167014122 12.375206533541046 0.001\n",
            "19 720 0.37712958455085754 11.199556749720465 0.001\n",
            "19 730 0.5379745960235596 12.310679782626881 0.001\n",
            "19 740 0.16369247436523438 12.369285168181626 0.001\n",
            "19 750 0.1198439970612526 11.538869333289316 0.001\n",
            "19 760 0.49975547194480896 11.021617253861162 0.001\n",
            "19 770 0.1326988935470581 10.107536002776119 0.001\n",
            "19 780 0.22309689223766327 11.969943179710219 0.001\n",
            "19 790 0.16893276572227478 10.744624874796024 0.001\n",
            "19 800 0.6087124347686768 11.20887986803663 0.001\n",
            "19 810 0.17449834942817688 12.202037598403436 0.001\n",
            "19 820 0.44937869906425476 11.670376115582329 0.001\n",
            "19 830 0.22411741316318512 10.197942202476725 0.001\n",
            "19 840 0.4174198508262634 12.082447944310692 0.001\n",
            "19 850 0.522950291633606 12.53014754867254 0.001\n",
            "19 860 0.09466429054737091 12.72209811442082 0.001\n",
            "tensor(67.6450, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 20\n",
            "20 10 0.21917594969272614 12.530381508122574 0.001\n",
            "20 20 0.3517361581325531 12.823745139659083 0.001\n",
            "20 30 0.2452125996351242 12.27194101468781 0.001\n",
            "20 40 0.1664862334728241 11.905210848738285 0.001\n",
            "20 50 0.21898402273654938 11.940773047808028 0.001\n",
            "20 60 0.17408955097198486 11.788938763234894 0.001\n",
            "20 70 0.15209393203258514 12.323348393622233 0.001\n",
            "20 80 0.16083121299743652 12.179962306997599 0.001\n",
            "20 90 4.2554168701171875 12.314845521401512 0.001\n",
            "20 100 0.17063476145267487 11.342409668203127 0.001\n",
            "20 110 0.24531447887420654 10.802092528385952 0.001\n",
            "20 120 0.5526260137557983 11.462023526325353 0.001\n",
            "20 130 0.26551857590675354 12.589052310264332 0.001\n",
            "20 140 0.21420098841190338 10.968583327340363 0.001\n",
            "20 150 0.27370062470436096 11.068225359546114 0.001\n",
            "20 160 1.6195473670959473 12.392080161552178 0.001\n",
            "20 170 0.29139941930770874 11.297811376049918 0.001\n",
            "20 180 0.16171938180923462 12.11810686361256 0.001\n",
            "20 190 0.25417405366897583 10.490795581879302 0.001\n",
            "20 200 0.27603626251220703 11.48385798448806 0.001\n",
            "20 210 0.12626580893993378 12.561867731083503 0.001\n",
            "20 220 0.5484926104545593 11.216755987719692 0.001\n",
            "20 230 0.2823491096496582 9.741197646163984 0.001\n",
            "20 240 0.41416212916374207 11.500247111265875 0.001\n",
            "20 250 0.17266494035720825 9.441707758229661 0.001\n",
            "20 260 0.20472554862499237 12.778230446939075 0.001\n",
            "20 270 0.41159558296203613 11.635040181531197 0.001\n",
            "20 280 0.2447238266468048 12.759823766567518 0.001\n",
            "20 290 0.7195281386375427 10.30085957070583 0.001\n",
            "20 300 0.08060286194086075 11.40062448822884 0.001\n",
            "20 310 0.4322751760482788 11.95624343473359 0.001\n",
            "20 320 0.12018829584121704 12.283837410564974 0.001\n",
            "20 330 0.3060614764690399 12.2305914669085 0.001\n",
            "20 340 0.7012127637863159 11.644343019591838 0.001\n",
            "20 350 0.13124142587184906 11.995730012676972 0.001\n",
            "20 360 0.09624861925840378 11.141418555200346 0.001\n",
            "20 370 0.23540207743644714 11.783366402000274 0.001\n",
            "20 380 0.08543077111244202 12.159931811905809 0.001\n",
            "20 390 0.4841043949127197 12.546208068296199 0.001\n",
            "20 400 0.32281699776649475 12.169334137019847 0.001\n",
            "20 410 0.16921751201152802 12.103262724061137 0.001\n",
            "20 420 0.5498157143592834 12.391842185344178 0.001\n",
            "20 430 0.4985119700431824 10.342401388750186 0.001\n",
            "20 440 0.4270751476287842 12.310544285323294 0.001\n",
            "20 450 0.326648086309433 12.211745164532896 0.001\n",
            "20 460 0.3487960696220398 12.2486464386882 0.001\n",
            "20 470 0.363776832818985 12.775856173902316 0.001\n",
            "20 480 0.09542269259691238 12.578575536083935 0.001\n",
            "20 490 0.46726879477500916 12.19396712751715 0.001\n",
            "20 500 0.1090272068977356 12.680242370374401 0.001\n",
            "20 510 0.1522146463394165 12.546011045013852 0.001\n",
            "20 520 0.2553313076496124 11.593649108840829 0.001\n",
            "20 530 0.12561118602752686 10.72757976204863 0.001\n",
            "20 540 0.6777843236923218 12.454173211512595 0.001\n",
            "20 550 0.2522367238998413 11.82287549320072 0.001\n",
            "20 560 0.2204480618238449 12.034355969041181 0.001\n",
            "20 570 0.9188153743743896 12.025574714291448 0.001\n",
            "20 580 0.15325132012367249 11.959627065748231 0.001\n",
            "20 590 0.9163435697555542 11.189995364518893 0.001\n",
            "20 600 0.14200475811958313 11.035536641035591 0.001\n",
            "20 610 0.5176866054534912 12.104572579242912 0.001\n",
            "20 620 0.23141369223594666 11.657061543934542 0.001\n",
            "20 630 0.3474667966365814 11.956652436981976 0.001\n",
            "20 640 0.26353931427001953 12.120470595034694 0.001\n",
            "20 650 0.18273138999938965 10.670214667174614 0.001\n",
            "20 660 0.1150849312543869 11.501469114322186 0.001\n",
            "20 670 0.1711753010749817 12.703862296963818 0.001\n",
            "20 680 0.3355594277381897 11.552082164219646 0.001\n",
            "20 690 0.6129687428474426 12.489059837585504 0.001\n",
            "20 700 0.20383404195308685 10.08543746983034 0.001\n",
            "20 710 0.16828088462352753 12.74021221516762 0.001\n",
            "20 720 0.505013644695282 12.564812387755438 0.001\n",
            "20 730 0.14792509377002716 11.003098168119562 0.001\n",
            "20 740 0.20846422016620636 11.892999120991295 0.001\n",
            "20 750 0.11364004015922546 10.343606522608969 0.001\n",
            "20 760 0.15472738444805145 10.61464165210971 0.001\n",
            "20 770 0.10666929930448532 10.99510119072195 0.001\n",
            "20 780 0.1159229502081871 12.853799395357461 0.001\n",
            "20 790 0.1787649393081665 12.713710655220913 0.001\n",
            "20 800 0.40567323565483093 10.408509344707252 0.001\n",
            "20 810 0.27478232979774475 10.083267472700642 0.001\n",
            "20 820 0.2349078357219696 11.547438734388926 0.001\n",
            "20 830 0.19406583905220032 10.172534933722032 0.001\n",
            "20 840 0.1762315332889557 12.631153533131412 0.001\n",
            "20 850 0.9221954345703125 12.270890855060243 0.001\n",
            "20 860 0.23114927113056183 12.605946215221763 0.001\n",
            "tensor(66.5969, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 21\n",
            "21 10 0.13419465720653534 12.479909070486737 0.001\n",
            "21 20 0.19926998019218445 11.64425412006399 0.001\n",
            "21 30 0.39053913950920105 11.207487148998139 0.001\n",
            "21 40 0.12706178426742554 11.760840976578162 0.001\n",
            "21 50 0.32658013701438904 10.710889508864444 0.001\n",
            "21 60 0.1708282232284546 10.976010362863788 0.001\n",
            "21 70 0.14255326986312866 11.989891944428564 0.001\n",
            "21 80 0.644019603729248 10.783464496075402 0.001\n",
            "21 90 0.3285199999809265 11.466567154794271 0.001\n",
            "21 100 0.12683938443660736 11.60480399636719 0.001\n",
            "21 110 0.24608391523361206 12.13207499416074 0.001\n",
            "21 120 0.5622829794883728 12.24096969237089 0.001\n",
            "21 130 0.17239028215408325 12.711360295788948 0.001\n",
            "21 140 0.5179386138916016 11.491668869944505 0.001\n",
            "21 150 0.18128710985183716 12.471142503101579 0.001\n",
            "21 160 0.7387248873710632 11.744712427309658 0.001\n",
            "21 170 1.6916706562042236 11.007935164444374 0.001\n",
            "21 180 0.08965010195970535 12.881591009952258 0.001\n",
            "21 190 0.24244311451911926 11.959771999509554 0.001\n",
            "21 200 0.1446208506822586 12.338556149378668 0.001\n",
            "21 210 0.43104657530784607 10.938561764230315 0.001\n",
            "21 220 0.11861729621887207 11.395142201420486 0.001\n",
            "21 230 0.21080254018306732 11.565030313265803 0.001\n",
            "21 240 0.10503819584846497 7.652610529695961 0.001\n",
            "21 250 0.27012068033218384 9.734432187674791 0.001\n",
            "21 260 0.13818319141864777 7.585665325315368 0.001\n",
            "21 270 0.3965662121772766 11.855072770993074 0.001\n",
            "21 280 0.1379387527704239 12.16434891807786 0.001\n",
            "21 290 0.1858210265636444 12.08366626549152 0.001\n",
            "21 300 0.1412426233291626 10.05030539308127 0.001\n",
            "21 310 0.3336518108844757 12.4337658699485 0.001\n",
            "21 320 0.40250107645988464 10.036813339331694 0.001\n",
            "21 330 0.16307947039604187 12.37648461313748 0.001\n",
            "21 340 0.3739960789680481 11.548177934777073 0.001\n",
            "21 350 0.34092462062835693 10.856320132109303 0.001\n",
            "21 360 0.5220445990562439 10.132301011888405 0.001\n",
            "21 370 0.2988220751285553 10.704014863057747 0.001\n",
            "21 380 0.11589784920215607 12.039926370594166 0.001\n",
            "21 390 0.44094687700271606 9.84589959976056 0.001\n",
            "21 400 0.34288522601127625 11.66065072779956 0.001\n",
            "21 410 0.17783716320991516 11.818786112101261 0.001\n",
            "21 420 0.5381265878677368 12.26337444950021 0.001\n",
            "21 430 0.24735389649868011 12.004888617935045 0.001\n",
            "21 440 0.12737083435058594 10.958052043055702 0.001\n",
            "21 450 0.33836647868156433 12.57489864583794 0.001\n",
            "21 460 0.22572119534015656 10.924081961139496 0.001\n",
            "21 470 0.4406011998653412 12.26787601100636 0.001\n",
            "21 480 0.10729723423719406 11.636250646758302 0.001\n",
            "21 490 3.3498971462249756 10.698984065561602 0.001\n",
            "21 500 0.2805006802082062 7.145991532426377 0.001\n",
            "21 510 0.12431570142507553 12.157182659734426 0.001\n",
            "21 520 0.2183838039636612 11.803295481007822 0.001\n",
            "21 530 0.17728228867053986 10.790677571475982 0.001\n",
            "21 540 0.11322631686925888 12.523936781694998 0.001\n",
            "21 550 0.15593138337135315 8.864629753112382 0.001\n",
            "21 560 0.20154207944869995 9.365463282185027 0.001\n",
            "21 570 0.4580756723880768 12.13060130190615 0.001\n",
            "21 580 0.1520920991897583 10.520259877234908 0.001\n",
            "21 590 0.2709447741508484 11.25403045666211 0.001\n",
            "21 600 0.22099994122982025 12.126655675211909 0.001\n",
            "21 610 0.09392978250980377 12.000165941625742 0.001\n",
            "21 620 0.22640162706375122 10.458613361693056 0.001\n",
            "21 630 0.1628294438123703 11.666456431627601 0.001\n",
            "21 640 0.2929767966270447 11.328631418738018 0.001\n",
            "21 650 0.25240224599838257 12.093674492096719 0.001\n",
            "21 660 0.155191108584404 12.179847356396607 0.001\n",
            "21 670 0.5885741114616394 11.885441601839645 0.001\n",
            "21 680 0.1828986257314682 11.055527330068841 0.001\n",
            "21 690 0.16531231999397278 10.663561559553607 0.001\n",
            "21 700 0.1406325101852417 11.182790309217468 0.001\n",
            "21 710 0.10711634904146194 12.014422563936572 0.001\n",
            "21 720 0.4100017547607422 12.04403190259801 0.001\n",
            "21 730 0.2527344226837158 11.012877621243808 0.001\n",
            "21 740 0.14426852762699127 11.759876465759973 0.001\n",
            "21 750 2.1402175426483154 9.380565253272561 0.001\n",
            "21 760 0.15558849275112152 11.606168754578729 0.001\n",
            "21 770 0.13326218724250793 11.160258098849198 0.001\n",
            "21 780 0.2857522964477539 12.084641099351801 0.001\n",
            "21 790 1.2348613739013672 10.175817324080173 0.001\n",
            "21 800 0.11846674978733063 11.066692172215223 0.001\n",
            "21 810 0.09789518266916275 11.776526148507124 0.001\n",
            "21 820 0.07466261088848114 12.446929297425624 0.001\n",
            "21 830 0.23436537384986877 9.910270587012201 0.001\n",
            "21 840 0.3004593253135681 12.600048065368902 0.001\n",
            "21 850 0.22575224936008453 12.055403700286561 0.001\n",
            "21 860 0.24106429517269135 12.58741829574988 0.001\n",
            "tensor(66.1869, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 22\n",
            "22 10 0.2570291757583618 10.42681590632953 0.001\n",
            "22 20 0.15118420124053955 12.544688334644338 0.001\n",
            "22 30 0.16711825132369995 12.532777755782432 0.001\n",
            "22 40 0.21688269078731537 11.84244534136325 0.001\n",
            "22 50 0.21505984663963318 12.084292926346329 0.001\n",
            "22 60 0.19427263736724854 12.201868984873135 0.001\n",
            "22 70 0.1223372146487236 12.540037611406602 0.001\n",
            "22 80 0.15838247537612915 11.175795524943213 0.001\n",
            "22 90 0.1462520956993103 10.973813496629147 0.001\n",
            "22 100 0.7507848143577576 11.872430161825855 0.001\n",
            "22 110 0.29418709874153137 11.933859231070171 0.001\n",
            "22 120 0.14048488438129425 11.062897535821909 0.001\n",
            "22 130 0.0687054768204689 12.708317678854941 0.001\n",
            "22 140 0.09823410958051682 12.056278677038607 0.001\n",
            "22 150 0.297415167093277 10.954138419822053 0.001\n",
            "22 160 0.20795021951198578 11.407988957301338 0.001\n",
            "22 170 0.30845439434051514 12.853622135802128 0.001\n",
            "22 180 0.10447291284799576 12.911499991149768 0.001\n",
            "22 190 0.12098418176174164 12.339518090543551 0.001\n",
            "22 200 1.054674506187439 12.450568680213669 0.001\n",
            "22 210 0.4782828688621521 10.913820503563192 0.001\n",
            "22 220 0.11311686038970947 10.863849108602125 0.001\n",
            "22 230 0.17134834825992584 11.795171473164697 0.001\n",
            "22 240 0.10574541985988617 12.16163303177917 0.001\n",
            "22 250 0.4941180646419525 11.743980736115583 0.001\n",
            "22 260 0.44439154863357544 13.034137549284285 0.001\n",
            "22 270 0.15687766671180725 12.564332493325495 0.001\n",
            "22 280 0.11096587032079697 12.798496270427211 0.001\n",
            "22 290 0.36321741342544556 11.251856730303636 0.001\n",
            "22 300 0.09179345518350601 10.103263088546491 0.001\n",
            "22 310 0.19283775985240936 12.787590168232732 0.001\n",
            "22 320 0.14667508006095886 12.388740178005081 0.001\n",
            "22 330 0.19319474697113037 11.49618877160861 0.001\n",
            "22 340 0.2114681899547577 11.653353152414756 0.001\n",
            "22 350 0.08532684296369553 12.20873265715712 0.001\n",
            "22 360 0.21573790907859802 10.982433700064348 0.001\n",
            "22 370 0.08970750123262405 12.29268478538818 0.001\n",
            "22 380 0.07113727182149887 11.775385500817675 0.001\n",
            "22 390 0.14150896668434143 10.772994978617387 0.001\n",
            "22 400 0.45217666029930115 12.425413669038107 0.001\n",
            "22 410 0.5762157440185547 11.707691792369618 0.001\n",
            "22 420 0.2256172150373459 12.76299790037428 0.001\n",
            "22 430 0.5525689125061035 11.64589493909533 0.001\n",
            "22 440 0.13913685083389282 12.62121422988414 0.001\n",
            "22 450 0.3846833407878876 11.56200170219804 0.001\n",
            "22 460 0.1763727068901062 12.124508579621665 0.001\n",
            "22 470 0.1392761766910553 12.032854138755088 0.001\n",
            "22 480 0.2599491775035858 10.312478140741355 0.001\n",
            "22 490 0.19470961391925812 11.910476285132152 0.001\n",
            "22 500 0.14270690083503723 9.929814510114939 0.001\n",
            "22 510 0.3943372368812561 12.411625364161884 0.001\n",
            "22 520 0.3760331869125366 11.862457505840295 0.001\n",
            "22 530 0.09554654359817505 11.59249555017523 0.001\n",
            "22 540 0.42391660809516907 11.632870044549222 0.001\n",
            "22 550 0.45865920186042786 8.871933175536899 0.001\n",
            "22 560 0.25580722093582153 9.075937849262498 0.001\n",
            "22 570 0.5367937088012695 9.915688477511942 0.001\n",
            "22 580 0.20188608765602112 12.132539983280639 0.001\n",
            "22 590 0.4912547469139099 11.112416974218624 0.001\n",
            "22 600 1.97618567943573 10.287241948498844 0.001\n",
            "22 610 0.1487296223640442 12.049109345498394 0.001\n",
            "22 620 0.29065167903900146 8.452670791932054 0.001\n",
            "22 630 0.16542117297649384 8.101725886363855 0.001\n",
            "22 640 0.4414013922214508 10.596172113421856 0.001\n",
            "22 650 0.16109275817871094 12.49793541264557 0.001\n",
            "22 660 0.12809009850025177 12.42694145776361 0.001\n",
            "22 670 4.219332695007324 10.014669926650367 0.001\n",
            "22 680 0.18662162125110626 12.175551528619108 0.001\n",
            "22 690 0.25653138756752014 7.504150107906831 0.001\n",
            "22 700 0.21597132086753845 8.640304386943868 0.001\n",
            "22 710 0.21651320159435272 11.646371914119092 0.001\n",
            "22 720 0.4117317497730255 12.118815886228921 0.001\n",
            "22 730 0.07783673703670502 11.432117861662048 0.001\n",
            "22 740 0.13149692118167877 8.839367105880841 0.001\n",
            "22 750 0.12670257687568665 10.843900134246233 0.001\n",
            "22 760 0.06586220115423203 10.74790819840164 0.001\n",
            "22 770 0.30424368381500244 11.79605884383659 0.001\n",
            "22 780 0.08687249571084976 12.580339320608935 0.001\n",
            "22 790 0.14853236079216003 11.645312920807061 0.001\n",
            "22 800 0.253412127494812 12.029549706130567 0.001\n",
            "22 810 0.32741302251815796 10.025052449046298 0.001\n",
            "22 820 0.1647433042526245 11.744753536261468 0.001\n",
            "22 830 0.1031355932354927 11.79850602470643 0.001\n",
            "22 840 0.09486439824104309 12.689900551550119 0.001\n",
            "22 850 0.19692924618721008 11.782158234377446 0.001\n",
            "22 860 0.2058035135269165 12.438614740054314 0.001\n",
            "tensor(63.1291, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 23\n",
            "23 10 0.07816651463508606 9.004391308408104 0.001\n",
            "23 20 0.10676105320453644 12.4679914062111 0.001\n",
            "23 30 0.34780585765838623 12.408430849608234 0.001\n",
            "23 40 0.36600345373153687 10.992968705464701 0.001\n",
            "23 50 0.1257506161928177 11.790637186587928 0.001\n",
            "23 60 0.29699772596359253 12.300354408700535 0.001\n",
            "23 70 0.6947845220565796 11.757099228724037 0.001\n",
            "23 80 0.13785983622074127 11.511807392932033 0.001\n",
            "23 90 0.257033109664917 11.459275596230516 0.001\n",
            "23 100 1.8200204372406006 5.923484688534708 0.001\n",
            "23 110 0.45918941497802734 11.245589328457202 0.001\n",
            "23 120 1.5634244680404663 10.710123703942976 0.001\n",
            "23 130 0.18965847790241241 12.194844606229417 0.001\n",
            "23 140 0.2553611695766449 12.188943999290919 0.001\n",
            "23 150 0.27294230461120605 11.522987297884995 0.001\n",
            "23 160 0.6024752259254456 12.184624055044711 0.001\n",
            "23 170 0.1690598726272583 10.338558314399787 0.001\n",
            "23 180 0.32618895173072815 9.627393619157905 0.001\n",
            "23 190 0.12659762799739838 11.994940984105853 0.001\n",
            "23 200 0.14577282965183258 12.568558705957797 0.001\n",
            "23 210 0.14018595218658447 9.401444409078886 0.001\n",
            "23 220 0.1384870409965515 11.056598352174847 0.001\n",
            "23 230 0.6600489020347595 12.510003698445459 0.001\n",
            "23 240 0.11502286791801453 12.549154773658858 0.001\n",
            "23 250 0.39315885305404663 10.77283587705628 0.001\n",
            "23 260 0.30778348445892334 12.397830980211907 0.001\n",
            "23 270 0.13846412301063538 11.935480795840244 0.001\n",
            "23 280 0.3934994041919708 12.133917613083662 0.001\n",
            "23 290 0.052762389183044434 12.708423568266088 0.001\n",
            "23 300 0.841682493686676 11.274745553700013 0.001\n",
            "23 310 0.2169128954410553 12.22167295699195 0.001\n",
            "23 320 0.6667362451553345 11.66400695228991 0.001\n",
            "23 330 0.08061540871858597 12.200174234017519 0.001\n",
            "23 340 0.1145319864153862 11.780908951746435 0.001\n",
            "23 350 0.21893079578876495 12.559808232755845 0.001\n",
            "23 360 0.14369122684001923 11.555670656950396 0.001\n",
            "23 370 0.20300626754760742 11.943323158230443 0.001\n",
            "23 380 0.3367096185684204 11.151549312251325 0.001\n",
            "23 390 0.8639739751815796 11.930668327850572 0.001\n",
            "23 400 0.23033367097377777 12.305352469693592 0.001\n",
            "23 410 0.26409462094306946 12.057673704591156 0.001\n",
            "23 420 0.2782975137233734 12.377598583496257 0.001\n",
            "23 430 0.3018094003200531 12.419977421205559 0.001\n",
            "23 440 0.18182286620140076 10.85736695576992 0.001\n",
            "23 450 0.26763370633125305 12.454543025040904 0.001\n",
            "23 460 0.10665226727724075 11.92971817585815 0.001\n",
            "23 470 0.4484896659851074 12.309017888455042 0.001\n",
            "23 480 0.13333585858345032 10.150986799712241 0.001\n",
            "23 490 0.26263561844825745 10.675809424905998 0.001\n",
            "23 500 0.11128785461187363 12.310038455075945 0.001\n",
            "23 510 0.38938504457473755 11.682386889044865 0.001\n",
            "23 520 0.15791939198970795 11.15742291255731 0.001\n",
            "23 530 0.4105587601661682 8.15380689961523 0.001\n",
            "23 540 0.36354780197143555 13.048674655315494 0.001\n",
            "23 550 0.23453782498836517 10.924707936964785 0.001\n",
            "23 560 0.19291020929813385 10.194509242509772 0.001\n",
            "23 570 0.1338503509759903 12.465045644645642 0.001\n",
            "23 580 0.24466447532176971 12.086190712548023 0.001\n",
            "23 590 0.36941832304000854 11.344119924134597 0.001\n",
            "23 600 0.5528327226638794 12.788418691263939 0.001\n",
            "23 610 0.26965710520744324 10.76336742726304 0.001\n",
            "23 620 0.2110796868801117 11.637824628141711 0.001\n",
            "23 630 0.16972117125988007 12.51046079392717 0.001\n",
            "23 640 0.10896036028862 11.047555832997507 0.001\n",
            "23 650 0.15190018713474274 9.904151106461908 0.001\n",
            "23 660 0.11660535633563995 12.69857378467426 0.001\n",
            "23 670 0.6723057627677917 12.121311255110689 0.001\n",
            "23 680 0.2678760886192322 12.544791514784448 0.001\n",
            "23 690 0.7844637632369995 9.876177626451357 0.001\n",
            "23 700 0.14562734961509705 10.606273785892201 0.001\n",
            "23 710 0.3005546033382416 12.498168170718444 0.001\n",
            "23 720 0.1371929943561554 11.988743859929942 0.001\n",
            "23 730 0.1126171424984932 11.740972716953614 0.001\n",
            "23 740 1.6991004943847656 8.677966101694915 0.001\n",
            "23 750 0.10990732908248901 9.524808067113918 0.001\n",
            "23 760 0.3063088059425354 10.808132826808617 0.001\n",
            "23 770 0.08698823302984238 12.684077027350893 0.001\n",
            "23 780 0.16414396464824677 12.441114398641483 0.001\n",
            "23 790 0.11349885165691376 12.618727102197813 0.001\n",
            "23 800 0.486441045999527 12.509220183896664 0.001\n",
            "23 810 0.14597231149673462 12.705873086797483 0.001\n",
            "23 820 0.23304836452007294 12.11710912046824 0.001\n",
            "23 830 0.2565583884716034 12.847076805034316 0.001\n",
            "23 840 0.19569063186645508 12.376283754575486 0.001\n",
            "23 850 0.5679907202720642 12.49690207149295 0.001\n",
            "23 860 0.07647789269685745 12.37417513322147 0.001\n",
            "tensor(63.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 24\n",
            "24 10 0.13906759023666382 10.380623745131341 0.001\n",
            "24 20 0.08032458275556564 12.396585127244945 0.001\n",
            "24 30 0.22459112107753754 11.568770445230228 0.001\n",
            "24 40 2.1876628398895264 11.673770432488247 0.001\n",
            "24 50 0.26885533332824707 11.36912695282588 0.001\n",
            "24 60 0.4427783191204071 10.657424464562947 0.001\n",
            "24 70 0.10613062232732773 11.98891520186595 0.001\n",
            "24 80 0.32911038398742676 11.072169792642846 0.001\n",
            "24 90 0.2212297022342682 10.894656900975292 0.001\n",
            "24 100 0.13487125933170319 12.367388610531421 0.001\n",
            "24 110 0.10001381486654282 10.406708044020608 0.001\n",
            "24 120 0.19671876728534698 10.989569966639351 0.001\n",
            "24 130 0.10168807208538055 12.512933419849492 0.001\n",
            "24 140 0.1844862848520279 12.383884401835894 0.001\n",
            "24 150 0.38941019773483276 13.000806679710958 0.001\n",
            "24 160 0.10052414238452911 12.847627733732763 0.001\n",
            "24 170 0.15018510818481445 12.590261565188678 0.001\n",
            "24 180 0.25597238540649414 11.895225238511191 0.001\n",
            "24 190 0.16959765553474426 12.427843581709222 0.001\n",
            "24 200 0.13990800082683563 12.69622901804329 0.001\n",
            "24 210 0.5321606993675232 11.889021168520472 0.001\n",
            "24 220 0.223739892244339 12.35785992346873 0.001\n",
            "24 230 0.22176013886928558 11.84397526612774 0.001\n",
            "24 240 0.27600154280662537 11.206207051330006 0.001\n",
            "24 250 0.25942105054855347 12.782533632099083 0.001\n",
            "24 260 0.1681060492992401 11.189547575140143 0.001\n",
            "24 270 0.19750118255615234 12.160116895654973 0.001\n",
            "24 280 0.0683562383055687 12.613528186290173 0.001\n",
            "24 290 0.10352252423763275 12.564548911765455 0.001\n",
            "24 300 0.11831291019916534 12.665491001328663 0.001\n",
            "24 310 0.09968787431716919 10.059796767978874 0.001\n",
            "24 320 0.19719381630420685 12.153950472039826 0.001\n",
            "24 330 1.1056429147720337 12.462008707026605 0.001\n",
            "24 340 0.25833362340927124 10.522800256904302 0.001\n",
            "24 350 0.31263136863708496 12.46592552193936 0.001\n",
            "24 360 0.31045129895210266 11.812062694582869 0.001\n",
            "24 370 0.12468065321445465 11.686862706034292 0.001\n",
            "24 380 0.2866777777671814 12.631343729695585 0.001\n",
            "24 390 0.2568185031414032 8.721748257310354 0.001\n",
            "24 400 0.19245514273643494 12.589836410025514 0.001\n",
            "24 410 0.33888357877731323 12.593229002860593 0.001\n",
            "24 420 0.2219274640083313 12.641337099740726 0.001\n",
            "24 430 0.3020063042640686 11.1411374080272 0.001\n",
            "24 440 0.15562953054904938 12.392995539849576 0.001\n",
            "24 450 0.5834311246871948 12.365036957548751 0.001\n",
            "24 460 0.16301028430461884 10.973813496629147 0.001\n",
            "24 470 0.12221674621105194 12.229619689018705 0.001\n",
            "24 480 0.13874948024749756 12.375179149009933 0.001\n",
            "24 490 0.20396511256694794 11.250717369014353 0.001\n",
            "24 500 0.09692893922328949 13.16664534645089 0.001\n",
            "24 510 0.4298095405101776 10.902990176582366 0.001\n",
            "24 520 0.19531358778476715 11.082065146670171 0.001\n",
            "24 530 0.37645334005355835 10.429804100998146 0.001\n",
            "24 540 0.22837844491004944 12.23727327564765 0.001\n",
            "24 550 0.1334080547094345 10.901573257992998 0.001\n",
            "24 560 0.18855901062488556 9.782781546542397 0.001\n",
            "24 570 0.0953124538064003 11.672909485847553 0.001\n",
            "24 580 0.2264683097600937 12.544622675438891 0.001\n",
            "24 590 0.1761658489704132 12.700505984914322 0.001\n",
            "24 600 0.20332783460617065 10.085552663376395 0.001\n",
            "24 610 0.14046448469161987 12.235640063711733 0.001\n",
            "24 620 0.2415238916873932 11.89099295635891 0.001\n",
            "24 630 0.24828873574733734 12.151784899445039 0.001\n",
            "24 640 3.3007309436798096 11.90149488569335 0.001\n",
            "24 650 0.2713165581226349 11.333628768144921 0.001\n",
            "24 660 0.1302061229944229 12.13698110859843 0.001\n",
            "24 670 0.2631112039089203 11.573534839154288 0.001\n",
            "24 680 0.5397463440895081 10.2090500782844 0.001\n",
            "24 690 0.21698243916034698 10.417583427456936 0.001\n",
            "24 700 0.19468940794467926 10.590433194082008 0.001\n",
            "24 710 0.41752681136131287 12.90495515591588 0.001\n",
            "24 720 0.07687300443649292 12.421467086904727 0.001\n",
            "24 730 0.4149491786956787 11.1986223008377 0.001\n",
            "24 740 0.12854748964309692 11.842428623057463 0.001\n",
            "24 750 0.12830425798892975 7.394762951467231 0.001\n",
            "24 760 0.44709205627441406 11.722612885554897 0.001\n",
            "24 770 0.11797179281711578 12.326653206490274 0.001\n",
            "24 780 0.5739705562591553 12.253369870697462 0.001\n",
            "24 790 0.21402578055858612 12.356121566703933 0.001\n",
            "24 800 0.10396762192249298 12.41936142933706 0.001\n",
            "24 810 0.16517320275306702 8.637217345463139 0.001\n",
            "24 820 0.3277362883090973 12.409394538516558 0.001\n",
            "24 830 0.27235010266304016 11.635072457335612 0.001\n",
            "24 840 0.12178526073694229 12.391640827999415 0.001\n",
            "24 850 0.13661551475524902 12.153994495746483 0.001\n",
            "24 860 0.12368354946374893 12.426278756277126 0.001\n",
            "tensor(61.9036, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 25\n",
            "25 10 0.35972273349761963 12.852794989182861 0.001\n",
            "25 20 0.49463605880737305 12.727811773131211 0.001\n",
            "25 30 0.18281802535057068 12.760706929650825 0.001\n",
            "25 40 0.14345687627792358 12.623854315508774 0.001\n",
            "25 50 0.1262996792793274 12.123211926942275 0.001\n",
            "25 60 0.23558403551578522 11.703844981335617 0.001\n",
            "25 70 0.12853436172008514 12.022963514498388 0.001\n",
            "25 80 0.17959117889404297 11.721679203073014 0.001\n",
            "25 90 0.25296762585639954 9.544705706325868 0.001\n",
            "25 100 0.1938200294971466 8.551008758338362 0.001\n",
            "25 110 0.3466533124446869 12.250918241124642 0.001\n",
            "25 120 0.6999446153640747 8.193528285060983 0.001\n",
            "25 130 0.2888690233230591 8.392699440977477 0.001\n",
            "25 140 0.1671762764453888 10.489083728406438 0.001\n",
            "25 150 0.14277517795562744 12.50067319621043 0.001\n",
            "25 160 0.2265777438879013 12.742853931116407 0.001\n",
            "25 170 0.16607511043548584 12.235488366699656 0.001\n",
            "25 180 0.7757983803749084 11.55764488031237 0.001\n",
            "25 190 0.5082610249519348 7.451466823478413 0.001\n",
            "25 200 0.30203860998153687 12.36152937050365 0.001\n",
            "25 210 0.19127503037452698 12.284260138575975 0.001\n",
            "25 220 0.13547813892364502 10.9635444133171 0.001\n",
            "25 230 0.11595740914344788 11.824767059951228 0.001\n",
            "25 240 0.24500469863414764 12.180077259768366 0.001\n",
            "25 250 0.2567169964313507 12.04442099293224 0.001\n",
            "25 260 0.19902917742729187 12.370653239060884 0.001\n",
            "25 270 0.14553366601467133 12.084519236517727 0.001\n",
            "25 280 0.36030375957489014 10.525559772891246 0.001\n",
            "25 290 0.19579027593135834 12.062727966230261 0.001\n",
            "25 300 0.10745612531900406 11.809842652432437 0.001\n",
            "25 310 0.16409480571746826 11.534450623603073 0.001\n",
            "25 320 0.6435841917991638 12.706306115261837 0.001\n",
            "25 330 0.17332050204277039 12.520095550598759 0.001\n",
            "25 340 0.3532187044620514 12.594439060349986 0.001\n",
            "25 350 0.056463249027729034 12.142225117154283 0.001\n",
            "25 360 0.14087089896202087 11.653539325413446 0.001\n",
            "25 370 0.16888636350631714 11.430622390977167 0.001\n",
            "25 380 0.39295095205307007 11.691146424201206 0.001\n",
            "25 390 0.16343311965465546 12.354274529844794 0.001\n",
            "25 400 0.18665660917758942 12.506581208240805 0.001\n",
            "25 410 0.168207585811615 11.808629049684006 0.001\n",
            "25 420 0.6540619730949402 11.810507746036171 0.001\n",
            "25 430 0.580495297908783 12.613679918741791 0.001\n",
            "25 440 0.13814425468444824 12.295351395402182 0.001\n",
            "25 450 0.6346145868301392 12.616620455702524 0.001\n",
            "25 460 0.39669322967529297 10.916668184929636 0.001\n",
            "25 470 0.14937762916088104 12.023894109105921 0.001\n",
            "25 480 1.1158021688461304 12.119017228763099 0.001\n",
            "25 490 0.1720735728740692 11.868566707036866 0.001\n",
            "25 500 0.27808448672294617 11.511586227717473 0.001\n",
            "25 510 0.06557945907115936 12.094432970368047 0.001\n",
            "25 520 0.7611458897590637 11.152898506407 0.001\n",
            "25 530 0.37422358989715576 12.736710969129987 0.001\n",
            "25 540 0.26862993836402893 11.420973557896858 0.001\n",
            "25 550 0.16207221150398254 10.076018699581518 0.001\n",
            "25 560 0.11571014672517776 11.529662883512527 0.001\n",
            "25 570 0.12835724651813507 12.205277657943805 0.001\n",
            "25 580 0.18090438842773438 11.316603834928241 0.001\n",
            "25 590 0.5178568363189697 10.454579217483854 0.001\n",
            "25 600 0.3348361849784851 11.598217532391219 0.001\n",
            "25 610 0.24418573081493378 11.095924910847504 0.001\n",
            "25 620 0.08048998564481735 10.287822298967859 0.001\n",
            "25 630 0.7798701524734497 12.496259810409667 0.001\n",
            "25 640 0.28390586376190186 12.30980361944802 0.001\n",
            "25 650 0.32412225008010864 10.042815352134513 0.001\n",
            "25 660 0.16468530893325806 10.909278652174903 0.001\n",
            "25 670 0.12687338888645172 9.802357980986814 0.001\n",
            "25 680 0.8074488639831543 11.689957092549934 0.001\n",
            "25 690 0.2874661087989807 12.598704476060743 0.001\n",
            "25 700 0.2767203152179718 11.488615618070934 0.001\n",
            "25 710 0.12796694040298462 11.986379841250562 0.001\n",
            "25 720 0.15031209588050842 11.56079070187298 0.001\n",
            "25 730 0.10659538954496384 12.452074935632576 0.001\n",
            "25 740 0.08423154056072235 10.625007362110054 0.001\n",
            "25 750 0.09347724914550781 12.195385337812986 0.001\n",
            "25 760 0.18239888548851013 11.654275981261149 0.001\n",
            "25 770 0.1876487135887146 11.618763746765874 0.001\n",
            "25 780 0.15724682807922363 12.618319003122762 0.001\n",
            "25 790 0.12067960947751999 12.040738611611696 0.001\n",
            "25 800 0.16978898644447327 11.914248624449993 0.001\n",
            "25 810 0.07349371165037155 12.865913857692263 0.001\n",
            "25 820 0.34545886516571045 11.634846530465055 0.001\n",
            "25 830 0.2505611181259155 12.413214053971666 0.001\n",
            "25 840 0.4924977719783783 12.857887569080463 0.001\n",
            "25 850 0.3857196867465973 12.81804297583803 0.001\n",
            "25 860 0.2587070167064667 12.198568647966361 0.001\n",
            "tensor(62.3389, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 26\n",
            "26 10 0.1829306036233902 12.733607882485272 0.001\n",
            "26 20 0.4292919635772705 12.385310562628543 0.001\n",
            "26 30 0.10095145553350449 12.505611687755064 0.001\n",
            "26 40 0.15361613035202026 12.217854718855321 0.001\n",
            "26 50 0.5959358215332031 11.954360681155363 0.001\n",
            "26 60 0.2634737193584442 12.154276255021353 0.001\n",
            "26 70 0.0948718786239624 12.677243367900548 0.001\n",
            "26 80 1.0224354267120361 11.559587919810827 0.001\n",
            "26 90 0.1143844723701477 12.559159489615265 0.001\n",
            "26 100 0.325550377368927 12.586662825522005 0.001\n",
            "26 110 0.3357561230659485 11.611695291222995 0.001\n",
            "26 120 0.44400554895401 11.981851410386518 0.001\n",
            "26 130 0.42887362837791443 11.879357077108263 0.001\n",
            "26 140 0.3306199908256531 12.978971814402396 0.001\n",
            "26 150 0.25350886583328247 12.40741225549903 0.001\n",
            "26 160 0.3341797888278961 12.43581189010492 0.001\n",
            "26 170 0.6729361414909363 12.648542122940407 0.001\n",
            "26 180 0.13078150153160095 11.983006733881682 0.001\n",
            "26 190 0.30711668729782104 11.035449535684357 0.001\n",
            "26 200 0.13610561192035675 12.432614127808918 0.001\n",
            "26 210 0.19965089857578278 11.211778731776166 0.001\n",
            "26 220 0.32033517956733704 9.542110839941783 0.001\n",
            "26 230 0.19094780087471008 9.706088138731 0.001\n",
            "26 240 0.17519864439964294 10.371479262872226 0.001\n",
            "26 250 0.15813559293746948 11.092601567370968 0.001\n",
            "26 260 0.11345794051885605 6.070600716507646 0.001\n",
            "26 270 0.1969713270664215 11.347150567451676 0.001\n",
            "26 280 0.29325273633003235 10.785731413866015 0.001\n",
            "26 290 1.0917466878890991 10.50287969390106 0.001\n",
            "26 300 0.15948212146759033 11.11277028119047 0.001\n",
            "26 310 0.19943958520889282 10.461769854003538 0.001\n",
            "26 320 0.10476484149694443 12.836863674064007 0.001\n",
            "26 330 0.22977811098098755 12.459602429654318 0.001\n",
            "26 340 0.30582377314567566 11.525409570287811 0.001\n",
            "26 350 0.10472683608531952 9.813653536923418 0.001\n",
            "26 360 0.24698136746883392 12.258929934946744 0.001\n",
            "26 370 0.10765335708856583 11.194430413868984 0.001\n",
            "26 380 0.21388791501522064 12.59427833619842 0.001\n",
            "26 390 0.47911548614501953 12.208084140609822 0.001\n",
            "26 400 0.21114148199558258 12.007741196679072 0.001\n",
            "26 410 0.060921572148799896 12.813236329643507 0.001\n",
            "26 420 0.3630998730659485 11.91011271051139 0.001\n",
            "26 430 0.1706101894378662 12.336487576536783 0.001\n",
            "26 440 0.27790674567222595 12.279656668820964 0.001\n",
            "26 450 0.29606348276138306 12.556988098071532 0.001\n",
            "26 460 0.23600436747074127 10.588134020813742 0.001\n",
            "26 470 0.1886136680841446 5.857688322062919 0.001\n",
            "26 480 0.16970908641815186 10.629490643799121 0.001\n",
            "26 490 0.34558770060539246 12.37418425990394 0.001\n",
            "26 500 0.40557605028152466 11.641918973231634 0.001\n",
            "26 510 0.48063045740127563 12.500039860733303 0.001\n",
            "26 520 0.2029515504837036 11.48135100030248 0.001\n",
            "26 530 0.3164810538291931 11.631257084423801 0.001\n",
            "26 540 0.18514002859592438 11.722146025721734 0.001\n",
            "26 550 0.2087828516960144 8.95013021986405 0.001\n",
            "26 560 0.1679501235485077 12.202445839933784 0.001\n",
            "26 570 0.8742163181304932 12.716273922764998 0.001\n",
            "26 580 0.44329410791397095 10.965056317979272 0.001\n",
            "26 590 0.08569321036338806 12.927975835006107 0.001\n",
            "26 600 0.4277779459953308 11.588860138343389 0.001\n",
            "26 610 0.7412292957305908 11.885610003705128 0.001\n",
            "26 620 0.28319886326789856 11.847262178647945 0.001\n",
            "26 630 0.1615612506866455 12.174199765473523 0.001\n",
            "26 640 0.13710768520832062 12.256323866101916 0.001\n",
            "26 650 0.39305034279823303 11.720565530789404 0.001\n",
            "26 660 0.11255383491516113 11.879003810691744 0.001\n",
            "26 670 0.1676129698753357 10.556309133477672 0.001\n",
            "26 680 0.16681335866451263 11.125954368059631 0.001\n",
            "26 690 0.2863897681236267 10.777244036522806 0.001\n",
            "26 700 0.15502195060253143 12.18527008143261 0.001\n",
            "26 710 0.36024782061576843 11.2631496267036 0.001\n",
            "26 720 0.2868085205554962 11.952631796426724 0.001\n",
            "26 730 0.1464492827653885 12.376977657249848 0.001\n",
            "26 740 0.08060123771429062 12.630497398950546 0.001\n",
            "26 750 0.146044060587883 10.681070041375323 0.001\n",
            "26 760 0.12678582966327667 9.368533145708215 0.001\n",
            "26 770 0.19348293542861938 12.286554170139159 0.001\n",
            "26 780 0.14806416630744934 11.866778611073585 0.001\n",
            "26 790 0.2458413541316986 12.255518089750408 0.001\n",
            "26 800 0.20041830837726593 10.94957724877221 0.001\n",
            "26 810 0.15011624991893768 12.121153622463835 0.001\n",
            "26 820 0.23868219554424286 11.796813628635435 0.001\n",
            "26 830 0.8951926231384277 12.25781930963436 0.001\n",
            "26 840 0.3512675166130066 12.020430958820684 0.001\n",
            "26 850 0.13799948990345 12.648637482518536 0.001\n",
            "26 860 0.1554088592529297 11.592079043736613 0.001\n",
            "tensor(61.2400, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 27\n",
            "27 10 0.15595686435699463 12.432899739962547 0.001\n",
            "27 20 0.19125933945178986 11.387291899699184 0.001\n",
            "27 30 0.22871866822242737 11.74197521878132 0.001\n",
            "27 40 0.11623857170343399 11.729103630333503 0.001\n",
            "27 50 0.12912172079086304 12.214412340496477 0.001\n",
            "27 60 0.3606400191783905 11.807490484877148 0.001\n",
            "27 70 0.33087676763534546 11.504245208626187 0.001\n",
            "27 80 0.14466264843940735 12.377443346094003 0.001\n",
            "27 90 0.24052347242832184 11.07190674297299 0.001\n",
            "27 100 0.177301287651062 11.632265132080704 0.001\n",
            "27 110 0.25879862904548645 12.79317746654578 0.001\n",
            "27 120 0.05022305250167847 12.982065063474106 0.001\n",
            "27 130 0.11253531277179718 12.61997054350099 0.001\n",
            "27 140 0.08837947994470596 12.588863385165338 0.001\n",
            "27 150 0.09235354512929916 12.614011846206823 0.001\n",
            "27 160 0.5833438038825989 10.941800570528933 0.001\n",
            "27 170 0.21326515078544617 10.73952146788427 0.001\n",
            "27 180 0.19329839944839478 11.809044638979552 0.001\n",
            "27 190 0.09834121912717819 12.111379655858462 0.001\n",
            "27 200 1.1817048788070679 10.553765828557768 0.001\n",
            "27 210 0.23667100071907043 12.385118560312174 0.001\n",
            "27 220 0.18187086284160614 9.50295274810051 0.001\n",
            "27 230 0.07102642953395844 12.815693044537722 0.001\n",
            "27 240 0.09750399738550186 11.072177099756543 0.001\n",
            "27 250 0.15269380807876587 10.950406270049976 0.001\n",
            "27 260 0.3700896203517914 12.630687575755294 0.001\n",
            "27 270 0.1372445523738861 12.235577599780628 0.001\n",
            "27 280 0.16879962384700775 12.565000591656593 0.001\n",
            "27 290 0.20690199732780457 11.489189946180854 0.001\n",
            "27 300 0.18564964830875397 9.361079012694233 0.001\n",
            "27 310 0.09602028131484985 12.852263308283348 0.001\n",
            "27 320 0.1203586682677269 12.319746808487203 0.001\n",
            "27 330 0.3638421595096588 12.199650819759151 0.001\n",
            "27 340 0.228838250041008 11.51515750517513 0.001\n",
            "27 350 0.11387913674116135 12.469103376209492 0.001\n",
            "27 360 0.14494656026363373 12.871787081578322 0.001\n",
            "27 370 0.12393643707036972 12.089569898749264 0.001\n",
            "27 380 0.5126845240592957 12.323610902990184 0.001\n",
            "27 390 0.0854826420545578 12.317955849173245 0.001\n",
            "27 400 0.09195300936698914 12.365711371192841 0.001\n",
            "27 410 0.06284549832344055 12.544613295608507 0.001\n",
            "27 420 0.22777599096298218 11.62987029641681 0.001\n",
            "27 430 0.17907355725765228 12.215105997097885 0.001\n",
            "27 440 0.2068246603012085 12.347337154430619 0.001\n",
            "27 450 0.23790179193019867 11.685812615667299 0.001\n",
            "27 460 0.29822400212287903 12.147623181626965 0.001\n",
            "27 470 0.08073757588863373 12.550985430810377 0.001\n",
            "27 480 0.12696918845176697 11.574692612630512 0.001\n",
            "27 490 0.16775353252887726 12.416907262579755 0.001\n",
            "27 500 0.3242524564266205 12.290694590757447 0.001\n",
            "27 510 0.1527465581893921 11.573646614077628 0.001\n",
            "27 520 0.07427173107862473 11.148688914583417 0.001\n",
            "27 530 0.2996462285518646 12.294891863212387 0.001\n",
            "27 540 0.264282763004303 12.650869307200956 0.001\n",
            "27 550 0.11833374202251434 10.555970398110432 0.001\n",
            "27 560 0.08318670839071274 4.901970901770466 0.001\n",
            "27 570 0.13444483280181885 10.109515428005354 0.001\n",
            "27 580 0.20178957283496857 7.92036773977605 0.001\n",
            "27 590 0.4434337317943573 11.919462782086296 0.001\n",
            "27 600 0.282581090927124 12.269123043598448 0.001\n",
            "27 610 0.3390728831291199 10.662761846654464 0.001\n",
            "27 620 0.12199469655752182 12.093029424527119 0.001\n",
            "27 630 0.5100052952766418 11.795179765730513 0.001\n",
            "27 640 0.7783183455467224 10.048048181139235 0.001\n",
            "27 650 0.15374045073986053 8.753566052650115 0.001\n",
            "27 660 0.5423325300216675 10.954467428557251 0.001\n",
            "27 670 0.2165871560573578 11.058179764127718 0.001\n",
            "27 680 0.29025232791900635 12.989563267657278 0.001\n",
            "27 690 0.29299798607826233 12.316264865658493 0.001\n",
            "27 700 0.6407478451728821 11.333460332386245 0.001\n",
            "27 710 0.4653736650943756 12.76569764168243 0.001\n",
            "27 720 0.23683276772499084 12.093099158244062 0.001\n",
            "27 730 0.14022879302501678 10.841216382129057 0.001\n",
            "27 740 0.08292320370674133 12.803604530389185 0.001\n",
            "27 750 0.1710413247346878 12.420961296140275 0.001\n",
            "27 760 0.49401527643203735 10.724507648712175 0.001\n",
            "27 770 0.3820986747741699 12.847096480245652 0.001\n",
            "27 780 0.10938207060098648 12.631029908436188 0.001\n",
            "27 790 0.21329328417778015 12.765406247717367 0.001\n",
            "27 800 0.2228868305683136 12.407760944566267 0.001\n",
            "27 810 0.0849740281701088 12.587276638174288 0.001\n",
            "27 820 0.04432721436023712 12.383445649205424 0.001\n",
            "27 830 0.3556564748287201 12.311131461848237 0.001\n",
            "27 840 0.4329324960708618 12.946441607106754 0.001\n",
            "27 850 0.15147779881954193 12.216013205386131 0.001\n",
            "27 860 0.08489415794610977 12.664592287961206 0.001\n",
            "tensor(59.7741, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 28\n",
            "28 10 0.5982879996299744 13.231430730089228 0.001\n",
            "28 20 0.17200475931167603 12.526910446847369 0.001\n",
            "28 30 0.35014092922210693 11.815806228061774 0.001\n",
            "28 40 0.27444273233413696 12.56074855916735 0.001\n",
            "28 50 0.42632395029067993 9.894390813084701 0.001\n",
            "28 60 0.2692159414291382 12.30634534782564 0.001\n",
            "28 70 0.310618132352829 11.986311332873234 0.001\n",
            "28 80 0.11462654173374176 12.038647745105708 0.001\n",
            "28 90 0.26927080750465393 11.976685134381716 0.001\n",
            "28 100 1.3087077140808105 11.558775586129924 0.001\n",
            "28 110 0.19252069294452667 8.33481096264163 0.001\n",
            "28 120 0.21947786211967468 10.137309010227813 0.001\n",
            "28 130 0.25767815113067627 9.897285935675844 0.001\n",
            "28 140 0.12483430653810501 12.829216928506158 0.001\n",
            "28 150 0.1359819769859314 12.468436170410419 0.001\n",
            "28 160 0.44208472967147827 12.655831738519414 0.001\n",
            "28 170 0.23814307153224945 7.829733376206448 0.001\n",
            "28 180 0.3393459618091583 12.348182315322457 0.001\n",
            "28 190 0.4798877239227295 12.574201222103389 0.001\n",
            "28 200 0.3693256974220276 12.23723757236522 0.001\n",
            "28 210 0.19198116660118103 11.969482030669218 0.001\n",
            "28 220 0.3030332624912262 9.900422283541328 0.001\n",
            "28 230 0.08665383607149124 12.277167557726166 0.001\n",
            "28 240 0.314899742603302 13.070205192646128 0.001\n",
            "28 250 0.14756233990192413 12.443836571517577 0.001\n",
            "28 260 0.29931631684303284 12.99994033602441 0.001\n",
            "28 270 0.12722674012184143 12.273009310161441 0.001\n",
            "28 280 0.2514934539794922 12.49940658942768 0.001\n",
            "28 290 0.2525242269039154 11.1502300864253 0.001\n",
            "28 300 0.15394124388694763 12.655306681873657 0.001\n",
            "28 310 0.11917713284492493 11.09356975330234 0.001\n",
            "28 320 0.23336055874824524 11.733672766438085 0.001\n",
            "28 330 0.2729785144329071 11.123726576128288 0.001\n",
            "28 340 0.3565949499607086 12.6578369010767 0.001\n",
            "28 350 1.2701325416564941 12.456965846805868 0.001\n",
            "28 360 0.304456889629364 11.97461645248436 0.001\n",
            "28 370 0.25150614976882935 11.605614784603338 0.001\n",
            "28 380 0.1281265765428543 9.901719455090788 0.001\n",
            "28 390 0.18544918298721313 11.114330989087865 0.001\n",
            "28 400 0.12322001159191132 12.839299981327121 0.001\n",
            "28 410 0.459783673286438 12.441474210803039 0.001\n",
            "28 420 0.09860850870609283 12.601798946170574 0.001\n",
            "28 430 0.3997196555137634 10.676679040008553 0.001\n",
            "28 440 0.3876681625843048 11.905531881677128 0.001\n",
            "28 450 0.8029608130455017 11.955987822537427 0.001\n",
            "28 460 0.12979795038700104 11.720704728251794 0.001\n",
            "28 470 0.9808746576309204 11.415790154118328 0.001\n",
            "28 480 0.15206609666347504 11.832222556043122 0.001\n",
            "28 490 0.39310383796691895 11.348839222901672 0.001\n",
            "28 500 0.18378736078739166 11.528878517131952 0.001\n",
            "28 510 0.1322590559720993 9.942313162535793 0.001\n",
            "28 520 0.23503342270851135 8.854374670808541 0.001\n",
            "28 530 0.14066168665885925 9.622633804335607 0.001\n",
            "28 540 0.3338773250579834 12.810046942334534 0.001\n",
            "28 550 0.1225479245185852 12.422055695287794 0.001\n",
            "28 560 0.07672709226608276 12.27477824179619 0.001\n",
            "28 570 0.2094954252243042 11.46825234615261 0.001\n",
            "28 580 0.1254703253507614 10.418773796223903 0.001\n",
            "28 590 0.1966470628976822 7.8087933784593275 0.001\n",
            "28 600 0.14822004735469818 12.093491425387427 0.001\n",
            "28 610 0.1945468783378601 11.99980545359353 0.001\n",
            "28 620 0.13558222353458405 11.371454133850262 0.001\n",
            "28 630 0.23358109593391418 11.329381122713237 0.001\n",
            "28 640 0.3489384353160858 12.251222404617389 0.001\n",
            "28 650 0.3419662415981293 10.68317846097223 0.001\n",
            "28 660 0.12240785360336304 11.719640361091882 0.001\n",
            "28 670 0.0840531513094902 8.778854415324416 0.001\n",
            "28 680 0.35199451446533203 11.240804862622342 0.001\n",
            "28 690 0.33927249908447266 10.317253457898866 0.001\n",
            "28 700 0.4714999198913574 11.559078206901873 0.001\n",
            "28 710 0.14938773214817047 12.311736763970966 0.001\n",
            "28 720 0.21754108369350433 11.554357535166405 0.001\n",
            "28 730 0.12503303587436676 12.51137508510713 0.001\n",
            "28 740 0.11030377447605133 12.1873856334969 0.001\n",
            "28 750 0.2030046433210373 11.660966811468288 0.001\n",
            "28 760 0.10211730748414993 10.130006822888678 0.001\n",
            "28 770 0.30141690373420715 12.810134971592362 0.001\n",
            "28 780 0.11378874629735947 12.195101669363408 0.001\n",
            "28 790 0.0950058251619339 12.048270017953321 0.001\n",
            "28 800 0.20037490129470825 9.902718859546347 0.001\n",
            "28 810 0.08119966089725494 12.67618016460613 0.001\n",
            "28 820 0.2383410483598709 11.994469331815784 0.001\n",
            "28 830 0.11637502908706665 11.55305266738696 0.001\n",
            "28 840 0.3494614064693451 12.268835934494899 0.001\n",
            "28 850 0.28835079073905945 12.399516944630443 0.001\n",
            "28 860 0.15127021074295044 12.816104220314681 0.001\n",
            "tensor(58.4796, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 29\n",
            "29 10 0.16104112565517426 12.419407396769678 0.001\n",
            "29 20 0.29158255457878113 10.826007895650424 0.001\n",
            "29 30 0.3508193790912628 11.136892331262688 0.001\n",
            "29 40 0.2537081837654114 12.094188852036421 0.001\n",
            "29 50 0.40944454073905945 12.292711806038936 0.001\n",
            "29 60 0.16050878167152405 12.297270989456896 0.001\n",
            "29 70 0.10853233188390732 12.128321293362509 0.001\n",
            "29 80 0.2562071681022644 10.082703911151711 0.001\n",
            "29 90 0.25432485342025757 12.376959395656279 0.001\n",
            "29 100 0.13567695021629333 11.935251542663154 0.001\n",
            "29 110 0.2285269796848297 10.540902094079653 0.001\n",
            "29 120 0.1692747324705124 12.22691913589874 0.001\n",
            "29 130 0.32754456996917725 12.0655820703602 0.001\n",
            "29 140 0.1838565617799759 12.165954330217616 0.001\n",
            "29 150 0.2014973759651184 12.577594821789088 0.001\n",
            "29 160 0.5962945818901062 12.66890536237345 0.001\n",
            "29 170 0.10546056926250458 11.13146866880575 0.001\n",
            "29 180 0.28279998898506165 11.79566075449916 0.001\n",
            "29 190 0.12131956219673157 10.220157275212127 0.001\n",
            "29 200 0.16057586669921875 12.294504441192398 0.001\n",
            "29 210 0.1868838369846344 12.454117741377544 0.001\n",
            "29 220 0.26324042677879333 12.188536660673096 0.001\n",
            "29 230 0.15110765397548676 12.703439053883484 0.001\n",
            "29 240 0.11654376983642578 12.759105680113102 0.001\n",
            "29 250 0.4758651852607727 12.62572583631093 0.001\n",
            "29 260 0.21375256776809692 11.885677365787513 0.001\n",
            "29 270 0.08590390533208847 12.72087304842847 0.001\n",
            "29 280 0.10199694335460663 12.540600015547543 0.001\n",
            "29 290 0.2420886754989624 11.727299924787433 0.001\n",
            "29 300 0.2629302442073822 12.279422990923576 0.001\n",
            "29 310 1.5977319478988647 11.846944280464692 0.001\n",
            "29 320 0.31403684616088867 12.970944283719614 0.001\n",
            "29 330 0.5254550576210022 12.016840693854924 0.001\n",
            "29 340 0.1238163411617279 12.193612626480665 0.001\n",
            "29 350 0.2029590904712677 12.70604629464137 0.001\n",
            "29 360 0.2588774561882019 10.372934883884845 0.001\n",
            "29 370 0.05803539603948593 12.470604850368792 0.001\n",
            "29 380 0.13480845093727112 12.165345635084936 0.001\n",
            "29 390 0.3374304175376892 12.787560928204757 0.001\n",
            "29 400 0.24465487897396088 12.746774979049425 0.001\n",
            "29 410 0.29676178097724915 12.540206327352202 0.001\n",
            "29 420 0.07633144408464432 12.906533651408838 0.001\n",
            "29 430 0.6471538543701172 12.02400613483742 0.001\n",
            "29 440 0.1493394821882248 11.565500687289488 0.001\n",
            "29 450 0.5695083141326904 10.748211163722162 0.001\n",
            "29 460 0.485960990190506 10.727861003189478 0.001\n",
            "29 470 0.6020756363868713 11.15253522971188 0.001\n",
            "29 480 0.28812462091445923 11.12143332529028 0.001\n",
            "29 490 0.10937787592411041 12.193364488019013 0.001\n",
            "29 500 0.10521719604730606 11.782067218085876 0.001\n",
            "29 510 0.22309793531894684 11.441816754608688 0.001\n",
            "29 520 0.6319499611854553 11.228459431295924 0.001\n",
            "29 530 0.21436570584774017 9.983936168376854 0.001\n",
            "29 540 0.09313206374645233 12.783975165407742 0.001\n",
            "29 550 0.21509471535682678 11.604755834294217 0.001\n",
            "29 560 0.19344013929367065 10.303086280251147 0.001\n",
            "29 570 0.4256594181060791 12.654581220743681 0.001\n",
            "29 580 0.07408324629068375 12.440976014890122 0.001\n",
            "29 590 0.18690799176692963 9.288907491933145 0.001\n",
            "29 600 0.578498363494873 12.483874304176393 0.001\n",
            "29 610 0.14607356488704681 11.708386284538667 0.001\n",
            "29 620 0.15806016325950623 11.713716189184868 0.001\n",
            "29 630 0.37137657403945923 11.135066612420115 0.001\n",
            "29 640 0.0929475799202919 9.512020415117211 0.001\n",
            "29 650 0.08057531714439392 10.03878918574782 0.001\n",
            "29 660 0.15889866650104523 9.923623918600951 0.001\n",
            "29 670 0.1381053626537323 10.284164662209047 0.001\n",
            "29 680 0.34834426641464233 11.408252704129753 0.001\n",
            "29 690 0.2507377862930298 12.507028729938282 0.001\n",
            "29 700 0.5499749183654785 10.026754001954286 0.001\n",
            "29 710 0.176997110247612 10.46462799184896 0.001\n",
            "29 720 0.15790246427059174 12.376420702887843 0.001\n",
            "29 730 0.3296433687210083 12.300688087688107 0.001\n",
            "29 740 0.1461542695760727 12.422147670386773 0.001\n",
            "29 750 0.18452802300453186 10.873347258884992 0.001\n",
            "29 760 0.33476048707962036 11.028173837593382 0.001\n",
            "29 770 0.11671140044927597 12.74503199321165 0.001\n",
            "29 780 0.17741212248802185 12.066979251852603 0.001\n",
            "29 790 0.1392163783311844 12.263535802967274 0.001\n",
            "29 800 0.12094009667634964 12.250676710286216 0.001\n",
            "29 810 0.22204925119876862 11.619431633784497 0.001\n",
            "29 820 0.16813114285469055 10.544936867583772 0.001\n",
            "29 830 0.18082351982593536 11.704416534289933 0.001\n",
            "29 840 0.24415913224220276 10.254288660285297 0.001\n",
            "29 850 1.0503894090652466 12.422552377003358 0.001\n",
            "29 860 0.17400209605693817 12.760512817392469 0.001\n",
            "tensor(57.9182, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch 30\n",
            "30 10 0.3059537708759308 12.586426759672639 0.001\n",
            "30 20 0.29886195063591003 12.128444041220419 0.001\n",
            "30 30 0.13202928006649017 12.504362719486863 0.001\n",
            "30 40 0.2306469976902008 12.659938229170832 0.001\n",
            "30 50 0.08246905356645584 11.946767535573878 0.001\n",
            "30 60 0.3489318788051605 11.64078809694958 0.001\n",
            "30 70 0.5212509632110596 10.461417588947718 0.001\n",
            "30 80 0.7053558826446533 8.302605461328826 0.001\n",
            "30 90 0.4094904065132141 9.97589808386972 0.001\n",
            "30 100 0.28441160917282104 9.3785568824484 0.001\n",
            "30 110 0.06064312532544136 12.007113856776735 0.001\n",
            "30 120 0.43854793906211853 11.320215538123339 0.001\n",
            "30 130 0.451347678899765 12.62733179642688 0.001\n",
            "30 140 0.10014320909976959 12.363733907160807 0.001\n",
            "30 150 0.5341416001319885 11.89325204692872 0.001\n",
            "30 160 0.1255217045545578 11.716317504834306 0.001\n",
            "30 170 0.23220226168632507 10.578553751186186 0.001\n",
            "30 180 0.13375677168369293 12.116024042527224 0.001\n",
            "30 190 0.6259133815765381 12.84927096241539 0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfMxQckTdFbF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6f7e5b5-f15e-41c9-bf39-60bb5d0afd74"
      },
      "source": [
        "['./data/custom/images/image_3027.jpg', './data/custom/images/image_0522.jpg', './data/custom/images/image_1944.jpg', './data/custom/images/image_2820.jpg']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./data/custom/images/image_3027.jpg',\n",
              " './data/custom/images/image_0522.jpg',\n",
              " './data/custom/images/image_1944.jpg',\n",
              " './data/custom/images/image_2820.jpg']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6nlyY7gDKeg"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.ticker import NullLocator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrKYntZzEnK0"
      },
      "source": [
        "cmap = plt.get_cmap('tab20b')\n",
        "colors = [cmap(i) for i in np.linspace(0, 1, 5)]\n",
        "print(len(colors))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf6xDXGBDdRW"
      },
      "source": [
        "def non_max_suppression(prediction, num_classes, conf_thres=0.5, nms_thres=0.4):\n",
        "    \"\"\"\n",
        "    Removes detections with lower object confidence score than 'conf_thres' and performs\n",
        "    Non-Maximum Suppression to further filter detections.\n",
        "    Returns detections with shape:\n",
        "        (x1, y1, x2, y2, object_conf, class_score, class_pred)\n",
        "    \"\"\"\n",
        "\n",
        "    # From (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "    box_corner = prediction.new(prediction.shape)\n",
        "    box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2\n",
        "    box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2\n",
        "    box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2\n",
        "    box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2\n",
        "    prediction[:, :, :4] = box_corner[:, :, :4]\n",
        "\n",
        "    output = [None for _ in range(len(prediction))]\n",
        "    for image_i, image_pred in enumerate(prediction):\n",
        "        # Filter out confidence scores below threshold\n",
        "        conf_mask = (image_pred[:, 4] >= conf_thres).squeeze()\n",
        "        image_pred = image_pred[conf_mask]\n",
        "        # If none are remaining => process next image\n",
        "        if not image_pred.size(0):\n",
        "            continue\n",
        "        # Get score and class with highest confidence\n",
        "        class_conf, class_pred = torch.max(image_pred[:, 5:5 + num_classes], 1,  keepdim=True)\n",
        "        # Detections ordered as (x1, y1, x2, y2, obj_conf, class_conf, class_pred)\n",
        "        detections = torch.cat((image_pred[:, :5], class_conf.float(), class_pred.float()), 1)\n",
        "        # Iterate through all predicted classes\n",
        "        unique_labels = detections[:, -1].cpu().unique()\n",
        "        if prediction.is_cuda:\n",
        "            unique_labels = unique_labels.cuda()\n",
        "        for c in unique_labels:\n",
        "            # Get the detections with the particular class\n",
        "            detections_class = detections[detections[:, -1] == c]\n",
        "            # Sort the detections by maximum objectness confidence\n",
        "            _, conf_sort_index = torch.sort(detections_class[:, 4], descending=True)\n",
        "            detections_class = detections_class[conf_sort_index]\n",
        "            # Perform non-maximum suppression\n",
        "            max_detections = []\n",
        "            while detections_class.size(0):\n",
        "                # Get detection with highest confidence and save as max detection\n",
        "                max_detections.append(detections_class[0].unsqueeze(0))\n",
        "                # Stop if we're at the last detection\n",
        "                if len(detections_class) == 1:\n",
        "                    break\n",
        "                # Get the IOUs for all boxes with lower confidence\n",
        "                ious = bbox_iou(max_detections[-1], detections_class[1:])\n",
        "                # Remove detections with IoU >= NMS threshold\n",
        "                detections_class = detections_class[1:][ious < nms_thres]\n",
        "\n",
        "            max_detections = torch.cat(max_detections).data\n",
        "            # Add max detections to outputs\n",
        "            output[image_i] = max_detections if output[image_i] is None else torch.cat((output[image_i], max_detections))\n",
        "\n",
        "    return output\n",
        "\n",
        "def bbox_iou(box1, box2, x1y1x2y2=True):\n",
        "    \"\"\"\n",
        "    Returns the IoU of two bounding boxes\n",
        "    \"\"\"\n",
        "    if not x1y1x2y2:\n",
        "        # Transform from center and width to exact coordinates\n",
        "        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n",
        "        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n",
        "        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n",
        "        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n",
        "    else:\n",
        "        # Get the coordinates of bounding boxes\n",
        "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,2], box1[:,3]\n",
        "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,2], box2[:,3]\n",
        "\n",
        "    # get the corrdinates of the intersection rectangle\n",
        "    inter_rect_x1 =  torch.max(b1_x1, b2_x1)\n",
        "    inter_rect_y1 =  torch.max(b1_y1, b2_y1)\n",
        "    inter_rect_x2 =  torch.min(b1_x2, b2_x2)\n",
        "    inter_rect_y2 =  torch.min(b1_y2, b2_y2)\n",
        "    # Intersection area\n",
        "    inter_area =    torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * \\\n",
        "                    torch.clamp(inter_rect_y2 - inter_rect_y1 + 1, min=0)\n",
        "    # Union Area\n",
        "    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n",
        "    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n",
        "\n",
        "    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
        "\n",
        "    return iou"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeQZCXM5-ku8"
      },
      "source": [
        "# YOLO loss with 3 scales\n",
        "yolo_losses = []\n",
        "for i in range(3):\n",
        "    yolo_losses.append(YOLOLoss(config[\"yolo\"][\"anchors\"][i],\n",
        "                                config[\"yolo\"][\"classes\"], (config[\"img_w\"], config[\"img_h\"])))\n",
        "\n",
        "# prepare images path\n",
        "images_name = os.listdir('/content/MiDaS/YOLOv3_PyTorch/data/yolo_data/customdata/images/')\n",
        "images_path = [os.path.join('/content/MiDaS/YOLOv3_PyTorch/data/yolo_data/customdata/images/', name) for name in images_name]\n",
        "if len(images_path) == 0:\n",
        "    raise Exception(\"no image found in {}\".format(config[\"images_path\"]))\n",
        "\n",
        "# Start inference\n",
        "batch_size = config[\"batch_size\"]\n",
        "for step in range(0, len(images_path), batch_size):\n",
        "    # preprocess\n",
        "    images = []\n",
        "    images_origin = []\n",
        "    for path in images_path[step*batch_size: (step+1)*batch_size]:\n",
        "        logging.info(\"processing: {}\".format(path))\n",
        "        image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "        if image is None:\n",
        "            logging.error(\"read path error: {}. skip it.\".format(path))\n",
        "            continue\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        images_origin.append(image)  # keep for save result\n",
        "        if len(images_origin) == 0:\n",
        "            continue\n",
        "        image = cv2.resize(image, (config[\"img_w\"], config[\"img_h\"]),\n",
        "                            interpolation=cv2.INTER_LINEAR)\n",
        "        image = image.astype(np.float32)\n",
        "        image /= 255.0\n",
        "        image = np.transpose(image, (2, 0, 1))\n",
        "        image = image.astype(np.float32)\n",
        "        images.append(image)\n",
        "    images = np.asarray(images)\n",
        "    images = torch.from_numpy(images).cuda()\n",
        "    # inference\n",
        "    with torch.no_grad():\n",
        "        print(images.shape)\n",
        "        outputs = detector(images)\n",
        "        output_list = []\n",
        "        for i in range(3):\n",
        "            output_list.append(yolo_losses[i](outputs[i]))\n",
        "        output = torch.cat(output_list, 1)\n",
        "        batch_detections = non_max_suppression(output, config[\"yolo\"][\"classes\"],\n",
        "                                                conf_thres=0.5,\n",
        "                                                nms_thres=0.45)\n",
        "\n",
        "    # write result images. Draw bounding boxes and labels of detections\n",
        "    classes = open('/content/MiDaS/YOLOv3_PyTorch/data/yolo_data/customdata/classes.txt', \"r\").read().split(\"\\n\")[:-1]\n",
        "    if not os.path.isdir(\"./output/\"):\n",
        "        os.makedirs(\"./output/\")\n",
        "    for idx, detections in enumerate(batch_detections):\n",
        "        # plt.figure()\n",
        "        fig, ax = plt.subplots(1)\n",
        "        ax.imshow(images_origin[idx])\n",
        "        if detections is not None:\n",
        "            unique_labels = detections[:, -1].cpu().unique()\n",
        "            n_cls_preds = len(unique_labels)\n",
        "            bbox_colors = random.sample(colors, n_cls_preds)\n",
        "            for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\n",
        "                color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n",
        "                # Rescale coordinates to original dimensions\n",
        "                ori_h, ori_w = images_origin[idx].shape[:2]\n",
        "                pre_h, pre_w = config[\"img_h\"], config[\"img_w\"]\n",
        "                box_h = ((y2 - y1) / pre_h) * ori_h\n",
        "                box_w = ((x2 - x1) / pre_w) * ori_w\n",
        "                y1 = (y1 / pre_h) * ori_h\n",
        "                x1 = (x1 / pre_w) * ori_w\n",
        "                # Create a Rectangle patch\n",
        "                bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2,\n",
        "                                            edgecolor=color,\n",
        "                                            facecolor='none')\n",
        "                # Add the bbox to the plot\n",
        "                ax.add_patch(bbox)\n",
        "                # Add label\n",
        "                print(int(cls_pred), len(classes), classes[int(cls_pred)])\n",
        "                plt.text(x1, y1, s=classes[int(cls_pred)], color='white',\n",
        "                            verticalalignment='top',\n",
        "                            bbox={'color': color, 'pad': 0})\n",
        "        # Save generated image with detections\n",
        "        plt.axis('off')\n",
        "        plt.gca().xaxis.set_major_locator(NullLocator())\n",
        "        plt.gca().yaxis.set_major_locator(NullLocator())\n",
        "        plt.savefig('output/{}_{}.jpg'.format(step, idx), bbox_inches='tight', pad_inches=0.0)\n",
        "        plt.close()\n",
        "logging.info(\"Save all results to ./output/\")    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmYOhCr1Dwn9"
      },
      "source": [
        "# ! rm -rf /content/MiDaS/YOLOv3_PyTorch/common/data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRuSCHgsCptZ"
      },
      "source": [
        "# ! cp -r /content/drive/MyDrive/Annotated\\ Images/labels/  /content/MiDaS/YOLOv3_PyTorch/data/custom/labels/\n",
        "# ! cp -r /content/drive/MyDrive/Annotated\\ Images/images/  /content/MiDaS/YOLOv3_PyTorch/data/custom/images/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBEfSLRCBE_l"
      },
      "source": [
        "# ! cp /content/drive/MyDrive/ConstructionPPE/input_file_dir.txt  /content/MiDaS/YOLOv3_PyTorch/data/custom/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJLK9HhXJMrD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}