{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inferencing midas + yolo",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1fb4LJi4FTC",
        "outputId": "418cce9a-5e9c-4330-e331-ad1c87c3e3d2"
      },
      "source": [
        "! git clone https://github.com/intel-isl/MiDaS.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MiDaS'...\n",
            "remote: Enumerating objects: 250, done.\u001b[K\n",
            "remote: Counting objects: 100% (250/250), done.\u001b[K\n",
            "remote: Compressing objects: 100% (173/173), done.\u001b[K\n",
            "remote: Total 394 (delta 58), reused 201 (delta 37), pack-reused 144\u001b[K\n",
            "Receiving objects: 100% (394/394), 231.02 KiB | 5.50 MiB/s, done.\n",
            "Resolving deltas: 100% (139/139), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88-3dCsK4u47",
        "outputId": "0b08e7f4-8f7a-49b4-e317-7baf7152f9f0"
      },
      "source": [
        "! git clone https://github.com/BobLiu20/YOLOv3_PyTorch.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'YOLOv3_PyTorch'...\n",
            "remote: Enumerating objects: 178, done.\u001b[K\n",
            "remote: Total 178 (delta 0), reused 0 (delta 0), pack-reused 178\u001b[K\n",
            "Receiving objects: 100% (178/178), 899.65 KiB | 13.63 MiB/s, done.\n",
            "Resolving deltas: 100% (73/73), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFCxs2Nbl8-L",
        "outputId": "d454c260-0d35-4081-9ddd-202c252b7d00"
      },
      "source": [
        "! wget https://github.com/intel-isl/MiDaS/releases/download/v2_1/model-f6b98070.pt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-06 10:01:22--  https://github.com/intel-isl/MiDaS/releases/download/v2_1/model-f6b98070.pt\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/193518067/f6db3a00-236a-11eb-9db9-6689df01a8ba?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20201206%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201206T100122Z&X-Amz-Expires=300&X-Amz-Signature=12a341edbd72d4cde721192e48eb461d474a7c75d38534f1d50d4c44ade200b6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=193518067&response-content-disposition=attachment%3B%20filename%3Dmodel-f6b98070.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2020-12-06 10:01:22--  https://github-production-release-asset-2e65be.s3.amazonaws.com/193518067/f6db3a00-236a-11eb-9db9-6689df01a8ba?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20201206%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201206T100122Z&X-Amz-Expires=300&X-Amz-Signature=12a341edbd72d4cde721192e48eb461d474a7c75d38534f1d50d4c44ade200b6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=193518067&response-content-disposition=attachment%3B%20filename%3Dmodel-f6b98070.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.20.56\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.20.56|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 422509849 (403M) [application/octet-stream]\n",
            "Saving to: ‘model-f6b98070.pt’\n",
            "\n",
            "model-f6b98070.pt   100%[===================>] 402.94M  96.4MB/s    in 4.1s    \n",
            "\n",
            "2020-12-06 10:01:26 (98.0 MB/s) - ‘model-f6b98070.pt’ saved [422509849/422509849]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzEiQoV8ZD4A"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import OrderedDict"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Aj7dceH5Hsl"
      },
      "source": [
        "from MiDaS.midas.midas_net import MidasNet\n",
        "\n",
        "class YOLOLayers(nn.Module):\n",
        "    def __init__(self, config, is_training=True):\n",
        "        super(YOLOLayers, self).__init__()\n",
        "        self.config = config\n",
        "        self.training = is_training\n",
        "        \n",
        "        _out_filters = [256, 512, 1024, 2048]\n",
        "        final_out_filter0 = len(config[\"yolo\"][\"anchors\"][0]) * (5 + config[\"yolo\"][\"classes\"])\n",
        "        self.embedding0 = self._make_embedding([512, 2048], _out_filters[-1], final_out_filter0)\n",
        "        #  embedding1\n",
        "        final_out_filter1 = len(config[\"yolo\"][\"anchors\"][1]) * (5 + config[\"yolo\"][\"classes\"])\n",
        "        self.embedding1_cbl = self._make_cbl(512, 256, 1)\n",
        "        self.embedding1_upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.embedding1 = self._make_embedding([256, 1024], _out_filters[-2] + 256, final_out_filter1)\n",
        "        #  embedding2\n",
        "        final_out_filter2 = len(config[\"yolo\"][\"anchors\"][2]) * (5 + config[\"yolo\"][\"classes\"])\n",
        "        self.embedding2_cbl = self._make_cbl(256, 128, 1)\n",
        "        self.embedding2_upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.embedding2 = self._make_embedding([128, 512], _out_filters[-3] + 128, final_out_filter2)\n",
        "\n",
        "    def _make_cbl(self, _in, _out, ks):\n",
        "        pad = (ks - 1) // 2 if ks else 0\n",
        "        return nn.Sequential(OrderedDict([\n",
        "            (\"conv\", nn.Conv2d(_in, _out, kernel_size=ks, stride=1, padding=pad, bias=False)),\n",
        "            (\"bn\", nn.BatchNorm2d(_out)),\n",
        "            (\"relu\", nn.LeakyReLU(0.1)),\n",
        "        ]))\n",
        "\n",
        "    def _make_embedding(self, filters_list, in_filters, out_filter):\n",
        "        m = nn.ModuleList([\n",
        "            self._make_cbl(in_filters, filters_list[0], 1),\n",
        "            self._make_cbl(filters_list[0], filters_list[1], 3),\n",
        "            self._make_cbl(filters_list[1], filters_list[0], 1),\n",
        "            self._make_cbl(filters_list[0], filters_list[1], 3),\n",
        "            self._make_cbl(filters_list[1], filters_list[0], 1),\n",
        "            self._make_cbl(filters_list[0], filters_list[1], 3)])\n",
        "        m.add_module(\"conv_out\", nn.Conv2d(filters_list[1], out_filter, kernel_size=1,\n",
        "                                           stride=1, padding=0, bias=True))\n",
        "        return m\n",
        "\n",
        "class MainModel(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, config, midas_path):\n",
        "        super(MainModel, self).__init__()\n",
        "        self.midas = MidasNet(midas_path, non_negative = True)\n",
        "        self.pretrained = self.midas.pretrained\n",
        "        self.scratch = self.midas.scratch\n",
        "        self.yolo = YOLOLayers(config, is_training = False)\n",
        "        print('Loading yolo pretrained')\n",
        "        state_dict = torch.load('yolo_saved_model.pth', map_location = torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "        self.yolo.load_state_dict(state_dict, strict = False)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        layer_1 = self.pretrained.layer1(x)\n",
        "        layer_2 = self.pretrained.layer2(layer_1)\n",
        "        layer_3 = self.pretrained.layer3(layer_2)\n",
        "        layer_4 = self.pretrained.layer4(layer_3)\n",
        "\n",
        "        layer_1_rn = self.scratch.layer1_rn(layer_1)\n",
        "        layer_2_rn = self.scratch.layer2_rn(layer_2)\n",
        "        layer_3_rn = self.scratch.layer3_rn(layer_3)\n",
        "        layer_4_rn = self.scratch.layer4_rn(layer_4)\n",
        "\n",
        "        path_4 = self.scratch.refinenet4(layer_4_rn)\n",
        "        path_3 = self.scratch.refinenet3(path_4, layer_3_rn)\n",
        "        path_2 = self.scratch.refinenet2(path_3, layer_2_rn)\n",
        "        path_1 = self.scratch.refinenet1(path_2, layer_1_rn)\n",
        "\n",
        "        midas_out = self.scratch.output_conv(path_1)\n",
        "\n",
        "        x2, x1, x0 = layer_2, layer_3, layer_4\n",
        "\n",
        "        def _branch(_embedding, _in):\n",
        "            for i, e in enumerate(_embedding):\n",
        "                _in = e(_in)\n",
        "                if i == 4:\n",
        "                    out_branch = _in\n",
        "            return _in, out_branch\n",
        "        \n",
        "        #  yolo branch 0\n",
        "        out0, out0_branch = _branch(self.yolo.embedding0, x0)\n",
        "        #  yolo branch 1\n",
        "        x1_in = self.yolo.embedding1_cbl(out0_branch)\n",
        "        x1_in = self.yolo.embedding1_upsample(x1_in)\n",
        "        x1_in = torch.cat([x1_in, x1], 1)\n",
        "        out1, out1_branch = _branch(self.yolo.embedding1, x1_in)\n",
        "        #  yolo branch 2\n",
        "        x2_in = self.yolo.embedding2_cbl(out1_branch)\n",
        "        x2_in = self.yolo.embedding2_upsample(x2_in)\n",
        "        x2_in = torch.cat([x2_in, x2], 1)\n",
        "        out2, out2_branch = _branch(self.yolo.embedding2, x2_in)\n",
        "\n",
        "        return (out0, out1, out2), torch.squeeze(midas_out, dim=1)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z4pmHtoYrzk"
      },
      "source": [
        "config = {\"yolo\": {\n",
        "    \"anchors\": [[[116, 90], [156, 198], [373, 326]],\n",
        "                [[30, 61], [62, 45], [59, 119]],\n",
        "                [[10, 13], [16, 30], [33, 23]]],\n",
        "    \"classes\": 4,\n",
        "},\n",
        "\"classes_names_path\" : 'classes'\n",
        ",\n",
        "\"lr\": {\n",
        "        \"backbone_lr\": 0.001,\n",
        "        \"other_lr\": 0.01,\n",
        "        \"freeze_backbone\": True,   #  freeze backbone wegiths to finetune\n",
        "        \"decay_gamma\": 0.1,\n",
        "        \"decay_step\": 20,           #  decay lr in every ? epochs\n",
        "    },\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"sgd\",\n",
        "        \"weight_decay\": 4e-05,\n",
        "    },\n",
        "    \"batch_size\": 1,\n",
        "    \"train_path\": \"../data/coco/trainvalno5k.txt\",\n",
        "    \"epochs\": 100,\n",
        "    \"img_h\": 416,\n",
        "    \"img_w\": 416,\n",
        "    \"parallels\": [0],                         #  config GPU device\n",
        "    \"working_dir\": \"YOUR_WORKING_DIR\",              #  replace with your working dir\n",
        "    \"pretrain_snapshot\": \"\",                        #  load checkpoint\n",
        "    \"evaluate_type\": \"\", \n",
        "    \"try\": 0,\n",
        "    \"export_onnx\": False,\n",
        "\n",
        "}"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQ2nmLTVRp3N",
        "outputId": "cb9a9c83-544e-40bd-98fa-ae9642b71257"
      },
      "source": [
        "midas_path = 'model-f6b98070.pt'\n",
        "model = MainModel(config, midas_path)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading weights:  model-f6b98070.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_WSL-Images_master\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading yolo pretrained\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Sz0gUFfYm6-"
      },
      "source": [
        "from torchsummary import summary\n",
        "print(summary(model, (3, 256, 256)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uuAP8OkCnRY"
      },
      "source": [
        "! cp /content/drive/MyDrive/yolo_saved_model.pth ./"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3E3-oCjC9uJ"
      },
      "source": [
        "# state_dict = torch.load('yolo_saved_model.pth', map_location = torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "# model.load_state_dict(state_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujjW_NAfY7kA"
      },
      "source": [
        "import MiDaS.utils as utils\n",
        "from torchvision.transforms import Compose\n",
        "from MiDaS.midas.transforms import Resize, NormalizeImage, PrepareForNet\n",
        "import glob\n",
        "import os\n",
        "import cv2"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CR4xU4_Dfoxz"
      },
      "source": [
        "input_path = 'YOLOv3_PyTorch/test/images/'\n",
        "output_path = 'output/'\n",
        "img_names = glob.glob(os.path.join(input_path, \"*\"))\n",
        "num_images = len(img_names)\n",
        "os.makedirs(output_path, exist_ok=True)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ne1lVfmoHFBf"
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.ticker import NullLocator\n",
        "import numpy as np\n",
        "\n",
        "from YOLOv3_PyTorch.nets.yolo_loss import YOLOLoss\n",
        "from YOLOv3_PyTorch.common.utils import non_max_suppression, bbox_iou"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQvsaq2CoDl3"
      },
      "source": [
        "cmap = plt.get_cmap('tab20b')\n",
        "colors = [cmap(i) for i in np.linspace(0, 1, 5)]\n",
        "\n",
        "net_w, net_h = 416, 416\n",
        "optimize = True\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c30dLNEkhKRe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61e7d6a3-9e83-4873-f871-c1a2eb4ba59d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0DVQRziAIH0",
        "outputId": "bb29659d-f0b0-48f6-818d-7c2e3fe80677"
      },
      "source": [
        "import logging\n",
        "import random\n",
        "yolo_losses = []\n",
        "for i in range(3):\n",
        "    yolo_losses.append(YOLOLoss(config[\"yolo\"][\"anchors\"][i],\n",
        "                                config[\"yolo\"][\"classes\"], (config[\"img_w\"], config[\"img_h\"])))\n",
        "\n",
        "# prepare images path\n",
        "images_name = os.listdir('/content/YOLOv3_PyTorch/test/images/')\n",
        "images_path = [os.path.join('/content/YOLOv3_PyTorch/test/images/', name) for name in images_name]\n",
        "if len(images_path) == 0:\n",
        "    raise Exception(\"no image found in {}\".format(config[\"images_path\"]))\n",
        "\n",
        "# Start inference\n",
        "batch_size = config[\"batch_size\"]\n",
        "print(batch_size)\n",
        "for step in range(0, len(images_path), batch_size):\n",
        "    # preprocess\n",
        "    images = []\n",
        "    images_origin = []\n",
        "    for path in images_path[step*batch_size: (step+1)*batch_size]:\n",
        "        logging.info(\"processing: {}\".format(path))\n",
        "        image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "        if image is None:\n",
        "            logging.error(\"read path error: {}. skip it.\".format(path))\n",
        "            continue\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        images_origin.append(image)  # keep for save result\n",
        "        if len(images_origin) == 0:\n",
        "            continue\n",
        "        image = cv2.resize(image, (config[\"img_w\"], config[\"img_h\"]),\n",
        "                            interpolation=cv2.INTER_LINEAR)\n",
        "        image = image.astype(np.float32)\n",
        "        image /= 255.0\n",
        "        image = np.transpose(image, (2, 0, 1))\n",
        "        image = image.astype(np.float32)\n",
        "        images.append(image)\n",
        "    images = np.asarray(images)\n",
        "    images = torch.from_numpy(images).to(device)\n",
        "    # inference\n",
        "    with torch.no_grad():\n",
        "        # print(images.shape)\n",
        "        outputs, prediction = model.forward(images)\n",
        "        # print(prediction.shape)\n",
        "        # print(path)\n",
        "        # print(outputs.shape)\n",
        "        output_list = []\n",
        "        for i in range(3):\n",
        "            output_list.append(yolo_losses[i](outputs[i]))\n",
        "        output = torch.cat(output_list, 1)\n",
        "        batch_detections = non_max_suppression(output, config[\"yolo\"][\"classes\"],\n",
        "                                                conf_thres=0.5,\n",
        "                                                nms_thres=0.45)\n",
        "        \n",
        "\n",
        "        prediction = (\n",
        "            torch.nn.functional.interpolate(\n",
        "                prediction.unsqueeze(1),\n",
        "                size=images.shape[2:],\n",
        "                mode=\"bicubic\",\n",
        "                align_corners=False,\n",
        "            )\n",
        "            .squeeze()\n",
        "            .cpu()\n",
        "            .numpy()\n",
        "        )\n",
        "\n",
        "    img_name = images_path[step].split('/')[-1]   \n",
        "    filename = os.path.join(\n",
        "    output_path, os.path.splitext(os.path.basename(img_name))[0]\n",
        "    )\n",
        "    # print('id', images_path[step])\n",
        "    # print(filename + str(step))\n",
        "    utils.write_depth(filename, prediction, bits=2)\n",
        "    \n",
        "\n",
        "    # write result images. Draw bounding boxes and labels of detections\n",
        "    classes = open('classes', \"r\").read().split(\"\\n\")[:-1]\n",
        "    if not os.path.isdir(\"./output/\"):\n",
        "        os.makedirs(\"./output/\")\n",
        "    for idx, detections in enumerate(batch_detections):\n",
        "        # plt.figure()\n",
        "        fig, ax = plt.subplots(1)\n",
        "        ax.imshow(images_origin[idx])\n",
        "        if detections is not None:\n",
        "            unique_labels = detections[:, -1].cpu().unique()\n",
        "            n_cls_preds = len(unique_labels)\n",
        "            bbox_colors = random.sample(colors, n_cls_preds)\n",
        "            for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\n",
        "                color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n",
        "                # Rescale coordinates to original dimensions\n",
        "                ori_h, ori_w = images_origin[idx].shape[:2]\n",
        "                pre_h, pre_w = config[\"img_h\"], config[\"img_w\"]\n",
        "                box_h = ((y2 - y1) / pre_h) * ori_h\n",
        "                box_w = ((x2 - x1) / pre_w) * ori_w\n",
        "                y1 = (y1 / pre_h) * ori_h\n",
        "                x1 = (x1 / pre_w) * ori_w\n",
        "                # Create a Rectangle patch\n",
        "                bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2,\n",
        "                                            edgecolor=color,\n",
        "                                            facecolor='none')\n",
        "                # Add the bbox to the plot\n",
        "                ax.add_patch(bbox)\n",
        "                # Add label\n",
        "                print(int(cls_pred), len(classes), classes[int(cls_pred)])\n",
        "                plt.text(x1, y1, s=classes[int(cls_pred)], color='white',\n",
        "                            verticalalignment='top',\n",
        "                            bbox={'color': color, 'pad': 0})\n",
        "        # Save generated image with detections\n",
        "        plt.axis('off')\n",
        "        plt.gca().xaxis.set_major_locator(NullLocator())\n",
        "        plt.gca().yaxis.set_major_locator(NullLocator())\n",
        "        plt.savefig('output/{}_{}.jpg'.format(step, idx), bbox_inches='tight', pad_inches=0.0)\n",
        "        plt.close()\n",
        "logging.info(\"Save all results to ./output/\") "
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "0 4 hardhat\n",
            "0 4 hardhat\n",
            "1 4 vest\n",
            "0 4 hardhat\n",
            "2 4 mask\n",
            "0 4 hardhat\n",
            "2 4 mask\n",
            "0 4 hardhat\n",
            "1 4 vest\n",
            "0 4 hardhat\n",
            "0 4 hardhat\n",
            "1 4 vest\n",
            "0 4 hardhat\n",
            "0 4 hardhat\n",
            "0 4 hardhat\n",
            "1 4 vest\n",
            "2 4 mask\n",
            "2 4 mask\n",
            "0 4 hardhat\n",
            "1 4 vest\n",
            "0 4 hardhat\n",
            "1 4 vest\n",
            "1 4 vest\n",
            "0 4 hardhat\n",
            "1 4 vest\n",
            "0 4 hardhat\n",
            "0 4 hardhat\n",
            "1 4 vest\n",
            "0 4 hardhat\n",
            "2 4 mask\n",
            "0 4 hardhat\n",
            "2 4 mask\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igJa84RTnvo9"
      },
      "source": [
        "! cp -r /content/drive/MyDrive/sample_ppe/* /content/YOLOv3_PyTorch/test/images"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vvKLAvPKQtI"
      },
      "source": [
        "# ! rm -r /content/output/*"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OB9EGGZVxXdT",
        "outputId": "cb76d280-0f3f-4bd4-9f5c-5a6d03b3a84f"
      },
      "source": [
        "!ls -a /content/YOLOv3_PyTorch/test/images/"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".  ..  image_0001.jpg  image_0010.jpg  image_0016.jpg  image_0019.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RtJATBBreeY"
      },
      "source": [
        "rm -r /content/YOLOv3_PyTorch/test/images/.*_checkpoints"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMfG5NWMxMDo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}